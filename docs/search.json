[
  {
    "objectID": "setup/workshop_setup.html",
    "href": "setup/workshop_setup.html",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "If you plan to use this repository with the Openscapes 2i2c JupyterHub Cloud Workspace there are no additional setup requirements for the Python environment. All packages needed are included unless specified within a notebook, in which case a cell will be dedicated to installing the necessary Python libraries using the appropriate package manager.\nAfter completing the prerequisites you will have access to the Openscapes 2i2c JupyterHub cloud workspace. Click here to start JupyterLab. Use your email and the provided password to sign in. This password will be provided in the workshop. If you’re interested in using the 2i2c cloud workspace outside of the workshop, please contact us.\nAfter signing in you will be prompted for some server options:\nBe sure to select the radio button for Python and a size of 28 GB RAM and up to 4 CPUs.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#cloning-the-nasa-neon-hsi-workshop-repository",
    "href": "setup/workshop_setup.html#cloning-the-nasa-neon-hsi-workshop-repository",
    "title": "Cloud Workspace Setup",
    "section": "Cloning the nasa-neon-hsi-workshop Repository",
    "text": "Cloning the nasa-neon-hsi-workshop Repository\nOnce the Python environment is spun up, you can clone the workshop repository.\nTo clone the repository, navigate to the directory where you want to store the repository on Openscapes, or on your local machine, then type the following from the terminal or command prompt:\ngit clone https://github.com/NEONScience/neon-nasa-hsi-workshop.git\nTo locate and start running the NEON notebooks, change directories (cd) to neon-nasa-hsi-workshop/neon\ncd neon-nasa-hsi-workshop/neon\nIf you plan to edit or contribute to this NEON NASA Airborne Hyperspectral Workshop Repository repository, or the NEON-Data-Skills repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#troubleshooting",
    "href": "setup/workshop_setup.html#troubleshooting",
    "title": "Cloud Workspace Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWe recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/prerequisites.html",
    "href": "setup/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nTo follow along during the workshop, or to run through the notebooks contained within the repository using the Openscapes 2i2c Cloud JupyterHub (cloud workspace), the following are required. All software and accounts are free.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All workshop participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.\n\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov/users/new\nRemember your username and password; you will need them to download or access data during the workshop and beyond.\n\nNEON User accout and API token\n\nCreate a NEON User account (if you don’t already have one) following the instructions here: https://www.neonscience.org/about/user-accounts/\nCreate an API token and save this; you will use this to download and access NEON data during the workshop and beyond. More detailed instructions on creating an API token can be found here: https://www.neonscience.org/resources/learning-hub/tutorials/neon-api-tokens-tutorial\nNote: you can also sign up for Data Notifications when you create your NEON User account. This will allow you to receive email notifications about updates or issues related to NEON data products of interest.",
    "crumbs": [
      "Setup Instructions",
      "Prerequisites"
    ]
  },
  {
    "objectID": "neon-nasa.html",
    "href": "neon-nasa.html",
    "title": "NEON and NASA Airborne and Field Tutorials",
    "section": "",
    "text": "Please view the NEON NASA Workshop Page for workshop details. Resources in this repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace.\nWelcome to the NEON - NASA Airborne Hyperspectral Data Resources Repository! This repository provides Python Jupyter notebooks to help the community work with visible to short-wave infrared (VSWIR) imaging spectroscopy data from NEON’s Airborne Observation Platform and missions carried out with the NASA AVIRIS sensors. These complimentary hyperspectral datasets provide an opportunity to conduct ecological research at large scales, and also can be paired with complimentary satellite datasets such as the EMIT (Earth Surface Mineral Dust Source Investigation) and future SBG (Surface Biology and Geology) VSWIR sensor.\nIn the interest of open science, this repository has been made public, but is still under active development. Contributions from all parties are welcome.",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "neon-nasa.html#contact-info",
    "href": "neon-nasa.html#contact-info",
    "title": "NEON and NASA Airborne and Field Tutorials",
    "section": "Contact Info",
    "text": "Contact Info\n\nNEON AOP\n\n\n\nOrganization\nNational Ecological Observatory Network Airborne Observation Platform (NEON AOP)1\n\n\nWebsite\nhttps://neonscience.org/\n\n\nContact\nhttps://www.neonscience.org/about/contact-us/\n\n\n\n1NEON is a project fully funded by the National Science Foundation and operated by Battelle.\n\n\nORNL DAAC\n\n\n\nOrganization\nNASA Earthdata Data Center, Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)\n\n\nWebsite\nhttps://www.earthdata.nasa.gov/centers/ornl-daac\n\n\nContact\nNASA Earthdata Forum: https://forum.earthdata.nasa.gov/",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html",
    "href": "neon/01_make-classification-training-df.html",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "",
    "text": "This notebook demonstrates how to generate a training dataset consisting of tree species, family, and location from the NEON Terrestrial Observation System (TOS) Vegetation Structure data product DP1.10098.001. We will use data from the Smithsonian Environmental Research Center (SERC) site in Maryland. In a subsequent tutorial titled Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray, we will use this training dataset to train a random forest machine learning model that predicts tree families from the hyperspectral signatures obtained from the airborne remote sensing data. These two tutorials outline a relatively simple modeling example, and represent a starting point for conducting machine learning analyses using NEON data!\n\n\nTo run this notebook, you will need the following Python packages, which can be installed using !pip install or !conda install from within the notebook. Note that to use the neonutilities package, you will need Python version 3.9 or higher.\n\nmatplotlib\nneonutilities\nnumpy\npandas\nrequests\nseaborn\n\n\n\n\n\nNEON API Token (optional, but strongly recommended), see NEON API Tokens Tutorial for more details on how to create and set up your token in Python (and R). Once you create your token (on the NEON User Accounts) page, this notebook will show you how to set it as an environment variable and use it for downloading AOP data.\n\n\n\n\n\nUse the neonutilities load_by_product function to read in NEON vegetation structure data at a given site\nUse the NEON locations API to determine the geographic position of the vegetation records in UTM x, y coordinates\nFilter the datset to include only the latest data and columns of interest\nFilter the data geospatially to keep data that are within a single AOP 1 km x 1 km tile\n\nDisclaimer: this notebook is intended to provide an example of how to create an initial training data set for pairing with remote sensing data, and to conduct some exploratory analysis of the vegetation structure data. This does not incorporate outlier detection and removal, or comprehensive pre-processing steps. As part of creating a machine learning model, it is important to assess the training data quality and look for outliers or other potential data quality issues which may impact model results. Refer to the Compare tree height measured from the ground to a Lidar-based Canopy Height Model lesson (the first additional resource above) for more details on how you would address geographic mismatch between the AOP and TOS data.\n\n\n\n\nThe lesson Compare tree height measured from the ground to a Lidar-based Canopy Height Model is another example of linking ground to airborne data, and shows similar steps of pre-processing TOS woody vegetation data.\nThe paper Individual canopy tree species maps for the National Ecological Observatory Network outlines methods for large-scale classification using NEON data. The associated NEON Science Seminar Harnessing NEON to enable the future of forest remote sensing may be a useful resource. This talk provides a high-level overview of modeling approaches for tree crown delineation and tree classification using NEON airborne remote sensing data. You can also watch the video below.\n\n\n\n\n\nRefer to the Vegetation Structure User Guide for more details on this data product, and to better understand the data quality flags, the sampling.",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#download-and-explore-vegetation-structure-data-dp1.10098.001",
    "href": "neon/01_make-classification-training-df.html#download-and-explore-vegetation-structure-data-dp1.10098.001",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "1. Download and Explore Vegetation Structure Data (DP1.10098.001)",
    "text": "1. Download and Explore Vegetation Structure Data (DP1.10098.001)\nIn this first section we’ll load the vegetation structure data, find the locations of the mapped trees, and join to the species and family observations.\nLet’s get started! First, import the required Python packages.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport neonutilities as nu\nimport numpy as np\nimport pandas as pd\nimport requests\nimport seaborn as sns\n\nSet up your NEON token. See the setup instructions at the beginning of the tutorial on how to set up a NEON user account and create a token, if you have not already done so.\n\n# copy and paste your NEON token from your NEON user account page here\nmy_token=\"\"\n\nWe can load the vegetation structure data using the load_by_product function in the neonutilities package (imported as nu). Inputs to the function can be shown by typing help(load_by_product).\nRefer tot e R neonUtilities cheat sheet or teh Python neonutilities documentationNEON Locations API.\n\neasting = []\nnorthing = []\ncoord_uncertainty = []\nelev_uncertainty = []\nfor i in veg_points:\n    vres = requests.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n    vres_json = vres.json()\n    easting.append(vres_json[\"data\"][\"locationUtmEasting\"])\n    northing.append(vres_json[\"data\"][\"locationUtmNorthing\"])\n    props = pd.DataFrame.from_dict(vres_json[\"data\"][\"locationProperties\"])\n    cu = props.loc[props[\"locationPropertyName\"]==\"Value for Coordinate uncertainty\"][\"locationPropertyValue\"]\n    coord_uncertainty.append(cu[cu.index[0]])\n    eu = props.loc[props[\"locationPropertyName\"]==\"Value for Elevation uncertainty\"][\"locationPropertyValue\"]\n    elev_uncertainty.append(eu[eu.index[0]])\n\npt_dict = dict(points=veg_points, \n               easting=easting,\n               northing=northing,\n               coordinateUncertainty=coord_uncertainty,\n               elevationUncertainty=elev_uncertainty)\n\npt_df = pd.DataFrame.from_dict(pt_dict)\npt_df.set_index(\"points\", inplace=True)\n\nveg_map = veg_map.join(pt_df, \n                     on=\"points\", \n                     how=\"inner\")\n\nNext, use the stemDistance and stemAzimuth data to calculate the precise locations of individuals, relative to the reference locations.\n\n\\(Easting = easting.pointID + stemDistance*sin(\\theta)\\)\n\\(Northing = northing.pointID + stemDistance*cos(\\theta)\\)\n\\(\\theta = stemAzimuth*\\pi/180\\)\n\nAlso adjust the coordinate and elevation uncertainties.\n\nveg_map[\"adjEasting\"] = (veg_map[\"easting\"]\n                        + veg_map[\"stemDistance\"]\n                        * np.sin(veg_map[\"stemAzimuth\"]\n                                   * np.pi / 180))\n\nveg_map[\"adjNorthing\"] = (veg_map[\"northing\"]\n                        + veg_map[\"stemDistance\"]\n                        * np.cos(veg_map[\"stemAzimuth\"]\n                                   * np.pi / 180))\n\nveg_map[\"adjCoordinateUncertainty\"] = veg_map[\"coordinateUncertainty\"] + 0.6\n\nveg_map[\"adjElevationUncertainty\"] = veg_map[\"elevationUncertainty\"] + 1\n\nLook at the columns to see all the information contained in this dataset.\n\n# look at a subset of the columns that may be relevant\nveg_map[['date','individualID','scientificName','taxonID','family','plotID','pointID','adjEasting','adjNorthing']].head(5)\n\n\nlen(veg_map)",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#filter-to-trees-within-an-aop-tile-extent",
    "href": "neon/01_make-classification-training-df.html#filter-to-trees-within-an-aop-tile-extent",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "3. Filter to trees within an AOP tile extent",
    "text": "3. Filter to trees within an AOP tile extent\nNow create a new dataframe containing only the veg data that are within a single AOP tile (which are 1 km x 1 km in size). For this, you will need to know the bounds (minimum and maximum UTM easting and northing) of the area you are sampling. For this exercise, we will choose the AOP data with SW (lower left) UTM coordinates of 364000, 4305000. This tile encompasses the NEON tower at the SERC site.\n\nveg_tower_tile = veg_map[(veg_map['adjEasting'].between(364000, 365000)) & (veg_map['adjNorthing'].between(4305000, 4306000))]\n\nHow many records do we have within this tile?\n\nlen(veg_tower_tile)\n\nThere are 211 unique vegetation records in this area. We can also look at the unique taxonIDs that are represented.\n\n# look at the unique Taxon IDs\nveg_tower_tile.taxonID.unique()\n\nLet’s keep only a subset of the columns that we are interested in, and look at the dataframe:\n\nveg_tower_tile_short = veg_tower_tile[['date','individualID','scientificName','taxonID','family','adjEasting','adjNorthing']]\nveg_tower_tile_short.reset_index(drop=True, inplace=True)\nveg_tower_tile_short\n\nTo get a better sense of the data, we can also look at the # of each species, to see if some species have more representation than others.\n\n# display the taxonID counts, sorted descending\nveg_tower_tile_taxon_counts = veg_tower_tile[['individualID','taxonID']].groupby(['taxonID']).count()\nveg_tower_tile_taxon_counts.sort_values(by='individualID',ascending=False)\n\n\n# display the family counts, sorted descending\nveg_tower_tile_family_counts = veg_tower_tile[['individualID','family']].groupby(['family']).count()\nveg_tower_tile_family_counts.sort_values(by='individualID',ascending=False)\n\nIt looks like there are a number of different species (and families) mapped in this tower plot. You can use the https://plants.usda.gov website to look up the species information. The top 5 most abundant mapped species are linked below.\n\nFAGR: American Beech (Fagus grandifolia Ehrh.)\nLITU: Tuliptree (Liriodendron tulipifera L.)\nLIST2: Sweetgum (Liquidambar styraciflua L.)\nACRU: Red Maple (Acer rubrum L.)\nCAGL8: Sweet pignut hickory (Carya glabra (Mill.))\n\nWhen carrying out classification, the species that only have small representation (1-5 samples) may not be modeled accurately due to a lack of sufficient training data. The challenge of mapping rarer species due to insufficient training data is well known. In the next tutorial, we will remove these poorly represented samples before generating a model.",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#write-training-dataframe-to-csv-file",
    "href": "neon/01_make-classification-training-df.html#write-training-dataframe-to-csv-file",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "4. Write training dataframe to csv file",
    "text": "4. Write training dataframe to csv file\nNonetheless, we have a fairly decent training dataset to work with. We can save the dataframe to a csv file called serc_training_data.csv as follows:\n\nveg_tower_tile_short.to_csv(r'./data/serc_training_data.csv',index=False)",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#plot-tree-families-in-map-view",
    "href": "neon/01_make-classification-training-df.html#plot-tree-families-in-map-view",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "5. Plot tree families in map view",
    "text": "5. Plot tree families in map view\nFinally, we can make a quick plot using seaborn (imported as sns) to show the spatial distrubtion of the trees surveyed in this area, along with their species (scientificName). Most of this code helps improve the formatting and appearance of the figure; the first sns.scatterplot chunk is all you really need to do to plot the essentials.\n\nax = sns.scatterplot(\n    data=veg_tower_tile_short,\n    x='adjEasting',\n    y='adjNorthing',\n    hue='family',\n)\n\n# Make the x and y dimensions are equal\nax.set_aspect('equal', adjustable='box')\n\n# Remove scientific notation on the x and y axes labels\nax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x:.0f}'))\nax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda y, _: f'{y:.0f}'))\n\n# Place the legend outside the plot at the center right\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n\n# Adjust layout to prevent legend overlap\nplt.tight_layout()\n\n# Add title and axis labels\nax.set_title(\"SERC Tree Families\", fontsize=14)\nax.set_xlabel(\"Easting (m)\", fontsize=12)\nax.set_ylabel(\"Northing (m)\", fontsize=12)\nplt.yticks(fontsize=8)  \nplt.xticks(fontsize=8)  \n\nplt.show()\n\nGreat! We can see all the trees that were surveyed in this AOP tile. The trees are sampled in discrete plots. For more information about the TOS sampling design, please refer to the Vegetation structure data product page.",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#recap",
    "href": "neon/01_make-classification-training-df.html#recap",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "Recap",
    "text": "Recap\nIn this lesson, we have created a training data set containing information about the tree family and species as well as their geographic locations in UTM x, y coordinates. We can now pair this training data set with the remote sensing data and create a model to predict the tree’s family based off airborne spectral data. The next tutorial, Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray, will show how to do this!\nNote: you may wish to create a training dataframe that contains additional information about the trees. For example, you can also include parameters like the growth form (e.g. whether the vegetation is a shrub, single-bole or multi-bole tree, etc.), the plant status (whether the tree is healthy or standing dead), and measurements such as the stem diameter and tree height. To do this, you would need to join the vst_mappingandtagging table with the vst_apparentindividual tables. Refer to the Quick Start Guide for Vegetation Structure for more information about the data tables and the joining instructions. You can also refer to the lesson Compare tree height measured from the ground to a Lidar-based Canopy Height Model which provides an example of how to do this and compare the TOS measured data with the AOP Lidar-derived Canopy Height Model (Ecosystem Structure) data product.",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "nasa/01_AVIRIS-NG_L3_BioSCape.html",
    "href": "nasa/01_AVIRIS-NG_L3_BioSCape.html",
    "title": "AVIRIS Data - Discovery, Access, and Visualization",
    "section": "",
    "text": "This tutorial will demonstrate Earthdata discovery and direct access of NASA airborne data archived through the ORNL DAAC. We’ll explore archived SBG-relevant airborne data using Earthdata Search and then, through a Notebook Tutorial, programmatically access and visualize Level 3 AVIRIS-Next Generation (ANG) Reflectance dataset collected during the Biodiversity Survey of the Cape (BioSCape) Campaign. BioSCape is an integrated field and airborne campaign in South Africa’s Greater Cape Floristic Region (GCFR) where collections occurred in 2023. The BioSCape Campaign utilzed four NASA airborne instruments to collect UV/visible to short wavelength infrared (UVSWIR) and thermal imaging (TIR) spectroscopy and laser altimetry LiDAR data over terrestrial and aquatic targets. Airborne Visible InfraRed Imaging Spectrometer - Next Generation (AVIRIS-NG), Portable Remote Imaging SpectroMeter (PRISM), Land, Vegetation, and Ice Sensor (LVIS), and Hyperspectral Thermal Emission Spectrometer (HyTES).\n\nLearn more about BioSCape\nNasa Earthdata Search: Discover Earthdata BioSCape Data\n\nThe BioSCape Campaign has produced an AVIRIS-NG L3 Resampled Mosaic dataset.\n- Surface reflectance data (Level 2b) derived from the AVIRIS-NG instrument were resampled to 5-m spatial resolution and mosaiced into a regular tile system of 807 tiles. A given tile includes multiple AVIRIS-NG scenes from multiple flight lines spanning multiple days. The mosaics in this dataset were generated by stitching together separate scenes and resampling to 5-m resolution in the Hartebeesthoek94 projected coordinate system (WGS-84 datum, EPSG 9221). The mosaic files are distributed on a tiled grid system, and the tile name is included in the file name. Mosaics were generated by manually grouping sets of flight lines into different chunks that should be placed ‘in front of’ or ‘behind’ other chunks. The selection criteria included a combination of the weather during observations, flight conditions, flightbox design, and the time the flight was taken.\n\nBrodrick, P.G., A.M. Chlus, R. Eckert, J.W. Chapman, M. Eastwood, S. Geier, M. Helmlinger, S.R. Lundeen, W. Olson-Duvall, R. Pavlick, L.M. Rios, D.R. Thompson, and R.O. Green. 2025. BioSCape: AVIRIS-NG L3 Resampled Reflectance Mosaics, V2. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/2427\n\nDataset Data Processing Levels - Level 3: Variables mapped on uniform space-time grid scales, usually with some completeness and consistency.\nNASA JPL BioSCape Data Portal\n\n\n\nBioSCape_MMGIS\n\n\n\nDemo: Live Demo of Earthdata Search",
    "crumbs": [
      "Tutorials",
      "3 NASA - AVIRIS Data - Discovery, Access, and Visualization"
    ]
  },
  {
    "objectID": "nasa/01_AVIRIS-NG_L3_BioSCape.html#overview",
    "href": "nasa/01_AVIRIS-NG_L3_BioSCape.html#overview",
    "title": "AVIRIS Data - Discovery, Access, and Visualization",
    "section": "",
    "text": "This tutorial will demonstrate Earthdata discovery and direct access of NASA airborne data archived through the ORNL DAAC. We’ll explore archived SBG-relevant airborne data using Earthdata Search and then, through a Notebook Tutorial, programmatically access and visualize Level 3 AVIRIS-Next Generation (ANG) Reflectance dataset collected during the Biodiversity Survey of the Cape (BioSCape) Campaign. BioSCape is an integrated field and airborne campaign in South Africa’s Greater Cape Floristic Region (GCFR) where collections occurred in 2023. The BioSCape Campaign utilzed four NASA airborne instruments to collect UV/visible to short wavelength infrared (UVSWIR) and thermal imaging (TIR) spectroscopy and laser altimetry LiDAR data over terrestrial and aquatic targets. Airborne Visible InfraRed Imaging Spectrometer - Next Generation (AVIRIS-NG), Portable Remote Imaging SpectroMeter (PRISM), Land, Vegetation, and Ice Sensor (LVIS), and Hyperspectral Thermal Emission Spectrometer (HyTES).\n\nLearn more about BioSCape\nNasa Earthdata Search: Discover Earthdata BioSCape Data\n\nThe BioSCape Campaign has produced an AVIRIS-NG L3 Resampled Mosaic dataset.\n- Surface reflectance data (Level 2b) derived from the AVIRIS-NG instrument were resampled to 5-m spatial resolution and mosaiced into a regular tile system of 807 tiles. A given tile includes multiple AVIRIS-NG scenes from multiple flight lines spanning multiple days. The mosaics in this dataset were generated by stitching together separate scenes and resampling to 5-m resolution in the Hartebeesthoek94 projected coordinate system (WGS-84 datum, EPSG 9221). The mosaic files are distributed on a tiled grid system, and the tile name is included in the file name. Mosaics were generated by manually grouping sets of flight lines into different chunks that should be placed ‘in front of’ or ‘behind’ other chunks. The selection criteria included a combination of the weather during observations, flight conditions, flightbox design, and the time the flight was taken.\n\nBrodrick, P.G., A.M. Chlus, R. Eckert, J.W. Chapman, M. Eastwood, S. Geier, M. Helmlinger, S.R. Lundeen, W. Olson-Duvall, R. Pavlick, L.M. Rios, D.R. Thompson, and R.O. Green. 2025. BioSCape: AVIRIS-NG L3 Resampled Reflectance Mosaics, V2. ORNL DAAC, Oak Ridge, Tennessee, USA. https://doi.org/10.3334/ORNLDAAC/2427\n\nDataset Data Processing Levels - Level 3: Variables mapped on uniform space-time grid scales, usually with some completeness and consistency.\nNASA JPL BioSCape Data Portal\n\n\n\nBioSCape_MMGIS\n\n\n\nDemo: Live Demo of Earthdata Search",
    "crumbs": [
      "Tutorials",
      "3 NASA - AVIRIS Data - Discovery, Access, and Visualization"
    ]
  },
  {
    "objectID": "nasa/01_AVIRIS-NG_L3_BioSCape.html#demo-earthdata-search---ornl-daac-airborne-facility-instrument-and-campaign-collections",
    "href": "nasa/01_AVIRIS-NG_L3_BioSCape.html#demo-earthdata-search---ornl-daac-airborne-facility-instrument-and-campaign-collections",
    "title": "AVIRIS Data - Discovery, Access, and Visualization",
    "section": "Demo: Earthdata Search - ORNL DAAC Airborne Facility Instrument and Campaign Collections",
    "text": "Demo: Earthdata Search - ORNL DAAC Airborne Facility Instrument and Campaign Collections\nNASA Earthdata Search\n\n\n\nEarthdata Search",
    "crumbs": [
      "Tutorials",
      "3 NASA - AVIRIS Data - Discovery, Access, and Visualization"
    ]
  },
  {
    "objectID": "nasa/01_AVIRIS-NG_L3_BioSCape.html#tutorial-programmatic-discovery-and-access-of-airborne-data",
    "href": "nasa/01_AVIRIS-NG_L3_BioSCape.html#tutorial-programmatic-discovery-and-access-of-airborne-data",
    "title": "AVIRIS Data - Discovery, Access, and Visualization",
    "section": "Tutorial: Programmatic Discovery and Access of Airborne Data",
    "text": "Tutorial: Programmatic Discovery and Access of Airborne Data\nBioSCape Campaign AVIRIS-NG Reflectance Level 3 Mosaic Dataset\n\nRequirements\n\nimport earthaccess\nimport geopandas as gpd\nimport pyproj\nfrom pyproj import Proj\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom shapely.ops import transform\nfrom shapely.ops import orient\nfrom shapely.geometry import Polygon, MultiPolygon\nimport folium\nimport hvplot.xarray\nimport holoviews as hv\nhvplot.extension('bokeh')\nimport rioxarray as rx\nfrom rioxarray import merge\nimport rasterio",
    "crumbs": [
      "Tutorials",
      "3 NASA - AVIRIS Data - Discovery, Access, and Visualization"
    ]
  },
  {
    "objectID": "nasa/01_AVIRIS-NG_L3_BioSCape.html#step-1-authentication-and-airbone-datasets-search-examples-with-earthaccess",
    "href": "nasa/01_AVIRIS-NG_L3_BioSCape.html#step-1-authentication-and-airbone-datasets-search-examples-with-earthaccess",
    "title": "AVIRIS Data - Discovery, Access, and Visualization",
    "section": "Step 1: Authentication and Airbone Datasets Search Examples with earthaccess",
    "text": "Step 1: Authentication and Airbone Datasets Search Examples with earthaccess\n\n\n\nearthaccess\n\n\nearthaccess is a Python library that simplifies data discovery and access to NASA Earthdata data by providing an abstraction layer to NASA’s APIs for programmatic access.\nIn this tutorial we’ll use earthaccess to: - handle authentication with NASA’s Earthdata Login (EDL), - search the NASA Earthdata Data holdings using NASA’s Common Metadata Repository (CMR), and - provide direct cloud file access\nEarthdata Login\nNASA Earthdata Login is a user registration and profile management system for users getting Earth science data from NASA Earthdata. If you download or access NASA Earthdata data, you need an Earthdata Login.\nUsing earthaccess we’ll login and authentice to NASA Earthdata Login. - For this exercise, we will be prompted for and interactively enter our Eathdata Login credentials (login, password)\n\nauth = earthaccess.login()\n\n\n1.1 earthaccess to search the NASA Common Metadata Repository for a specific airborne instrument\n\nresults = earthaccess.search_datasets(instrument=\"AVIRIS-3\")\n#results = earthaccess.search_datasets(instrument=\"AVIRIS-NG\")\n#results = earthaccess.search_datasets(instrument=\"AVIRIS\") # AVIRIS-Classic\n#results = earthaccess.search_datasets(instrument=\"MASTER\")\n#results = earthaccess.search_datasets(instrument=\"HYTES\")\n#results = earthaccess.search_datasets(instrument=\"PRISM\")\nprint(f\"Total Datasets (results) found: {len(results)}\")\n\nList the short-name for each dataset\n\nfor item in results:\n    summary = item.summary()\n    print(summary[\"short-name\"])\n\n\n\n1.2 Search UMM-G for Specific Campaigns\n\nAVIRIS-3 datasets contain campaign information in the Unified Metadata Model-Granule (UMM-G) AdditionalAttributes\nList the campaigns and number of granules in each campaign\nNote that at the time of this workshop, this functionality is specific to AVIRIS-3 Datasets\n\n\ndef  get_campaign_names(granules):\n    \"\"\"get campaign names for all granules\"\"\"\n    c = []\n    for g in granules:\n        for attrs in vars(g)['render_dict']['umm']['AdditionalAttributes']:\n            if attrs['Name'] == 'Campaign':\n                c += attrs['Values']\n    return c\n\n# earthdata search\ngranules = earthaccess.search_data(\n    #short_name = 'AV3_L1B_RDN_2356',\n    doi=\"10.3334/ORNLDAAC/2356\"\n)\n\ncampaigns = get_campaign_names(granules)\n\n#print campaign names and granules\nfor name in list(set(campaigns)):\n    print(f'{name} --&gt; {campaigns.count(name)} granules')\n\nIf you know a Campaign flown with AVIRIS-3, you can directly query the AdditionalAttributes\n\ndoi=\"10.3334/ORNLDAAC/2356\" # AV3_L1B_RDN_2356\nquery = earthaccess.DataGranules().doi(doi)\nquery.params['attribute[]'] = 'string,Campaign,SHIFT' \nl1b = query.get_all()\nprint(f'Granules found: {len(l1b)}')\n\n\n\n1.3 Search by Project knowing our dataset of interest was part of the BioSCape project.\n\nresults = earthaccess.search_datasets(project=\"BioSCape\")\nprint(f\"Total Datasets (results_projects) found: {len(results)}\")\n\nFor the instrument==“AVIRIS-NG” search, let’s look at the first result to see all of the CMR values that are returned - The first dataset listed is from the ABoVE Campaign - We see many useful fields for any one dataset including: short-name, concept-id, the S3BucketAndObjectPrefixNames\n\nfor index, item in enumerate(results):\n    if index == 0:\n        summary = item.summary()\n        print(summary)\n\nLet’s look at short-name for all of the search results\n\nfor item in results:\n    summary = item.summary()\n    print(summary[\"short-name\"])\n\n\nBioSCape_ANG_V02_L3_RFL_Mosaic_2427 is the short-name of the dataset of interest for this tutorial.\n\nLet’s use earthaccess to query the BioSCape: AVIRIS-NG L3 Resampled Reflectance Mosaics, V2 data to discover the files within that dataset.\n\nresults = earthaccess.search_data(\n    short_name = 'BioSCape_ANG_V02_L3_RFL_Mosaic_2427', \n)\nprint(f\"Total granules found: {len(results)}\")\n\nLet’s look at the first 2 results and examine the details of the granule-level CMR metadata information\n\n# granule-level CMR metadata information\nresults[:2]",
    "crumbs": [
      "Tutorials",
      "3 NASA - AVIRIS Data - Discovery, Access, and Visualization"
    ]
  },
  {
    "objectID": "nasa/01_AVIRIS-NG_L3_BioSCape.html#step-2.-further-define-and-refine-our-search-parameters",
    "href": "nasa/01_AVIRIS-NG_L3_BioSCape.html#step-2.-further-define-and-refine-our-search-parameters",
    "title": "AVIRIS Data - Discovery, Access, and Visualization",
    "section": "Step 2. Further Define and Refine our Search Parameters",
    "text": "Step 2. Further Define and Refine our Search Parameters\n\n2.1 Define more search parameters to limit our search\n\nbounding_box\ntemporal range\ngranule_name refined\n\nLet’s first create and visualize a bounding box for an area-of-interest within the South Africa Greater Cape Floristic Region (GCFR) of the BioSCape Campaign\n\nimport geopandas as gpd\nfrom shapely.geometry import Polygon\nfrom shapely.ops import transform\nimport pyproj\n\ndef create_geo_bb(coordinates, crs_in='epsg:4326', crs_out='epsg:4326'):\n    \n    polygon_shape = Polygon(coordinates)\n\n    if crs_in != crs_out:\n      project_in = pyproj.Proj(init=crs_in)\n      project_out = pyproj.Proj(init=crs_out)\n    \n      polygon_shape = transform(pyproj.Transformer.from_proj(project_in, project_out, always_xy=True).transform, polygon_shape)\n    \n    polygon = gpd.GeoDataFrame(geometry=[polygon_shape], crs=crs_out)\n    return polygon\n\ncoordinates = [\n    (17.9907, -33.1243),\n    (18.2469, -33.1243),\n    (18.2469, -33.2817),\n    (17.9907, -33.2817),\n    (17.9907, -33.1243)\n]\n\npolygon_gdf = create_geo_bb(coordinates)\nprint(polygon_gdf)\n\n\n#polygon_gdf.crs\n\n\npolygon_gdf.explore(fill=False, tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n\nWe’ll use this bounding box and temporal parameters to refine our search of BioSCape AVIRIS-NG L3 files in our region and time of interest\nAgain, using earthaccess we can query the BioSCape: AVIRIS-NG L3 Resampled Reflectance Mosaics, V2 data to discover files within the spatial and temporal subset of interest.\n\nrecall we discovered in our Earthdata Search investigation that datasets have NASA Earthdata Unique Identifiers (e.g. DOI, ConceptID, short_name)\nDataset of interest short_name: BioSCape_ANG_V02_L3_RFL_Mosaic_2427\nThe BioSCape Airborne Campaign took place from 2023-10-22 to 2023-11-26\n\n\n# bounding lon, lat as a list of tuples\nbounds = polygon_gdf.geometry.apply(orient, args=(1,))\n# simplifying the polygon to bypass the coordinates \n# limit of the CMR with a tolerance of .01 degrees\nxy = bounds.simplify(0.01).get_coordinates()\n\ndate_range = (\"2023-10-22\", \"2023-11-26\")  \n\nresults = earthaccess.search_data(\n    short_name = 'BioSCape_ANG_V02_L3_RFL_Mosaic_2427', \n    polygon=list(zip(xy.x, xy.y)),\n    temporal = date_range,\n    granule_name=('*AVIRIS-NG_BIOSCAPE_V02_L3*')\n)\nprint(f\"Total granules found: {len(results)}\")\n\nFor our search parameters, let’s explore the granules found\n- Let’s look at the first result\n\nresults[:2]\n\n\nresults[7]\n\nYou can download these files directly to your local machine by clicking on any of the files - We also see that these data are Cloud Hosted: True\n\n\n2.2 Create and Visualize the Bounding Boxes of the subset of files\nFrom each granule, we’ll use the CMR Geometry information to create a plot of the AVIRIS-3 flight lines from our temporal and spatial subset\nBelow, we define two functions to plot the search results over a basemap - Function 1: converts UMM geometry to multipolygons – UMM stands for NASA’s Unified Metadata Model - Function 2: converts the Polygon List [ ] to a geopandas dataframe\n\ndef convert_umm_geometry(gpoly):\n    \"\"\"converts UMM geometry to multipolygons\"\"\"\n    multipolygons = []\n    for gl in gpoly:\n        ltln = gl[\"Boundary\"][\"Points\"]\n        points = [(p[\"Longitude\"], p[\"Latitude\"]) for p in ltln]\n        multipolygons.append(Polygon(points))\n    return MultiPolygon(multipolygons)\n\ndef convert_list_gdf(datag):\n    \"\"\"converts List[] to geopandas dataframe\"\"\"\n    # create pandas dataframe from json\n    df = pd.json_normalize([vars(granule)['render_dict'] for granule in datag])\n    # keep only last string of the column names\n    df.columns=df.columns.str.split('.').str[-1]\n    # convert polygons to multipolygonal geometry\n    df[\"geometry\"] = df[\"GPolygons\"].apply(convert_umm_geometry)\n    # return geopandas dataframe\n    return gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n    \nsubset_gdf = convert_list_gdf(results)\nsubset_gdf.crs\n#subset_gdf.drop('Version', axis=1, inplace=True)\n#subset_gdf.explore(fill=False, tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n\n\n# let's visualize the bounding boxes of the selected files\nsubset_gdf = convert_list_gdf(results)\n\nmapObj = folium.Map(location=[-33.1456, 18.0622], zoom_start=11, control_scale=True)\n#-118.2036, 34.2705\n#folium.GeoJson(gdf).add_to(mapObj)\nsubset_gdf.drop('Version', axis=1, inplace=True)\nfolium.GeoJson(subset_gdf, name=\"SUBSET FLIGHT LINES\", color=\"blue\", style_function=lambda x: {\"fillOpacity\": 0}).add_to(mapObj)\n\nfolium.GeoJson(polygon_gdf, name=\"LA FIRE SUBSET AREA\", color=\"white\", style_function=lambda x: {\"fillOpacity\": 0}).add_to(mapObj)\n#folium.GeoJson(lasubset, name=\"SUBSET FLIGHT LINES\", style_function=lambda x: {\"fillOpacity\": 0}).add_to(mapObj)\n\n# create ESRI satellite base map\nesri = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}'\nfolium.TileLayer(tiles = esri, attr = 'Esri', name = 'Esri Satellite', overlay = False, control = True).add_to(mapObj)\nfolium.LayerControl().add_to(mapObj)\nmapObj\n\nLet’s add the GranuleUR to the visualization of the selected tile bounding boxes\n\n#Visualize the selected tile bounding boxes and the GranuleUR\n#m = AVNG_CP[['fid','geometry']].explore('fid')\nm = subset_gdf[['GranuleUR', 'geometry']].explore('GranuleUR', tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n#explore('LandType', tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\nm\n\nWe have a subset of files of interest for our region of interest. Now let’s see how to access those files",
    "crumbs": [
      "Tutorials",
      "3 NASA - AVIRIS Data - Discovery, Access, and Visualization"
    ]
  },
  {
    "objectID": "nasa/01_AVIRIS-NG_L3_BioSCape.html#step-3.-cloud-based-access-methods",
    "href": "nasa/01_AVIRIS-NG_L3_BioSCape.html#step-3.-cloud-based-access-methods",
    "title": "AVIRIS Data - Discovery, Access, and Visualization",
    "section": "Step 3. Cloud-based Access Methods",
    "text": "Step 3. Cloud-based Access Methods\nDatasets in NASA Earthdata Cloud - NASA Earthdata is in AMAZON AWS us-west-2 region (physically in Oregon) - Most data are in AWS Cloud Data Storage (S3) Buckets in this cloud - Access Earthdata Cloud from another Cloud that is in the same region - the openscapes 2i2c Hub is in that region - Airborne (AVIRIS) files can be big, processinge it in the cloud may be advantageous, saving your local device storage\nRecall from our earthaccess search results, each granule of the dataset has 3 data files: - *_QL.tif - *_RFL.nc - *_UNC.nc\nLet’s use the results from earthaccess API search to display the data_links of the Quick Look geoTIFF files\n\ndef get_s3_links(g, suffix_str):\n    return [i for i in g.data_links(access=\"direct\") if i.endswith(suffix_str)][0]\n\ntif_f = []\nfor g in results:\n    tif_f.append(get_s3_links(g, 'QL.tif'))\ntif_f\n\n\n3.1 Directly Open and Access NASA Earthdata from the AWS S3 Session\nCalling open( ) from the earthaccess API library on an S3FileSystem object… - returns an S3File object, which mimics the standard Python file protocol, allowing you to read and write data to S3 objects - returns a list of file-like objects that can be used to access files hosted on S3 third party libraries like xarray - *_paths contains references to files on the remote filesystem. The ornl-cumulus-prod-public is the S3 bucket in AWS us-west-2 region\n\ngtiff_paths = earthaccess.open(tif_f, provider=\"ORNL_CLOUD\")\n\n\ngtiff_paths\n\nNow that we’ve opened the S3 objects, we can treat them as if they are local files.\nLet’s open and visualize the geoTIFF files using familiar Python packages like rioxarray - First we’ll determine the crs and visualize one file\n\nwith rasterio.open(gtiff_paths[7]) as dataset:\n    # Read the CRS\n    crs = dataset.crs \n    print(f\"The CRS of the GeoTIFF is: {crs}\")\n    print(f\"The type of CRS is: {type(crs)}\")\n\nLet’s add and visualize a plot location. We’ll create a spectral profile of this plot location in a subsequent code block.\n\nterr_lat = -33.1733\nterr_lon = 18.1374\naqua_lat = -33.1925\naqua_lon = 18.1166\n# translate coordinates\nfrom pyproj import Proj\np = Proj(\"EPSG:3857\", preserve_units=False)\nterr_x,terr_y = p(terr_lon, terr_lat)\naqua_x, aqua_y = p(aqua_lon, aqua_lat)\nprint('terr_easting:', terr_x)\nprint('terr_northing', terr_y)\nprint('aqua_easting:', aqua_x)\nprint('aqua_northing', aqua_y)\n\nVisualize the tile geoTIFF and plot location\n\n# Open the GeoTIFF file\nraster = rx.open_rasterio(gtiff_paths[7])\n\n# Plot the RGB image\nplt.figure(figsize=(10, 10))\nraster.plot.imshow(rgb=\"band\")\nplt.scatter(terr_x,terr_y, color='red')\nplt.scatter(aqua_x,aqua_y, color='blue')\nplt.show()\n\nAnd now we’ll merge the 8 files and visualize the data available over our region of interest\n\ndata_arrays = []\nfor g in gtiff_paths:\n    data_arrays.append(rx.open_rasterio(g))\n\nmerged_array = rx.merge.merge_arrays(data_arrays, method='last')\nmerged_array.rio.to_raster('merged_image.tif')\n\n#plt.figure(figsize=(10, 10))\n#merged_array.plot.imshow(rgb='band')\n#plt.show()\n\n\nANG_L3_EPSG = 'EPSG:3857' \nsubset_gdf_9221 = subset_gdf.to_crs(ANG_L3_EPSG)  \nfig, ax = plt.subplots(figsize=(10, 10))\nmerged_array.plot.imshow(rgb='band', ax=ax)\n\nsubset_gdf_9221.plot(ax=ax, facecolor='none', edgecolor='red', alpha=0.3)\nplt.show()\n\n\n\nStep 3.2 List S3 Links for Mosaic AVIRIS-NG 425-band Reflectance netCDF Files\n\nAgain, we’ll use the results from earthaccess API search to display the data_links of the netCDF files\n\n\ndef get_s3_links(g, suffix_str):\n    return [i for i in g.data_links(access=\"direct\") if i.endswith(suffix_str)][0]\n\nrfl_f = []\nfor g in results:\n    rfl_f.append(get_s3_links(g, 'RFL.nc'))\nrfl_f\n\nRecall that these are Multifile Granules with 3 files per Granule. We’ve selected just the netCDF files in granule_arr\n\n\nStep 3.3 Directly Open, Access, and Visualize AVIRIS-NG Mosaic Data from the AWS S3 Session\nUsing xarray and the earthaccess.open function we can directly read from a remote filesystem, but not download a file.\n\npaths = earthaccess.open(rfl_f, provider=\"ORNL_CLOUD\")\n\n\npaths\n\n\nds_set = xr.open_dataset(paths[7], engine=\"h5netcdf\")\nds_set\n\nNotice that this xarray.Dataset is limited in what is showing and has no variables.\nThe netCDF data model for these data includes multi-group hierarchies within a single file where each group maps to an xarray.Dataset - In xarray, it is recommended to use DataTree to represent hierarchical data &gt;netCDF groups can only be loaded individually as Dataset objects, a whole file of many nested groups can be loaded as a single xarray.DataTree object. To open a whole netCDF file as a tree of groups use the xarray.open_datatree() function. - This implementation in XArray is decribed here: https://docs.xarray.dev/en/stable/user-guide/io.html\n\nds = xr.open_datatree(paths[7], engine=\"h5netcdf\")\nds\n\nNow we see that the netCDF files contains Groups (3) - reflectance - obs - scene info\nWe’ll open the file again as a datatree, and then convert it to a dataset with the reflectance variable\n\nrfl_ds = xr.open_datatree(paths[7], \n                              engine='h5netcdf').reflectance.to_dataset()\nrfl_ds\n\nNext, we’ll subset the wavelenghts that correspond to RGB bands and visualize the true color image with holoview\n\nds_rgb = rfl_ds.reflectance.sel(wavelength=[637, 552, 462], method=\"nearest\")\n\n\nds_rgb.hvplot.rgb('easting', 'northing', rasterize=True,robust=True, data_aspect=1, aspect='equal', \n                  bands='wavelength', frame_width=600)\n\n\n#### false color composite\n#ds_fcc = rfl_ds.reflectance.sel(wavelength=[800, 637, 552], method=\"nearest\")\n#ds_fcc.hvplot.rgb('easting', 'northing', rasterize=True,robust=True, data_aspect=1, aspect='equal', \n#                  bands='wavelength', frame_width=600)\n\n\n#### The Minimum Noise Fraction transformation (MNF) composite images (B456, B546, and B561), can be used to enhance the delineation of different rock types\n#ds_mnf = rfl_ds.reflectance.sel(wavelength=[456, 546, 561], method=\"nearest\")\n#ds_mnf.hvplot.rgb('easting', 'northing', rasterize=True,robust=True, data_aspect=1, aspect='equal', \n#                  bands='wavelength', frame_width=600)\n\n\n\nStep 3.4 Plot Terrestrial and Aquatic Plots Spectral Profiles\nConvert the latitude longitude plots to the AVIRIS-NG netCDF files projection\n\n#latitude = -33.1733\n#longitude = 18.1374\n# translate coordinates\n#from pyproj import Proj\np = Proj(\"EPSG:9221\", preserve_units=False)\nterr_x,terr_y = p(terr_lon, terr_lat)\nprint('terr_easting:',terr_x)\nprint('terr_northing', terr_y)\naqua_x,aqua_y = p(aqua_lon, aqua_lat)\nprint('aqua_easting:', aqua_x)\nprint('aqua_northing', aqua_y)\n\nDefine a list of bands that are atmospheric windows to avoid in plotting\n\n# Define a list of wavelengths that are \"bad\" \nbblist = np.ones((425,))  # create a 1D array with values ones\n# set tails and atmospheric window to zero\nbblist[0:14] = 0        # tail\nbblist[193:210] = 0     # atmospheric window\nbblist[281:320] = 0     # atmospheric window\nbblist[405:] = 0        # tail\n\nSelect and plot the spectral profiles from the terrestrial and aquatic plot locations nearest pixel\n\n# Compare spectra from a terrestrial and aquatic plot\nterr_plot = rfl_ds.reflectance.sel(easting=terr_x, northing=terr_y, method='nearest')\nterr_plot[bblist == 0] = np.nan\naqua_plot = rfl_ds.reflectance.sel(easting=aqua_x, northing=aqua_y, method='nearest')\naqua_plot[bblist == 0] = np.nan\n\nterr_plot.plot.line(ylim=(0,.4), color = 'g', label=\"Terrestrial Plot\")\naqua_plot.plot.line(ylim=(0,.4),color = 'b', label=\"Aquatic Plot\")\n\nplt.rcParams['figure.figsize'] = [10,7]\nplt.xlabel('AVIRIS-NG wavelength', fontsize=12)\nplt.ylabel('reflectance', fontsize=12)\nplt.legend(loc=\"upper left\")\nplt.show()\n\nThese next block of code will merge the selected files. We will not run this during the workshop. uncomment to run.\n\n#s3_obj = []\n#for fh in paths:\n#    s3_obj.append(xr.open_datatree(fh, engine='h5netcdf', \n#                                  ).reflectance.to_dataset())\n#ds = xr.combine_by_coords(s3_obj, combine_attrs='override')",
    "crumbs": [
      "Tutorials",
      "3 NASA - AVIRIS Data - Discovery, Access, and Visualization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "",
    "text": "NASA’s airborne science program and the NSF-funded National Ecological Observatory Network’s (NEON’s) Airborne Observation Platform (AOP) offer complementary remote sensing datasets ideal for carrying out large-scale ecological research. Both facilities operate similar airborne imaging spectrometers to collect visible to shortwave infrared (VSWIR) hyperspectral data, supporting regional ecosystem studies.\nThe Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC) archives data from NASA-funded ecological campaigns focusing on diverse environments such as river deltas and wetlands, the arctic, and tropics across multiple continents (North and Central America, South Africa). NEON’s AOP gathers high-resolution hyperspectral imagery, lidar, and RGB photography at 81 U.S. sites, offering repeat data spanning 2-10 years, with collections starting in 2013.\nThis workshop introduces NEON and NASA airborne and field datasets through live-coding exercises presented as Python Jupyter Notebook tutorials, demonstrating data access, exploration, and analysis. Participants will learn to apply these datasets to answer ecological research questions, gaining insights into regional and landscape areas of interest.\nThis workshop is hosted by National Ecological Observatory Network (NEON), NASA Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC) with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance.",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n8:00 AM\nIntroduction: NEON and NASA Airborne and Field Data\nBridget Hass and Michele Thornton\n\n\n8:05 AM\nOverview of NEON Airborne Observation Platform\nBridget Hass\n\n\n8:30 AM\nNEON Notebooks: Reflectance Visualization and Classification\nBridget Hass\n\n\n9:30 AM\nBreak\n\n\n\n9:35 AM\nOverview of Recent NASA Airborne Missions (SHIFT, BioSCape, AVUELO)\nMichele Thornton\n\n\n10:00 AM\nNASA Notebooks: Discovery and Analysis of VegPlot and AVIRIS Instrument Data\nMichele Thornton\n\n\n10:55 AM\nDiscussion and Wrap Up\nAll",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "Contact Info",
    "text": "Contact Info\n\nNEON AOP\n\n\n\nOrganization\nNational Ecological Observatory Network Airborne Observation Platform (NEON AOP)1\n\n\nWebsite\nhttps://neonscience.org/\n\n\nContact\nhttps://www.neonscience.org/about/contact-us\n\n\n\n1NEON is a project fully funded by the National Science Foundation and operated by Battelle.\n\n\nORNL DAAC\n\n\n\nOrganization\nNASA Earthdata Data Center, Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)\n\n\nWebsite\nhttps://www.earthdata.nasa.gov/centers/ornl-daac\n\n\nContact\nNASA Earthdata Forum: https://forum.earthdata.nasa.gov, ORNL DAAC - uso@daac.ornl.gov",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "background/neon_background.html",
    "href": "background/neon_background.html",
    "title": "NEON Airborne and Field Datasets",
    "section": "",
    "text": "NEON is a continental-scale observation facility designed to collect long-term open-access ecological data to better understand the complexities of Earth’s ecosystems and how they are changing. NEON uses cutting-edge sensor networks, instrumentation, observational sampling, natural history archive facilities and remote sensing methods and technologies to collect data on plants, animals, soil, nutrients, freshwater and the atmosphere.\nNEON operates 81 field sites strategically located across 20 ecoclimatic Domains across the United States, including 47 terrestrial sites and 34 freshwater aquatic sites. When logistically possible, aquatic and terrestrial field sites are colocated (i.e. in close proximity) to support understanding of linkages across terrestrial and aquatic ecosystems and their interactions with the atmosphere. For example, Domain 08, the Ozarks Complex, has three co-located sets of terrestrial and aquatic field sites. These sites are situated along the same watershed system, creating a unique opportunity to study hydrology, nutrient transport, and biogeochemical cycling through the watershed.\n\n\n\nNEON Field Sites Map; Green: Terrestrial Sites, Blue: Aquatic Sites",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#what-is-neon",
    "href": "background/neon_background.html#what-is-neon",
    "title": "NEON Airborne and Field Datasets",
    "section": "",
    "text": "NEON is a continental-scale observation facility designed to collect long-term open-access ecological data to better understand the complexities of Earth’s ecosystems and how they are changing. NEON uses cutting-edge sensor networks, instrumentation, observational sampling, natural history archive facilities and remote sensing methods and technologies to collect data on plants, animals, soil, nutrients, freshwater and the atmosphere.\nNEON operates 81 field sites strategically located across 20 ecoclimatic Domains across the United States, including 47 terrestrial sites and 34 freshwater aquatic sites. When logistically possible, aquatic and terrestrial field sites are colocated (i.e. in close proximity) to support understanding of linkages across terrestrial and aquatic ecosystems and their interactions with the atmosphere. For example, Domain 08, the Ozarks Complex, has three co-located sets of terrestrial and aquatic field sites. These sites are situated along the same watershed system, creating a unique opportunity to study hydrology, nutrient transport, and biogeochemical cycling through the watershed.\n\n\n\nNEON Field Sites Map; Green: Terrestrial Sites, Blue: Aquatic Sites",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#neon-airborne-observation-platform-aop",
    "href": "background/neon_background.html#neon-airborne-observation-platform-aop",
    "title": "NEON Airborne and Field Datasets",
    "section": "NEON Airborne Observation Platform (AOP)",
    "text": "NEON Airborne Observation Platform (AOP)\n\n\n\nNEON Airborne Remote Sensing\n\n\nAirborne remote sensing surveys are conducted over NEON field sites during peak greenness and provide quantitative information on land cover and changes to ecological structure and chemistry, including the presence and effects of invasive species. The surveys are supported by the NEON Airborne Observation Platform (AOP), a suite of earth observation instruments installed into a Twin Otter aircraft designed to collect high-resolution remote sensing data at low altitude. AOP was designed to collect regional-scale landscape information at the NEON field sites. The AOP maps areas where NEON’s observational and instrumented sampling is occurring and allows relationships to be drawn between NEON’s detailed in-situ observations to the broader environmental and ecological conditions.\n\nAOP Payload Sensors\nThe AOP consists of three complete and comparable instrument payloads. Typically, two of the payloads are dedicated to collections of the NEON field sites while the third is dedicated to NEON’s Research Support services which support externally driven research. The primary sensors on each payload include\n\nA discrete and full-waveform lidar to provide three-dimensional structural information of the landscape,\nAn imaging spectrometer to allow discrimination of land cover types and chemical content of vegetation,\nA high-resolution digital camera to provide spatially accurate and detailed contextual information, and\nA GPS antenna and receiver and Inertial Measurement Unit (IMU) to provide high-accuracy positioning and orientation of the aircraft.\n\n\n\nAOP Data Products\nThe AOP produces approximately 30 data products. The products are separated into categories of Level 1, Level 2, and Level 3 (L1, L2, L3). L1 represents the least processed data products. Additional processing steps are required to transition the L1 data to the derived L2 and L3 data. Broadly, the L1 and L2 products are provided by individual aircraft flight line, while L3 products are provided in 1 km by 1 km tiles. Generally, the data volume for L1 products is the highest and decreases for L2 and L3 products. Details of the different products within each Level can be found in the individual webpages for each sensor. All AOP data products can be found on the NEON Data Portal, and a subset of the L3 data products are available on Google Earth Engine.\n\nImaging Spectrometer Data Products\nLevel 1 (L1) products include at-sensor radiance and surface reflectance which are distributed by flightline. The image data is georeferenced to the ITRF00 datum and projected into the appropriate UTM zone, and provided at 1 m spatial resolution. Both the radiance and reflectance image data are stored in an HDF5 file format that includes extensive metadata and data quality information. The HDF5 format was selected because of the flexibility it allows in storing associated metadata.\nLevel 2 (L2) products are derived from the L1 surface reflectance and are produced at the same spatial resolution (1 m), datum and map projection as the Level 1 products. The L2 products include a suite of spectral indices designed to strategically combine bands to highlight vegetation characteristics such as photosynthetic activity or water content. For example, NDVI (Normalized Difference Vegetation Index) is a well-known and commonly used vegetation index which combines information from the NIR and Red regions to estimate vegetative greenness and can be used as a proxy for plant health. The L2 products also include fPAR (fraction of photosynthetically active radiation) and LAI (leaf area index), products further derived from vegetation indices. Additionally, a surface Albedo product that estimates the integrated reflectance of all the NIS bands into a single value is also provided. All L2 products are distributed by flightline in a GeoTIFF (gtiff) format. Currently, all vegetation indices, water indices, fPAR, and LAI are delivered with associated simulated error images.\nLevel 3 (L3) products include mosaics of all L1 and L2 products, excluding at-sensor radiance, and are distributed as 1 km x 1 km tiles instead of flightlines. Tiles are created by making a full mosaic of all the data and sub-setting the 1 km x 1 km tiles. The tiles are designed so their boundaries are set to even 1000 m UTM coordinate intervals. During the mosaic generation, the algorithm preferentially selects pixels that were collected under the best weather conditions in regions with multiple potential pixels due to flightline overlap. If weather conditions were equivalent, pixels acquired nearest to nadir of the image acquisition are selected. Generally, this will correspond to pixels that are nearest to the center of the flightline. The tiles are created at the same spatial resolution (1 m) as the L1 and L2 products are in delivered in gtiff format, with the exception of the surface reflectance, which is delivered in HDF5 format.\n\nBRDF and topographic corrections\nStarting in 2024, NEON began producing BRDF (Bidirectional Reflectance Distribution Function) and topographic corrected reflectance data, which include “bidirectional” in the name, and end with revision .002 in the Data Product IDs. As of 2025, these bidirectional reflectance are currently only available for data collected between 2022-2024. NEON is beginning to back-process earlier years (pre-2022) to apply the BRDF and topographic corrections. Please look at the data availability charts for each product on the data portal to determine whether the bidirectional data are available. Eventually, only bidirectional data products will be delivered, with the exception of the Level 1 Spectrometer orthorectified surface directional reflectance (DP1.30006.001), which will continue to be delivered, so that researchers who wish to carry out their own BRDF, topographic, or other corrections may do so.\nTable 1 below shows a full list of NEON’s spectrometer-derived data products, including the corresponding bidirectional reflectance data products, if applicable.\n\n\n\nTable 1: NEON AOP Imaging Spectrometer Datasets\n\n\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nBRDF-Corrected DPID\n\n\n\n\nSpectrometer orthorectified at-sensor radiance\nL1\nDP1.30008.001\n\n\n\nSpectrometer orthorectified surface (bi)directional reflectance\nL1\nDP1.30006.001\nDP1.30006.002\n\n\nAlbedo - spectrometer - flightline\nL2\nDP2.30011.001\nDP2.30011.002\n\n\nLAI - spectrometer - flightline\nL2\nDP2.30012.001\nDP2.30012.002\n\n\nfPAR - spectrometer - flightline\nL2\nDP2.30014.001\nDP2.30014.002\n\n\nCanopy water indices - flightline\nL2\nDP2.30019.001\nDP2.30019.002\n\n\nVegetation indices - spectrometer - flightline\nL2\nDP2.30026.001\nDP2.30026.002\n\n\nAlbedo - spectrometer - mosaic\nL3\nDP3.30011.001\nDP3.30011.002\n\n\nLAI - Spectrometer - mosaic\nL3\nDP3.30012.001\nDP3.30012.002\n\n\nfPAR - spectrometer - mosaic\nL3\nDP3.30014.001\nDP3.30014.002\n\n\nCanopy water indices - mosaic\nL3\nDP3.30019.001\nDP3.30019.002\n\n\nVegetation indices - spectrometer - mosaic\nL3\nDP3.30026.001\nDP3.30026.002\n\n\n\n\n\n\nIn addition to the spectrometer-derived data products, NEON generates 5 lidar-derived products (Table 2) and 2 RGB camera data products (Table 3), summarized below. These data products provide valuable structural and visual information that compliment the spectrometer data.\n\n\n\nLiDAR Data Products\n\n\n\nTable 2: NEON AOP Lidar Datasets\n\n\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nATBD Document #\n\n\n\n\nLiDAR Slant Range Waveform\nL1\nDP1.30001.001\nNEON.DOC.001293\n\n\nDiscrete Return LiDAR Point Cloud\nL1\nDP1.30003.001\nNEON.DOC.001292, NEON.DOC.001288\n\n\nEcosystem Structure\nL3\nDP3.30015.001\nNEON.DOC.002387\n\n\nElevation – LiDAR\nL3\nDP3.30024.001\nNEON.DOC.002390\n\n\nSlope and Aspect – LiDAR\nL3\nDP3.30025.001\nNEON.DOC.003791\n\n\n\n\n\n\n\n\nRGB Camera Products\n\n\n\nTable 3: NEON AOP Camera Datasets\n\n\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nATBD Document #\n\n\n\n\nHigh-resolution orthorectified camera imagery\nL1\nDP1.30010.001\nNEON.DOC.001211vB\n\n\nHigh-resolution orthorectified camera imagery mosaic\nL3\nDP3.30010.001\nNEON.DOC.005052vB",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#neon-field-data",
    "href": "background/neon_background.html#neon-field-data",
    "title": "NEON Airborne and Field Datasets",
    "section": "NEON Field Data",
    "text": "NEON Field Data\nIn addition to the AOP remote sensing data, NEON also provides Observational Sampling (OS) data and Instrumented Sampling (IS) data at terrestrial and aquatic sites. The field and instrumented sampling are briefly described below, but we encourage exploring the NEON website further for a more detailed understanding of the sensors and data products provided by the OS and IS groups.\n\nObservational Sampling\n\n\n\nNEON Observational Samples\n\n\nNEON field scientists collect a broad variety of observations and samples at terrestrial and aquatic field sites at regular intervals throughout the year. The data and samples collected by NEON’s Aquatic Observation System (AOS) and Terrestrial Observation System (TOS) are designed to provide standardized, continentally distributed observations of organisms, biogeochemistry, and physical properties.\n\n\nInstrumented Sampling\n\n\n\nNEON Instrumented Sampling\n\n\nNEON deploys automated instruments to collect meteorological, soil, phenological, surface water, and groundwater data at NEON field sites.\nWhere logistically possible, NEON colocated aquatic sites with terrestrial sites (21 in total) to support an understanding of linkages across atmospheric, terrestrial, and aquatic ecosystems. The suite of OS, IS, and AOP data provide an unparalleled opportunity to study ecosystem-level change over time in the United States.",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html",
    "href": "background/nasa_airborne_background.html",
    "title": "NASA Airborne and Field Datasets",
    "section": "",
    "text": "NASA has operated several airborne remote sensing platforms, including AVIRIS, HyTES (Hyperspectral Thermal Emissions Spectrometer), and MASTER (MODIS/ASTER Airborne Simulator). This workshop focuses on data from the AVIRIS sensors, which are the same family the NEON Imaging Spectrometers (AVIRIS-NG).",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-c-datasets",
    "href": "background/nasa_airborne_background.html#aviris-c-datasets",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS-C Datasets",
    "text": "AVIRIS-C Datasets\nThe AVIRIS-C is an imaging spectrometer that delivers calibrated images of the upwelling spectral radiance in 224 contiguous spectral channels with wavelengths from 400 to 2500 nanometers (nm).\n\n\n\nData Product\n\n\n\n\nL1B Calibrated Radiance, Facility Instrument Collection\n\n\nL2 Calibrated Reflectance, Facility Instrument Collection",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-ng-datasets",
    "href": "background/nasa_airborne_background.html#aviris-ng-datasets",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS-NG Datasets",
    "text": "AVIRIS-NG Datasets\nThe AVIRIS-NG is the successor to AVIRIS-Classic and provides high signal-to-noise ratio imaging spectroscopy measurements in 425 contiguous spectral channels with wavelengths in the solar reflected spectral range (380-2510 nm) with 5 nm sampling. The AVIRIS-NG started operation in 2014 and is expected to replace the AVIRIS-C instrument.\n\n\n\nData Product\n\n\n\n\nL1B Calibrated Radiance, Facility Instrument Collection\n\n\nL2 Surface Reflectance, Facility Instrument Collection",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-3-datasets",
    "href": "background/nasa_airborne_background.html#aviris-3-datasets",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS-3 Datasets",
    "text": "AVIRIS-3 Datasets\nThe AVIRIS-3 is the third of the AVIRIS spectrometer FI series and has higher signal-to-noise ratio performance than AVIRIS-C or AVIRIS-NG. The core spectrometer of AVIRIS-3 is an optically fast, F/1.8 Dyson imaging spectrometer spanning a wide width (39.5-degree field of view). The AVIRIS-3 provides measurements in 285 contiguous spectral channels with wavelengths in the solar reflected spectral range (390-2500 nm) with 7.4 nm sampling. The AVIRIS-3 started operation in 2023.\n\n\n\nData Product\n\n\n\n\nL1B Calibrated Radiance, Facility Instrument Collection\n\n\nL2A Orthocorrected Surface Reflectance, Facility Instrument Collection\n\n\nL2B Greenhouse Gas Enhancements, Facility Instrument Collection",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-data-tutorials",
    "href": "background/nasa_airborne_background.html#aviris-data-tutorials",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS Data Tutorials",
    "text": "AVIRIS Data Tutorials\n\nAVIRIS Data - Discovery, Access and Analysis",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html",
    "href": "background/nasa_neon_comparison.html",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "",
    "text": "While NASA and NEON operate similar imaging spectrometer instruments, there are a number of differences between the two in terms of the goals driving the airborne campaigns as well as the datasets provided. This section provides a high-level overview of some of the similarities and differences between the two. Table 1 shows some of the differences in terms of the major questions (when, where, why) about the two missions. Table 2 compares some of the differences in terms of the data products, processing methods, and data management (storage and access).\nWhile both NEON and NASA provide free, open datasets, the major difference between the NEON and NASA remote sensing airborne campaigns is related to their missions. NEON’s overarching mission is to produce a long-term archive of standardized data over the same sites over a 30 year time period, in order to provide a picture of long-term ecological change. NASA’s missions are campaign-driven, meaning that each campaign is typically designed around a specific research question, or instrument testing in some cases. For a more complete list of the NASA airborne campaigns hosted by the ORNL DAAC, refer to Datasets from Airborne Campaigns. Despite the differences in the drivers of the missions, the NEON and NASA datasets provide complimentary hyperspectral data, which could be used together for studies in their own right. In addition, both field and airborne datasets provide ground-truth (or close to ground-truth) sources that can help calibrate and validate data acquired from NASA’s upcoming Surface Biology and Geology SBG satellite mission.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-products",
    "href": "background/nasa_neon_comparison.html#data-products",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "Data Products",
    "text": "Data Products\nNEON data are processed to Level 1 (flightline), Level 2 (derived; flightline) and Level 3 (derived; tiled mosaic) data products, and include a number of derived data products such as vegetation and water indices, LAI, fPAR, and Albedo. NASA data are processed to Level 1 (flightline) radiance, however some missions (e.g. BioSCape) have been mosaicked and tiled.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-processing",
    "href": "background/nasa_neon_comparison.html#data-processing",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "Data Processing",
    "text": "Data Processing\n\nAtmospheric correction\nNEON uses ATCOR-4 for the atmospheric correction. NASA uses ISOFIT. There may be differences data derived from NEON v. NASA due to the different atmospheric correction method applied, as well as other corrections. NEON and NASA have been in communication about using the same atmospheric correction algorithm (ISOFIT) but this is still being scoped out.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-storage-and-access",
    "href": "background/nasa_neon_comparison.html#data-storage-and-access",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "Data Storage and Access",
    "text": "Data Storage and Access\n\nNEON\nNEON data are stored on Google Cloud Storage (GCS) and are accessible via the NEON Data Portal. A subset of the L3 data products are also available on Google Earth Engine.\nNEON provides an API for downloading from the Data Portal, and has developed tools in R (neonUtilities) and Python (neonutilities) for downloading NEON data, and wrangling OS and IS data.\n\n\nNASA\nNASA airborne data are stored on Amazon Web Services (AWS) and can be accessed through the ORNL DAAC. NASA provides tools including Earthdata Search as well as the Python earthaccess package to help data users discover and download datasets.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "NEON’s Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.\n\n\n\n\nBe respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.\n\n\n\n\nThe following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.\n\n\n\n\nIf you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.\n\n\n\nViolations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.",
    "crumbs": [
      "Code of Conduct",
      "NEON's Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-commitment",
    "href": "CODE_OF_CONDUCT.html#our-commitment",
    "title": "NEON’s Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.",
    "crumbs": [
      "Code of Conduct",
      "NEON's Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#expected-behavior",
    "href": "CODE_OF_CONDUCT.html#expected-behavior",
    "title": "NEON’s Code of Conduct",
    "section": "",
    "text": "Be respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.",
    "crumbs": [
      "Code of Conduct",
      "NEON's Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "href": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "title": "NEON’s Code of Conduct",
    "section": "",
    "text": "The following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.",
    "crumbs": [
      "Code of Conduct",
      "NEON's Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#reporting-violations",
    "href": "CODE_OF_CONDUCT.html#reporting-violations",
    "title": "NEON’s Code of Conduct",
    "section": "",
    "text": "If you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.",
    "crumbs": [
      "Code of Conduct",
      "NEON's Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "NEON’s Code of Conduct",
    "section": "",
    "text": "Violations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.",
    "crumbs": [
      "Code of Conduct",
      "NEON's Code of Conduct"
    ]
  },
  {
    "objectID": "instructor-bios.html",
    "href": "instructor-bios.html",
    "title": "Workshop Instructors",
    "section": "",
    "text": "Bridget is a Data Scientist at NEON and splits her time between the Airborne Observation Platform (AOP) and Data Skills teams. She enjoys working on all things related to remote sensing data - including generating the data products, field work, calibration/validation, data management and teaching. Prior to working in remote sensing, she received her M.S. from Oregon State University specializing in marine geophysics, and worked at the Scripps Institution of Oceanography, supporting marine geophysical expeditions.",
    "crumbs": [
      "Welcome",
      "Instructors"
    ]
  },
  {
    "objectID": "instructor-bios.html#bridget-hass-neon",
    "href": "instructor-bios.html#bridget-hass-neon",
    "title": "Workshop Instructors",
    "section": "",
    "text": "Bridget is a Data Scientist at NEON and splits her time between the Airborne Observation Platform (AOP) and Data Skills teams. She enjoys working on all things related to remote sensing data - including generating the data products, field work, calibration/validation, data management and teaching. Prior to working in remote sensing, she received her M.S. from Oregon State University specializing in marine geophysics, and worked at the Scripps Institution of Oceanography, supporting marine geophysical expeditions.",
    "crumbs": [
      "Welcome",
      "Instructors"
    ]
  },
  {
    "objectID": "instructor-bios.html#michele-thornton-ornl-daac",
    "href": "instructor-bios.html#michele-thornton-ornl-daac",
    "title": "Workshop Instructors",
    "section": "Michele Thornton (ORNL DAAC)",
    "text": "Michele Thornton (ORNL DAAC)\nMichele is an ecologist and geospatial professional working at the Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC); part of NASA’s Earth Observing System Data and Information System and the ORNL Biological and Environmental Systems Science Directorate. Michele has served as the Project Lead for the production and tool development and distribution of the Daymet dataset since 2009. She also provides geospatial technical and scientific support for datasets curated and distributed within the ORNL DAAC. Michele enjoys working with the NASA and ORNL Science Community to both provide data that are useful in their applications and to help with the long term curation of terrestrial ecology datasets.",
    "crumbs": [
      "Welcome",
      "Instructors"
    ]
  },
  {
    "objectID": "instructor-bios.html#rupesh-shrestha-ornl-daac",
    "href": "instructor-bios.html#rupesh-shrestha-ornl-daac",
    "title": "Workshop Instructors",
    "section": "Rupesh Shrestha (ORNL DAAC)",
    "text": "Rupesh Shrestha (ORNL DAAC)\nRupesh is a research scientist at the ORNL DAAC. Rupesh has an extensive background in remote sensing applied to vegetation measurements, cyber-infrastructure development, and data science. Rupesh has led the development of several tools, algorithms, and data services that enable the estimation of earth science observables from satellite, airborne, and ground-based remote sensing. Rupesh holds a Ph.D. from Virginia Tech and M.S. in Photogrammetry and Geoinformatics from Stuttgart, Germany. He also serves as an adjunct faculty in the Department of Forestry, Wildlife & Fisheries at the University of Tennessee, Knoxville.",
    "crumbs": [
      "Welcome",
      "Instructors"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "",
    "text": "In this notebook, we will use existing data of verified land cover and alien species locations to extract spectra from AVIRIS NG surface reflectance data.\nThis Notebook borrows heavily from an earlier BioSCape Workshop Notebook, but with updates to reading and analysis of AVIRIS-NG netCDF file formats.\n\n\n\nUnderstand how to inspect and prepare data for machine learning models\nTrain and interpret a machine learning model\nApply a trained model to AVIRIS imagery to create alien species maps",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#overview",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#overview",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "",
    "text": "In this notebook, we will use existing data of verified land cover and alien species locations to extract spectra from AVIRIS NG surface reflectance data.\nThis Notebook borrows heavily from an earlier BioSCape Workshop Notebook, but with updates to reading and analysis of AVIRIS-NG netCDF file formats.\n\n\n\nUnderstand how to inspect and prepare data for machine learning models\nTrain and interpret a machine learning model\nApply a trained model to AVIRIS imagery to create alien species maps",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#load-python-modules",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#load-python-modules",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "Load Python Modules",
    "text": "Load Python Modules\n\n# Modules imported separately - not available at the time of the workshop managed environment \n# !pip3 install --user xvec\n# !pip3 install --user shap\n# !pip3 install --user xgboost\n\n\nfrom os import path\nimport datetime as dt \nimport geopandas as gpd\nimport s3fs\nimport pandas as pd\nimport xarray as xr\nfrom shapely.geometry import box, mapping, Polygon, MultiPolygon\nimport rioxarray as riox\nimport numpy as np\nimport netCDF4 as nc\nimport hvplot.xarray\nimport holoviews as hv\nimport xvec\nimport shap\nimport xgboost\nimport matplotlib.pyplot as plt\nfrom dask.diagnostics import ProgressBar\nimport warnings\nimport earthaccess\n#our functions\nfrom utils import get_first_xr\n\nwarnings.filterwarnings('ignore')\nhvplot.extension('bokeh')",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#explore-sample-land-type-plot-level-data",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#explore-sample-land-type-plot-level-data",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "Explore Sample Land Type Plot-Level Data",
    "text": "Explore Sample Land Type Plot-Level Data\nFor plot-level training data, we will use a small dataset over the Cape Town Peninsula of South Africa of manually collected invasive plant and land cover label - ct_invasive.gpkg\n\n# let's first create a DataFrame and assign labels to each class\n\nlabel_df = pd.DataFrame({'LandType': ['Bare ground/Rock','Mature Fynbos', \n              'Recently Burnt Fynbos', 'Wetland', \n              'Forest', 'Pine', 'Eucalyptus' , 'Wattle', 'Water'],\n               'class': ['0','1','2','3','4','5','6','7','8']})\n\nlabel_df\n\n\n\n\n\n\n\n\nLandType\nclass\n\n\n\n\n0\nBare ground/Rock\n0\n\n\n1\nMature Fynbos\n1\n\n\n2\nRecently Burnt Fynbos\n2\n\n\n3\nWetland\n3\n\n\n4\nForest\n4\n\n\n5\nPine\n5\n\n\n6\nEucalyptus\n6\n\n\n7\nWattle\n7\n\n\n8\nWater\n8\n\n\n\n\n\n\n\n\n# Now let's open the dataset, project to the South African UTM projection also used by the AVIRIS-NG airborne data, and merge it with the label data frame created above.\nclass_data = gpd.read_file('~/shared-public/data/ct_invasive.gpkg')\n# class_data.crs\nclass_data_utm = (class_data\n                 .to_crs(\"EPSG:32734\")\n                 .merge(label_df, on='class', how='left')\n                 )\nclass_data_utm\n\n\n\n\n\n\n\n\nclass\ngroup\ngeometry\nLandType\n\n\n\n\n0\n0\n2\nPOINT (264211.767 6198008.161)\nBare ground/Rock\n\n\n1\n0\n1\nPOINT (257698.443 6240453.544)\nBare ground/Rock\n\n\n2\n0\n1\nPOINT (260264.755 6239108.581)\nBare ground/Rock\n\n\n3\n0\n1\nPOINT (258793.622 6239111.697)\nBare ground/Rock\n\n\n4\n0\n1\nPOINT (261938.819 6238865.583)\nBare ground/Rock\n\n\n...\n...\n...\n...\n...\n\n\n314\n7\n2\nPOINT (262122.203 6228661.57)\nWattle\n\n\n315\n3\n1\nPOINT (262105.078 6228827.113)\nWetland\n\n\n316\n3\n1\nPOINT (262316.257 6228870.308)\nWetland\n\n\n317\n7\n1\nPOINT (262822.284 6241735.006)\nWattle\n\n\n318\n7\n1\nPOINT (262935.024 6241659.615)\nWattle\n\n\n\n\n319 rows × 4 columns",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#summarize-and-visualize-the-land-types",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#summarize-and-visualize-the-land-types",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "Summarize and Visualize the Land Types",
    "text": "Summarize and Visualize the Land Types\n\n# examine the number of plots of each land type\nclass_data_utm.groupby(['LandType']).size()\n\nLandType\nBare ground/Rock         32\nEucalyptus               30\nForest                   34\nMature Fynbos            44\nPine                     36\nRecently Burnt Fynbos    32\nWater                    32\nWattle                   24\nWetland                  55\ndtype: int64\n\n\n\n# The group class will be used to determine training and test data\nclass_data_utm.groupby(['group']).size()\n\ngroup\n1    254\n2     65\ndtype: int64\n\n\n\n# Let's visualize the plot data in an interactive map, with color by class, using a Google satellite basemap\nclass_data_utm[['LandType', 'geometry']].explore('LandType', tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#earthdata-authentication",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#earthdata-authentication",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "Earthdata Authentication",
    "text": "Earthdata Authentication\n\n# ask for EDL credentials and persist them in a .netrc file\nauth = earthaccess.login(strategy=\"interactive\", persist=True)",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#aviris-ng-data-over-cape-town-peninsula",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#aviris-ng-data-over-cape-town-peninsula",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "AVIRIS-NG Data over Cape Town Peninsula",
    "text": "AVIRIS-NG Data over Cape Town Peninsula\n\n# search granules\ngranules = earthaccess.search_data(\n    doi='10.3334/ORNLDAAC/2427', # BioSCape: AVIRIS-NG L3\n    granule_name = \"AVIRIS-NG*\", # exclude geojson tile\n)\n\n\n# print one granule\ngranules[100]\n\n\n    \n            \n            \n    \n      \n        \n          \n            Data: AVIRIS-NG_BIOSCAPE_V02_L3_28_14_QL.tifAVIRIS-NG_BIOSCAPE_V02_L3_28_14_UNC.ncAVIRIS-NG_BIOSCAPE_V02_L3_28_14_RFL.nc\n            Size: 6760.12 MB\n            Cloud Hosted: True\n          \n          \n            \n          \n        \n      \n    \n    \n\n\n\nvars(granules[100])\n\n{'cloud_hosted': True,\n 'uuid': '488868c9-6027-4103-b433-e839d606464c',\n 'render_dict': Collection: {'ShortName': 'BioSCape_ANG_V02_L3_RFL_Mosaic_2427', 'Version': '2'}\n Spatial coverage: {'HorizontalSpatialDomain': {'Geometry': {'GPolygons': [{'Boundary': {'Points': [{'Longitude': 18.8167, 'Latitude': -33.438}, {'Longitude': 18.8225, 'Latitude': -33.3487}, {'Longitude': 18.7143, 'Latitude': -33.3439}, {'Longitude': 18.7085, 'Latitude': -33.4332}, {'Longitude': 18.8167, 'Latitude': -33.438}]}}]}}}\n Temporal coverage: {'RangeDateTime': {'BeginningDateTime': '2023-10-22T00:00:00Z', 'EndingDateTime': '2023-11-26T23:59:59Z'}}\n Size(MB): 6760.123710632324\n Data: ['https://data.ornldaac.earthdata.nasa.gov/protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_28_14_QL.tif', 'https://data.ornldaac.earthdata.nasa.gov/protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_28_14_UNC.nc', 'https://data.ornldaac.earthdata.nasa.gov/protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_28_14_RFL.nc']}\n\n\n\ndef convert_umm_geometry(gpoly):\n    \"\"\"converts UMM geometry to multipolygons\"\"\"\n    ltln = gpoly[0][\"Boundary\"][\"Points\"]\n    return Polygon([(p[\"Longitude\"], p[\"Latitude\"]) for p in ltln])\n\ndef convert_list_gdf(datag):\n    \"\"\"converts List[] to geopandas dataframe\"\"\"\n    # create pandas dataframe from json\n    df = pd.json_normalize([vars(granule)['render_dict'] for granule in datag])\n    # keep only last string of the column names\n    df.columns=df.columns.str.split('.').str[-1]\n    # creates polygon geometry\n    df[\"geometry\"] = df[\"GPolygons\"].apply(convert_umm_geometry)\n    # return geopandas dataframe\n    return gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n    \nAVNG_Coverage = convert_list_gdf(granules)\nAVNG_Coverage[['native-id', 'geometry']].explore(tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#open-a-single-aviris-ng-reflectance-file-to-inspect-the-data",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#open-a-single-aviris-ng-reflectance-file-to-inspect-the-data",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "Open a single AVIRIS-NG Reflectance file to inspect the data",
    "text": "Open a single AVIRIS-NG Reflectance file to inspect the data\n\nxarray is an open source project and Python package that introduces labels in the form of dimensions, coordinates, and attributes on top of raw NumPy-like arrays\n\n\n# search granules that spatially intersects with class_data\nsingle_granule = earthaccess.search_data(\n    doi='10.3334/ORNLDAAC/2427', # BioSCape: AVIRIS-NG L3\n    granule_name = \"AVIRIS-NG_BIOSCAPE_V02_L3_36_11*\", # select only one file\n)\nsingle_granule[0].data_links(access=\"direct\")\n\n['s3://ornl-cumulus-prod-protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_36_11_QL.tif',\n 's3://ornl-cumulus-prod-protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_36_11_UNC.nc',\n 's3://ornl-cumulus-prod-protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_36_11_RFL.nc']\n\n\n\n# earthaccess open\nfh = earthaccess.open(single_granule)\nfh\n\n\n\n\n\n\n\n\n\n\n[&lt;File-like object S3FileSystem, ornl-cumulus-prod-protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_36_11_QL.tif&gt;,\n &lt;File-like object S3FileSystem, ornl-cumulus-prod-protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_36_11_UNC.nc&gt;,\n &lt;File-like object S3FileSystem, ornl-cumulus-prod-protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_36_11_RFL.nc&gt;]\n\n\n\ndef separate_granule_type(fo):\n    \"\"\"separating granules by type\"\"\"\n    return ([f for f in fo if f.path.endswith(\"_RFL.nc\")],\n            [f for f in fo if f.path.endswith(\"_QL.tif\")],\n            [f for f in fo if f.path.endswith(\"_UNC.nc\")])\nrfl_f, ql_f, unc_f = separate_granule_type(fh)\n\n\n# plot a single file netcdf\nrfl_netcdf = xr.open_datatree(rfl_f[0],\n                              engine='h5netcdf', chunks={})\nrfl_netcdf = rfl_netcdf.reflectance.to_dataset()\nrfl_netcdf = rfl_netcdf.reflectance.where(rfl_netcdf.reflectance&gt;0)\nrfl_netcdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'reflectance' (wavelength: 425, northing: 2000, easting: 2000)&gt; Size: 7GB\ndask.array&lt;where, shape=(425, 2000, 2000), dtype=float32, chunksize=(10, 256, 256), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * easting     (easting) float64 16kB 7.9e+05 7.9e+05 7.9e+05 ... 8e+05 8e+05\n  * northing    (northing) float64 16kB 8.3e+05 8.3e+05 ... 8.2e+05 8.2e+05\n  * wavelength  (wavelength) float32 2kB 377.2 382.2 ... 2.496e+03 2.501e+03\nAttributes:\n    _QuantizeBitGroomNumberOfSignificantDigits:  5\n    long_name:                                   Mosaiced Hemispherical Direc...\n    grid_mapping:                                transverse_mercator\n    orthorectified:                              Truexarray.DataArray'reflectance'wavelength: 425northing: 2000easting: 2000dask.array&lt;chunksize=(10, 256, 256), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n6.33 GiB\n2.50 MiB\n\n\nShape\n(425, 2000, 2000)\n(10, 256, 256)\n\n\nDask graph\n2752 chunks in 4 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                         2000 2000 425\n\n\n\n\nCoordinates: (3)easting(easting)float647.9e+05 7.9e+05 ... 8e+05 8e+05description :UTM easting coordinate for center of grid cell for orthocorrected pixel locationstandard_name :projection_x_coordinatelong_name :X coordinate of projectionunits :maxis :Xarray([790000., 790005., 790010., ..., 799985., 799990., 799995.],\n      shape=(2000,))northing(northing)float648.3e+05 8.3e+05 ... 8.2e+05 8.2e+05description :UTM northing coordinate for center of grid cell for orthocorrected pixel locationstandard_name :projection_y_coordinatelong_name :Y coordinate of projectionunits :maxis :Yarray([830000., 829995., 829990., ..., 820015., 820010., 820005.],\n      shape=(2000,))wavelength(wavelength)float32377.2 382.2 ... 2.496e+03 2.501e+03standard_name :radiation_wavelengthlong_name :Wavelength centersunits :nmarray([ 377.19565,  382.20566,  387.21564, ..., 2490.8557 , 2495.8657 ,\n       2500.8757 ], shape=(425,), dtype=float32)Indexes: (3)eastingPandasIndexPandasIndex(Index([790000.0, 790005.0, 790010.0, 790015.0, 790020.0, 790025.0, 790030.0,\n       790035.0, 790040.0, 790045.0,\n       ...\n       799950.0, 799955.0, 799960.0, 799965.0, 799970.0, 799975.0, 799980.0,\n       799985.0, 799990.0, 799995.0],\n      dtype='float64', name='easting', length=2000))northingPandasIndexPandasIndex(Index([830000.0, 829995.0, 829990.0, 829985.0, 829980.0, 829975.0, 829970.0,\n       829965.0, 829960.0, 829955.0,\n       ...\n       820050.0, 820045.0, 820040.0, 820035.0, 820030.0, 820025.0, 820020.0,\n       820015.0, 820010.0, 820005.0],\n      dtype='float64', name='northing', length=2000))wavelengthPandasIndexPandasIndex(Index([ 377.1956481933594,  382.2056579589844, 387.21563720703125,\n       392.22564697265625, 397.22564697265625, 402.23565673828125,\n        407.2456359863281,  412.2556457519531,  417.2656555175781,\n         422.275634765625,\n       ...\n        2455.795654296875,    2460.8056640625,  2465.815673828125,\n        2470.815673828125,   2475.82568359375,  2480.835693359375,\n           2485.845703125,  2490.855712890625,   2495.86572265625,\n        2500.875732421875],\n      dtype='float32', name='wavelength', length=425))Attributes: (4)_QuantizeBitGroomNumberOfSignificantDigits :5long_name :Mosaiced Hemispherical Directional Reflectance Factorgrid_mapping :transverse_mercatororthorectified :True\n\n\n\nPlot just a red reflectance\n\nh = rfl_netcdf.sel({'wavelength': 660},method='nearest').hvplot('easting', 'northing',\n                                                      rasterize=True, data_aspect=1,\n                                                      cmap='magma',frame_width=400,clim=(0,0.3))\nh\n\n\n\n\n\n  \n\n\n\n\n\n\nPlot a quicklook image\n\n# plot a single file geotif\nql_tif = xr.open_dataset(ql_f[0], engine='rasterio')\nh = ql_tif.hvplot.rgb('x', 'y', bands='band', rasterize=True, data_aspect=1, frame_width=400)\nh",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "nasa/02_AVIRIS-NG_L3_invasive_species.html#select-the-aviris-ng-flight-line-data-to-selected-parameters",
    "href": "nasa/02_AVIRIS-NG_L3_invasive_species.html#select-the-aviris-ng-flight-line-data-to-selected-parameters",
    "title": "Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG",
    "section": "Select the AVIRIS-NG Flight Line data to selected parameters",
    "text": "Select the AVIRIS-NG Flight Line data to selected parameters\nFor our analysis demonstration in this Notebook, we’ll narrow the flight lines to the area of the Cape Penisula.\n\n# search granules that spatially intersects with class_data\ngranules_cp = earthaccess.search_data(\n    doi='10.3334/ORNLDAAC/2427', # BioSCape: AVIRIS-NG L3\n    bounding_box = tuple(class_data.total_bounds),\n    granule_name = \"AVIRIS-NG*\", # exclude geojson tile\n)\n\n\nAVNG_Coverage = convert_list_gdf(granules_cp)\nAVNG_Coverage[['native-id', 'geometry']].explore('native-id', tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}', attr='Google')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nExtract Spectra for each Land Plot\nNow that we are familiar with the data, we want to get the AVIRIS spectra at each label location. Below is a function that does this and returns the result as a xarray\n\nf_cp = earthaccess.open(granules_cp)\nrfl_f, ql_f, unc_f = separate_granule_type(f_cp)\nAVNG_CP = AVNG_Coverage.to_crs(\"EPSG:32734\")\nds_all =[]\nfor rfl in rfl_f:\n    native_id = path.basename(rfl.path)[:-7]\n    geof = AVNG_CP[AVNG_CP[\"native-id\"]==native_id].geometry\n    points = class_data_utm.clip(geof)\n    n_points = points.shape[0]\n    if n_points:\n        print(f'got {n_points} point from {native_id}')\n        ds = xr.open_datatree(rfl, engine='h5netcdf', chunks={})\n        points = points.to_crs(ds.transverse_mercator.crs_wkt)\n        ds_all.append(ds.reflectance.to_dataset().xvec.extract_points(points['geometry'], \n                                                                x_coords=\"easting\", \n                                                                y_coords=\"northing\",\n                                                                index=True))\nds_all = xr.concat(ds_all, dim='file')\n\n\n\n\n\n\n\n\n\n\ngot 40 point from AVIRIS-NG_BIOSCAPE_V02_L3_37_11\ngot 3 point from AVIRIS-NG_BIOSCAPE_V02_L3_37_12\ngot 26 point from AVIRIS-NG_BIOSCAPE_V02_L3_38_11\ngot 3 point from AVIRIS-NG_BIOSCAPE_V02_L3_35_10\ngot 11 point from AVIRIS-NG_BIOSCAPE_V02_L3_36_12\ngot 7 point from AVIRIS-NG_BIOSCAPE_V02_L3_39_12\ngot 22 point from AVIRIS-NG_BIOSCAPE_V02_L3_38_12\ngot 67 point from AVIRIS-NG_BIOSCAPE_V02_L3_36_11\ngot 32 point from AVIRIS-NG_BIOSCAPE_V02_L3_34_11\ngot 4 point from AVIRIS-NG_BIOSCAPE_V02_L3_39_11\ngot 104 point from AVIRIS-NG_BIOSCAPE_V02_L3_35_11\n\n\n\nds_all\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 6MB\nDimensions:      (file: 11, wavelength: 425, geometry: 319)\nCoordinates:\n  * wavelength   (wavelength) float32 2kB 377.2 382.2 ... 2.496e+03 2.501e+03\n  * geometry     (geometry) object 3kB POINT (797172.168653522 811982.4351414...\n    index        (file, geometry) float64 28kB 134.0 133.0 132.0 ... 2.0 300.0\nDimensions without coordinates: file\nData variables:\n    fwhm         (file, wavelength) float32 19kB dask.array&lt;chunksize=(1, 425), meta=np.ndarray&gt;\n    reflectance  (file, wavelength, geometry) float32 6MB dask.array&lt;chunksize=(1, 10, 3), meta=np.ndarray&gt;\nIndexes:\n    geometry  GeometryIndex (crs=PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ellipse Based\",SPHE ...)xarray.DatasetDimensions:file: 11wavelength: 425geometry: 319Coordinates: (3)wavelength(wavelength)float32377.2 382.2 ... 2.496e+03 2.501e+03standard_name :radiation_wavelengthlong_name :Wavelength centersunits :nmarray([ 377.19565,  382.20566,  387.21564, ..., 2490.8557 , 2495.8657 ,\n       2500.8757 ], shape=(425,), dtype=float32)geometry(geometry)objectPOINT (797172.168653522 811982.4...crs :PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ellipse Based\",SPHEROID[\"Unnamed\",6378137,298.257223562997]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",-30],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",-22],PARAMETER[\"standard_parallel_2\",-38],PARAMETER[\"false_easting\",1400000],PARAMETER[\"false_northing\",1300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]array([&lt;POINT (797172.169 811982.435)&gt;, &lt;POINT (797083.281 811985.861)&gt;,\n       &lt;POINT (796989.807 812202.856)&gt;, ..., &lt;POINT (795593.279 839426.992)&gt;,\n       &lt;POINT (795464.886 839503.763)&gt;, &lt;POINT (794927.264 839597.826)&gt;],\n      shape=(319,), dtype=object)index(file, geometry)float64134.0 133.0 132.0 ... 2.0 300.0array([[134., 133., 132., ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       ...,\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ...,  nan,  nan,  nan],\n       [ nan,  nan,  nan, ..., 305.,   2., 300.]], shape=(11, 319))Data variables: (2)fwhm(file, wavelength)float32dask.array&lt;chunksize=(1, 425), meta=np.ndarray&gt;long_name :Full width at half maximumunits :nm\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n18.26 kiB\n1.66 kiB\n\n\nShape\n(11, 425)\n(1, 425)\n\n\nDask graph\n11 chunks in 34 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                   425 11\n\n\n\n\nreflectance(file, wavelength, geometry)float32dask.array&lt;chunksize=(1, 10, 3), meta=np.ndarray&gt;_QuantizeBitGroomNumberOfSignificantDigits :5long_name :Mosaiced Hemispherical Directional Reflectance Factorgrid_mapping :transverse_mercatororthorectified :True\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n5.69 MiB\n120 B\n\n\nShape\n(11, 425, 319)\n(1, 10, 3)\n\n\nDask graph\n93654 chunks in 135 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                                                                                                     319 425 11\n\n\n\n\nIndexes: (2)wavelengthPandasIndexPandasIndex(Index([ 377.1956481933594,  382.2056579589844, 387.21563720703125,\n       392.22564697265625, 397.22564697265625, 402.23565673828125,\n        407.2456359863281,  412.2556457519531,  417.2656555175781,\n         422.275634765625,\n       ...\n        2455.795654296875,    2460.8056640625,  2465.815673828125,\n        2470.815673828125,   2475.82568359375,  2480.835693359375,\n           2485.845703125,  2490.855712890625,   2495.86572265625,\n        2500.875732421875],\n      dtype='float32', name='wavelength', length=425))geometryGeometryIndex (crs=PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ellipse Based\",SPHEROID[\"Unnamed\" ...)GeometryIndex(\n    [&lt;POINT (797172.169 811982.435)&gt;\n     &lt;POINT (797083.281 811985.861)&gt;\n     &lt;POINT (796989.807 812202.856)&gt;\n     &lt;POINT (798950.757 814518.901)&gt;\n     ...\n     &lt;POINT (795538.441 839110.696)&gt;\n     &lt;POINT (795593.279 839426.992)&gt;\n     &lt;POINT (795464.886 839503.763)&gt;\n     &lt;POINT (794927.264 839597.826)&gt;],\n    crs=PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ell ...)Attributes: (0)\n\n\n\nds = get_first_xr(ds_all)\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 549kB\nDimensions:      (wavelength: 425, index: 319)\nCoordinates:\n  * wavelength   (wavelength) float32 2kB 377.2 382.2 ... 2.496e+03 2.501e+03\n    geometry     (index) object 3kB POINT (797172.168653522 811982.435141402)...\n  * index        (index) int64 3kB 134 133 132 247 110 109 ... 77 16 305 2 300\nData variables:\n    reflectance  (index, wavelength) float32 542kB dask.array&lt;chunksize=(3, 10), meta=np.ndarray&gt;xarray.DatasetDimensions:wavelength: 425index: 319Coordinates: (3)wavelength(wavelength)float32377.2 382.2 ... 2.496e+03 2.501e+03standard_name :radiation_wavelengthlong_name :Wavelength centersunits :nmarray([ 377.19565,  382.20566,  387.21564, ..., 2490.8557 , 2495.8657 ,\n       2500.8757 ], shape=(425,), dtype=float32)geometry(index)objectPOINT (797172.168653522 811982.4...crs :PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ellipse Based\",SPHEROID[\"Unnamed\",6378137,298.257223562997]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",-30],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",-22],PARAMETER[\"standard_parallel_2\",-38],PARAMETER[\"false_easting\",1400000],PARAMETER[\"false_northing\",1300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]array([&lt;POINT (797172.169 811982.435)&gt;, &lt;POINT (797083.281 811985.861)&gt;,\n       &lt;POINT (796989.807 812202.856)&gt;, ..., &lt;POINT (795593.279 839426.992)&gt;,\n       &lt;POINT (795464.886 839503.763)&gt;, &lt;POINT (794927.264 839597.826)&gt;],\n      shape=(319,), dtype=object)index(index)int64134 133 132 247 ... 16 305 2 300array([134, 133, 132, ..., 305,   2, 300], shape=(319,))Data variables: (1)reflectance(index, wavelength)float32dask.array&lt;chunksize=(3, 10), meta=np.ndarray&gt;_QuantizeBitGroomNumberOfSignificantDigits :5long_name :Mosaiced Hemispherical Directional Reflectance Factorgrid_mapping :transverse_mercatororthorectified :True\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n529.59 kiB\n120 B\n\n\nShape\n(319, 425)\n(3, 10)\n\n\nDask graph\n4601 chunks in 137 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                             425 319\n\n\n\n\nIndexes: (2)wavelengthPandasIndexPandasIndex(Index([ 377.1956481933594,  382.2056579589844, 387.21563720703125,\n       392.22564697265625, 397.22564697265625, 402.23565673828125,\n        407.2456359863281,  412.2556457519531,  417.2656555175781,\n         422.275634765625,\n       ...\n        2455.795654296875,    2460.8056640625,  2465.815673828125,\n        2470.815673828125,   2475.82568359375,  2480.835693359375,\n           2485.845703125,  2490.855712890625,   2495.86572265625,\n        2500.875732421875],\n      dtype='float32', name='wavelength', length=425))indexPandasIndexPandasIndex(Index([134, 133, 132, 247, 110, 109,  25,  50,  87,  86,\n       ...\n        55, 209,   3, 217,  13,  77,  16, 305,   2, 300],\n      dtype='int64', name='index', length=319))Attributes: (0)\n\n\nThis data set just has the spectra. We need to merge with point data to add labels\n\nclass_xr =class_data_utm[['class','group']].to_xarray()\nds = ds.merge(class_xr.astype(int),join='left')\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 554kB\nDimensions:      (wavelength: 425, index: 319)\nCoordinates:\n  * wavelength   (wavelength) float32 2kB 377.2 382.2 ... 2.496e+03 2.501e+03\n  * index        (index) int64 3kB 134 133 132 247 110 109 ... 77 16 305 2 300\n    geometry     (index) object 3kB POINT (797172.168653522 811982.435141402)...\nData variables:\n    reflectance  (index, wavelength) float32 542kB dask.array&lt;chunksize=(3, 10), meta=np.ndarray&gt;\n    class        (index) int64 3kB 6 5 5 8 3 3 1 2 3 3 2 ... 1 1 0 8 0 3 0 0 0 4\n    group        (index) int64 3kB 1 2 1 1 1 1 2 1 1 2 1 ... 1 1 1 1 1 2 1 1 1 1xarray.DatasetDimensions:wavelength: 425index: 319Coordinates: (3)wavelength(wavelength)float32377.2 382.2 ... 2.496e+03 2.501e+03standard_name :radiation_wavelengthlong_name :Wavelength centersunits :nmarray([ 377.19565,  382.20566,  387.21564, ..., 2490.8557 , 2495.8657 ,\n       2500.8757 ], shape=(425,), dtype=float32)index(index)int64134 133 132 247 ... 16 305 2 300array([134, 133, 132, ..., 305,   2, 300], shape=(319,))geometry(index)objectPOINT (797172.168653522 811982.4...crs :PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ellipse Based\",SPHEROID[\"Unnamed\",6378137,298.257223562997]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",-30],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",-22],PARAMETER[\"standard_parallel_2\",-38],PARAMETER[\"false_easting\",1400000],PARAMETER[\"false_northing\",1300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]array([&lt;POINT (797172.169 811982.435)&gt;, &lt;POINT (797083.281 811985.861)&gt;,\n       &lt;POINT (796989.807 812202.856)&gt;, ..., &lt;POINT (795593.279 839426.992)&gt;,\n       &lt;POINT (795464.886 839503.763)&gt;, &lt;POINT (794927.264 839597.826)&gt;],\n      shape=(319,), dtype=object)Data variables: (3)reflectance(index, wavelength)float32dask.array&lt;chunksize=(3, 10), meta=np.ndarray&gt;_QuantizeBitGroomNumberOfSignificantDigits :5long_name :Mosaiced Hemispherical Directional Reflectance Factorgrid_mapping :transverse_mercatororthorectified :True\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n529.59 kiB\n120 B\n\n\nShape\n(319, 425)\n(3, 10)\n\n\nDask graph\n4601 chunks in 137 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                             425 319\n\n\n\n\nclass(index)int646 5 5 8 3 3 1 2 ... 0 8 0 3 0 0 0 4array([6, 5, 5, 8, 3, 3, 1, 2, 3, 3, 2, 2, 7, 7, 2, 3, 4, 8, 1, 2, 1, 2,\n       2, 0, 0, 5, 3, 3, 2, 3, 8, 3, 8, 1, 7, 7, 7, 7, 8, 8, 1, 2, 2, 1,\n       0, 6, 6, 6, 1, 1, 8, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3,\n       2, 2, 2, 1, 1, 1, 8, 8, 8, 3, 3, 8, 3, 8, 3, 3, 3, 0, 0, 1, 1, 0,\n       1, 4, 8, 3, 3, 3, 4, 3, 1, 3, 2, 3, 3, 2, 3, 2, 3, 3, 1, 8, 3, 6,\n       6, 5, 3, 3, 1, 4, 4, 0, 1, 4, 2, 3, 0, 3, 0, 0, 1, 4, 7, 7, 7, 5,\n       5, 3, 2, 2, 1, 5, 7, 7, 3, 5, 5, 3, 8, 3, 3, 3, 8, 8, 4, 4, 0, 5,\n       5, 1, 1, 8, 1, 1, 4, 4, 4, 4, 8, 3, 7, 7, 1, 4, 4, 1, 1, 0, 7, 7,\n       7, 7, 7, 2, 0, 2, 6, 5, 5, 6, 8, 6, 7, 7, 8, 8, 5, 5, 6, 0, 1, 6,\n       6, 8, 0, 6, 6, 0, 6, 6, 6, 6, 6, 8, 8, 3, 4, 4, 4, 1, 1, 5, 7, 0,\n       1, 6, 7, 7, 7, 1, 1, 6, 0, 0, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 3,\n       5, 6, 5, 5, 8, 1, 1, 8, 8, 1, 4, 4, 0, 5, 5, 5, 6, 4, 4, 0, 4, 3,\n       1, 1, 3, 0, 3, 0, 6, 6, 5, 4, 3, 0, 0, 5, 4, 4, 4, 3, 4, 3, 4, 3,\n       1, 5, 5, 5, 5, 4, 4, 8, 4, 1, 8, 4, 6, 6, 1, 8, 1, 2, 2, 2, 3, 0,\n       0, 1, 1, 0, 8, 0, 3, 0, 0, 0, 4])group(index)int641 2 1 1 1 1 2 1 ... 1 1 1 2 1 1 1 1array([1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2,\n       1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1,\n       1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n       1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2,\n       1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2,\n       2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1,\n       1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1,\n       1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1])Indexes: (2)wavelengthPandasIndexPandasIndex(Index([ 377.1956481933594,  382.2056579589844, 387.21563720703125,\n       392.22564697265625, 397.22564697265625, 402.23565673828125,\n        407.2456359863281,  412.2556457519531,  417.2656555175781,\n         422.275634765625,\n       ...\n        2455.795654296875,    2460.8056640625,  2465.815673828125,\n        2470.815673828125,   2475.82568359375,  2480.835693359375,\n           2485.845703125,  2490.855712890625,   2495.86572265625,\n        2500.875732421875],\n      dtype='float32', name='wavelength', length=425))indexPandasIndexPandasIndex(Index([134, 133, 132, 247, 110, 109,  25,  50,  87,  86,\n       ...\n        55, 209,   3, 217,  13,  77,  16, 305,   2, 300],\n      dtype='int64', name='index', length=319))Attributes: (0)\n\n\nWe have defined all the operations we want, but becasue of xarrays lazy computation, the calculations have not yet been done. We will now force it to perform this calculations. We want to keep the result in chunks, so we use .persist() and not .compute(). This should take approx 2 - 3 mins\n\n##  DUE TO RUN TIME LENGTH, WE WILL NOT RUN THIS IN THE WORKSHOP - For this workshop, we HAVE SAVED THIS OUTPUT (dsp.nc) and it is available FOR NEXT STEP\n# with ProgressBar():\n#     dsp = ds.persist()\n\n\n# dsp.drop_vars('geometry').to_netcdf('~/shared-public/data/dsp.nc')\ndsp = xr.open_dataset('~/shared-public/data/dsp.nc')\ndsp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 552kB\nDimensions:      (wavelength: 425, index: 319)\nCoordinates:\n  * wavelength   (wavelength) float32 2kB 377.2 382.2 ... 2.496e+03 2.501e+03\n  * index        (index) int64 3kB 134 133 132 247 110 109 ... 77 16 305 2 300\nData variables:\n    reflectance  (index, wavelength) float32 542kB ...\n    class        (index) int64 3kB ...\n    group        (index) int64 3kB ...xarray.DatasetDimensions:wavelength: 425index: 319Coordinates: (2)wavelength(wavelength)float32377.2 382.2 ... 2.496e+03 2.501e+03standard_name :radiation_wavelengthlong_name :Wavelength centersunits :nmarray([ 377.19565,  382.20566,  387.21564, ..., 2490.8557 , 2495.8657 ,\n       2500.8757 ], shape=(425,), dtype=float32)index(index)int64134 133 132 247 ... 16 305 2 300array([134, 133, 132, ..., 305,   2, 300], shape=(319,))Data variables: (3)reflectance(index, wavelength)float32..._QuantizeBitGroomNumberOfSignificantDigits :5long_name :Mosaiced Hemispherical Directional Reflectance Factorgrid_mapping :transverse_mercatororthorectified :True[135575 values with dtype=float32]class(index)int64...[319 values with dtype=int64]group(index)int64...[319 values with dtype=int64]Indexes: (2)wavelengthPandasIndexPandasIndex(Index([ 377.1956481933594,  382.2056579589844, 387.21563720703125,\n       392.22564697265625, 397.22564697265625, 402.23565673828125,\n        407.2456359863281,  412.2556457519531,  417.2656555175781,\n         422.275634765625,\n       ...\n        2455.795654296875,    2460.8056640625,  2465.815673828125,\n        2470.815673828125,   2475.82568359375,  2480.835693359375,\n           2485.845703125,  2490.855712890625,   2495.86572265625,\n        2500.875732421875],\n      dtype='float32', name='wavelength', length=425))indexPandasIndexPandasIndex(Index([134, 133, 132, 247, 110, 109,  25,  50,  87,  86,\n       ...\n        55, 209,   3, 217,  13,  77,  16, 305,   2, 300],\n      dtype='int64', name='index', length=319))Attributes: (0)\n\n\n\n\nInspect AVIRIS spectra\n\n# recall the class types\nlabel_df\n\n\n\n\n\n\n\n\nLandType\nclass\n\n\n\n\n0\nBare ground/Rock\n0\n\n\n1\nMature Fynbos\n1\n\n\n2\nRecently Burnt Fynbos\n2\n\n\n3\nWetland\n3\n\n\n4\nForest\n4\n\n\n5\nPine\n5\n\n\n6\nEucalyptus\n6\n\n\n7\nWattle\n7\n\n\n8\nWater\n8\n\n\n\n\n\n\n\n\ndsp_plot = dsp.where(dsp['class']==5, drop=True)\nh = dsp_plot['reflectance'].hvplot.line(x='wavelength',by='index',\n                                    color='green', alpha=0.5,legend=False)\nh\n\n\n\n\n\n  \n\n\n\n\n\nAt this point in a real machine learning workflow, you should closely inspect the spectra you have for each class. Do they make sense? Are there some spectra that look weird? You should re-evaluate your data to make sure that the assigned labels are true. This is a very important step\n\n\nPrep data for ML model\nAs you will know, not all of the wavelengths in the data are of equal quality, some will be degraded by atmospheric water absorption features or other factors. We should remove the bands from the analysis that we are not confident of. Probably the best way to do this is to use the uncertainties provided along with the reflectance files. We will simply use some prior knowledge to screen out the worst bands.\n\nwavelengths_to_drop = dsp.wavelength.where(\n    (dsp.wavelength &lt; 450) |\n    (dsp.wavelength &gt;= 1340) & (dsp.wavelength &lt;= 1480) |\n    (dsp.wavelength &gt;= 1800) & (dsp.wavelength &lt;= 1980) |\n    (dsp.wavelength &gt; 2400), drop=True\n)\n\n# Use drop_sel() to remove those specific wavelength ranges\ndsp = dsp.drop_sel(wavelength=wavelengths_to_drop)\n\nmask = (dsp['reflectance'] &gt; -1).all(dim='wavelength')  # Create a mask where all values along 'z' are non-negative\ndsp = dsp.sel(index=mask)\ndsp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 424kB\nDimensions:      (wavelength: 325, index: 319)\nCoordinates:\n  * wavelength   (wavelength) float32 1kB 452.3 457.3 ... 2.391e+03 2.396e+03\n  * index        (index) int64 3kB 134 133 132 247 110 109 ... 77 16 305 2 300\nData variables:\n    reflectance  (index, wavelength) float32 415kB 0.02169 0.02095 ... 0.04408\n    class        (index) int64 3kB 6 5 5 8 3 3 1 2 3 3 2 ... 1 1 0 8 0 3 0 0 0 4\n    group        (index) int64 3kB ...xarray.DatasetDimensions:wavelength: 325index: 319Coordinates: (2)wavelength(wavelength)float32452.3 457.3 ... 2.391e+03 2.396e+03standard_name :radiation_wavelengthlong_name :Wavelength centersunits :nmarray([ 452.32565,  457.33566,  462.34564, ..., 2385.6755 , 2390.6855 ,\n       2395.6855 ], shape=(325,), dtype=float32)index(index)int64134 133 132 247 ... 16 305 2 300array([134, 133, 132, ..., 305,   2, 300], shape=(319,))Data variables: (3)reflectance(index, wavelength)float320.02169 0.02095 ... 0.04677 0.04408_QuantizeBitGroomNumberOfSignificantDigits :5long_name :Mosaiced Hemispherical Directional Reflectance Factorgrid_mapping :transverse_mercatororthorectified :Truearray([[0.02169 , 0.020949, 0.023012, ..., 0.015063, 0.020992, 0.021133],\n       [0.024572, 0.026062, 0.028263, ..., 0.068257, 0.069917, 0.070469],\n       [0.015267, 0.015812, 0.0166  , ..., 0.026273, 0.02522 , 0.019117],\n       ...,\n       [0.059496, 0.062533, 0.062992, ..., 0.16354 , 0.162181, 0.161702],\n       [0.064827, 0.064649, 0.065027, ..., 0.166297, 0.17051 , 0.165916],\n       [0.021744, 0.023533, 0.02413 , ..., 0.045781, 0.046773, 0.044083]],\n      shape=(319, 325), dtype=float32)class(index)int646 5 5 8 3 3 1 2 ... 0 8 0 3 0 0 0 4array([6, 5, 5, ..., 0, 0, 4], shape=(319,))group(index)int64...[319 values with dtype=int64]Indexes: (2)wavelengthPandasIndexPandasIndex(Index([ 452.3256530761719,  457.3356628417969, 462.34564208984375,\n       467.35565185546875, 472.35565185546875, 477.36566162109375,\n        482.3756408691406,  487.3856506347656,  492.3956604003906,\n        497.4056396484375,\n       ...\n         2350.61572265625,   2355.61572265625,  2360.625732421875,\n          2365.6357421875,  2370.645751953125,   2375.65576171875,\n        2380.665771484375,  2385.675537109375,     2390.685546875,\n           2395.685546875],\n      dtype='float32', name='wavelength', length=325))indexPandasIndexPandasIndex(Index([134, 133, 132, 247, 110, 109,  25,  50,  87,  86,\n       ...\n        55, 209,   3, 217,  13,  77,  16, 305,   2, 300],\n      dtype='int64', name='index', length=319))Attributes: (0)\n\n\nNext we will normalize the data, there are a number of difference normalizations to try. In a ML workflow you should try a few and see which work best. We will only use a Brightness Normalization. In essence, we scale the reflectance of each wavelength by the total brightness of the spectra. This retains info on important shape features and relative reflectance, and removes info on absolute reflectance.\n\n# Calculate the L2 norm along the 'wavelength' dimension\nl2_norm = np.sqrt((dsp['reflectance'] ** 2).sum(dim='wavelength'))\n\n# Normalize the reflectance by dividing by the L2 norm\ndsp['reflectance'] = dsp['reflectance'] / l2_norm\n\nPlot the new, clean spectra\n\ndsp_norm_plot = dsp.where(dsp['class']==5, drop=True)\nh = dsp_norm_plot['reflectance'].hvplot.line(x='wavelength',by='index',\n                                         color='green',ylim=(-0.01,0.2),alpha=0.5,legend=False)\nh\n\n\n\n\n\n  \n\n\n\n\n\n\n\nTrain and evaluate the ML model\nWe will be using a model called xgboost. There are many, many different kinds of ML models. xgboost is a class of models called gradient boosted trees, related to random forests. When used for classification, random forests work by creating multiple decision trees, each trained on a random subset of the data and features, and then averaging their predictions to improve accuracy and reduce overfitting. Gradient boosted trees differ in that they build trees sequentially, with each new tree focusing on correcting the errors of the previous ones. This sequential approach allows xgboost to create highly accurate models by iteratively refining predictions and addressing the weaknesses of earlier trees.\nImport the Machine Learning libraries we will use.\n\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n\nOur dataset has a label indicating which set (training or test), our data belong to. We wil use this to split it\n\n# recall groups\nclass_data_utm.groupby(['group']).size()\n\ngroup\n1    254\n2     65\ndtype: int64\n\n\n\nclass_data_utm.crs\n\n&lt;Projected CRS: EPSG:32734&gt;\nName: WGS 84 / UTM zone 34S\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 18°E and 24°E, southern hemisphere between 80°S and equator, onshore and offshore. Angola. Botswana. Democratic Republic of the Congo (Zaire). Namibia. South Africa. Zambia.\n- bounds: (18.0, -80.0, 24.0, 0.0)\nCoordinate Operation:\n- name: UTM zone 34S\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\ndtrain = dsp.where(dsp['group']==1,drop=True)\ndtest = dsp.where(dsp['group']==2,drop=True)\n\n#create separte datasets for labels and features\ny_train = dtrain['class'].values.astype(int)\ny_test = dtest['class'].values.astype(int)\nX_train = dtrain['reflectance'].values\nX_test = dtest['reflectance'].values\n\n\nTrain ML model\nThe steps we will go through to train the model are:\n\nFirst, we define the hyperparameter grid. Initially, we set up a comprehensive grid (param_grid) with multiple values for several hyperparameters of the XGBoost model.\nNext, we create an XGBoost classifier object using the XGBClassifier class from the XGBoost library.\n\nWe then set up the GridSearchCV object using our defined XGBoost model and the hyperparameter grid. GridSearchCV allows us to perform an exhaustive search over the specified hyperparameter values to find the optimal combination that results in the best model performance. We choose a 5-fold cross-validation strategy (cv=5), meaning we split our training data into five subsets to validate the model’s performance across different data splits. We use accuracy as our scoring metric to evaluate the models.\nAfter setting up the grid search, we fit the GridSearchCV object to our training data (X_train and y_train). This process involves training multiple models with different hyperparameter combinations and evaluating their performance using cross-validation. Our goal is to identify the set of hyperparameters that yields the highest accuracy.\nOnce the grid search completes, we print out the best set of hyperparameters and the corresponding best score. The grid_search.best_params_ attribute provides the combination of hyperparameters that achieved the highest cross-validation accuracy, while the grid_search.best_score_ attribute shows the corresponding accuracy score. Finally, we extract the best model (best_model) from the grid search results. This model is trained with the optimal hyperparameters and is ready for making predictions or further analysis in our classification task.\nThis will take approx 30 seconds\n\n# Define the hyperparameter grid\nparam_grid = {\n    'max_depth': [5],\n    'learning_rate': [0.1],\n    'subsample': [0.75],\n    'n_estimators' : [50,100]\n}\n\n# Create the XGBoost model object\nxgb_model = xgb.XGBClassifier(tree_method='hist')\n\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n\n# Fit the GridSearchCV object to the training data\ngrid_search.fit(X_train, y_train)\n\n# Print the best set of hyperparameters and the corresponding score\nprint(\"Best set of hyperparameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\nbest_model = grid_search.best_estimator_\n\nBest set of hyperparameters:  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.75}\nBest score:  0.6376470588235295\n\n\n\n\n\nEvaluate model performance\nWe will use our best model to predict the classes of the test data Then, we calculate the F1 score using f1_score, which balances precision and recall, and print it to evaluate overall performance.\nNext, we assess how well the model performs for predicting Pine trees by calculating its precision and recall. Precision measures the accuracy of the positive predictions. It answers the question, “Of all the instances we labeled as Pines, how many were actually Pines?”. Recall measures the model’s ability to identify all actual positive instances. It answers the question, “Of all the actual Pines, how many did we correctly identify?”. You may also be familiar with the terms Users’ and Producers’ Accuracy. Precision = User’ Accuracy, and Recall = Producers’ Accuracy.\nFinally, we create and display a confusion matrix to visualize the model’s prediction accuracy across all classes\n\ny_pred = best_model.predict(X_test)\n\n# Step 2: Calculate acc and F1 score for the entire dataset\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {acc}\")\n\nf1 = f1_score(y_test, y_pred, average='weighted')  # 'weighted' accounts for class imbalance\nprint(f\"F1 Score (weighted): {f1}\")\n\n# Step 3: Calculate precision and recall for class 5 (Pine)\nprecision_class_5 = precision_score(y_test, y_pred, labels=[5], average='macro', zero_division=0)\nrecall_class_5 = recall_score(y_test, y_pred, labels=[5], average='macro', zero_division=0)\n\nprint(f\"Precision for Class 5: {precision_class_5}\")\nprint(f\"Recall for Class 5: {recall_class_5}\")\n\n# Step 4: Plot the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nConfusionMatrixDisplay(confusion_matrix=conf_matrix).plot()\nplt.show()\n\nAccuracy: 0.6615384615384615\nF1 Score (weighted): 0.6568699274581629\nPrecision for Class 5: 0.5\nRecall for Class 5: 0.6666666666666666\n\n\n\n\n\n\n\n\n\n\n\nSkipping Some steps in the BioSCape Workshop Tutorial\nFor the length of this workshop, we cannot cover all steps that were in the BioSCape Cape Town Workshop. But links are provided here:\n8.2.1.8. Interpret and understand ML model\n\nhttps://ornldaac.github.io/bioscape_workshop_sa/tutorials/Machine_Learning/Invasive_AVIRIS.html#interpret-and-understand-ml-model\n\n\n\nPredict over an example AVIRIS scene\nWe now have a trained model and are ready to deploy it to generate predictions across an entire AVIRIS scene and map the distribution of invasive plants. This involves handling a large volume of data, so we need to write the code to do this intelligently. We will accomplish this by applying the .predict() method of our trained model in parallel across the chunks of the AVIRIS xarray. The model will receive one chunk at a time so that the data is not too large, but it will be able to perform this operation in parallel across multiple chunks, and therefore will not take too long.\nThis model was only trained on data covering natural vegetaton in the Cape Peninsula, It is important that we only predict in the areas that match our training data. We will therefore filter to scenes that cover the Cape Peninsula and mask out non-protected areas - SAPAD_2024.gpkg\n\n#south africa protected areas\nSAPAD = (gpd.read_file('~/shared-public/data/SAPAD_2024.gpkg')\n         .query(\"SITE_TYPE!='Marine Protected Area'\")\n        )\n#SAPAD.plot()\n#SAPAD.to_crs(\"EPSG:32734\")\nSAPAD.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# Get the bounding box of the training data\nbbox = class_data_utm.total_bounds  # (minx, miny, maxx, maxy)\n#bbox\ngdf_bbox = gpd.GeoDataFrame({'geometry': [box(*bbox)]}, crs=class_data_utm.crs)  # Specify the CRS\ngdf_bbox['geometry'] = gdf_bbox.buffer(500)\ngdf_bbox.crs\n\n&lt;Projected CRS: EPSG:32734&gt;\nName: WGS 84 / UTM zone 34S\nAxis Info [cartesian]:\n- E[east]: Easting (metre)\n- N[north]: Northing (metre)\nArea of Use:\n- name: Between 18°E and 24°E, southern hemisphere between 80°S and equator, onshore and offshore. Angola. Botswana. Democratic Republic of the Congo (Zaire). Namibia. South Africa. Zambia.\n- bounds: (18.0, -80.0, 24.0, 0.0)\nCoordinate Operation:\n- name: UTM zone 34S\n- method: Transverse Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n#south africa protected areas\nSAPAD = (gpd.read_file('~/shared-public/data/SAPAD_2024.gpkg')\n         .query(\"SITE_TYPE!='Marine Protected Area'\")\n        )\nSAPAD = SAPAD.to_crs(\"EPSG:32734\")\n\n# Get the bounding box of the training data\nbbox = class_data_utm.total_bounds  # (minx, miny, maxx, maxy)\ngdf_bbox = gpd.GeoDataFrame({'geometry': [box(*bbox)]}, crs=class_data_utm.crs)  # Specify the CRS\ngdf_bbox['geometry'] = gdf_bbox.buffer(500)\n\n# protected areas that intersect with the training data\nSAPAD_CT = SAPAD.overlay(gdf_bbox,how='intersection')\n\n#keep only AVIRIS scenes that intersects with CT protected areas\nAVNG_sapad = AVNG_CP[AVNG_CP.intersects(SAPAD_CT.unary_union)]\n\n\nm = AVNG_sapad[['native-id','geometry']].explore('native-id')\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\nSAPAD.keys()\n\nIndex(['WDPAID', 'CUR_NME', 'WMCM_TYPE', 'MAJ_TYPE', 'SITE_TYPE', 'D_DCLAR',\n       'LEGAL_STAT', 'GIS_AREA', 'PROC_DOC', 'Shape_Leng', 'Shape_Area',\n       'geometry'],\n      dtype='object')\n\n\n\n#map = AVNG_Coverage[['fid', 'geometry']].explore('fid')\nmap = SAPAD[['SITE_TYPE', 'geometry']].explore('SITE_TYPE')\nmap\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nHere is the function that we will actually apply to each chunk. Simple really. The hard work is getting the data into and out of this functiON\n\ndef predict_on_chunk(chunk, model):\n    probabilities = model.predict_proba(chunk)\n    return probabilities\n\nNow we define the funciton that takes as input the path to the AVIRIS file and pass the data to the predict function. THhs is composed of 4 parts:\n\nPart 1: Opens the AVIRIS data file using xarray and sets a condition to identify valid data points where reflectance values are greater than zero.\nPart 2: Applies all the transformations that need to be done before the data goes to the model. It the spatial dimensions (x and y) into a single dimension, filters wavelengths, and normalizes the reflectance data.\nPart 3: Applies the machine learning model to the normalized data in parallel, predicting class probabilities for each data point using xarray’s apply_ufunc method. Most of the function invloves defining what to do with the dimensions of the old dataset and the new output\nPart 4: Unstacks the data to restore its original dimensions, sets spatial dimensions and coordinate reference system (CRS), clips the data, and transposes the data to match expected formats before returning the results.\n\n\ndef predict_xr(file,geometries):\n\n    rfl_f, ql_f, unc_f = separate_granule_type(file)\n    native_id = path.basename(rfl_f[0].path)[:-7]\n    \n    #part 1 - opening file\n    #open the file\n    print(f'file: {rfl_f[0]}')\n    ds = xr.open_datatree(rfl_f[0], engine='h5netcdf', chunks='auto')\n    \n    #get the geometries of the protected areas for masking\n    ds_crs = ds.transverse_mercator.crs_wkt\n    geometries = geometries[geometries[\"native-id\"]==native_id].to_crs(ds_crs).geometry.apply(mapping)\n    # geometries = geometries.to_crs(ds_crs).geometry.apply(mapping)\n    \n    #condition to use for masking no data later\n    condition = (ds['reflectance'] &gt; -1).any(dim='wavelength')\n    \n    #stack the data into a single dimension. This will be important for applying the model later\n    ds = ds.reflectance.to_dataset().stack(sample=('easting','northing'))\n    \n    #part 2 - pre-processing\n    #remove bad wavelenghts\n    wavelengths_to_drop = ds.wavelength.where(\n        (ds.wavelength &lt; 450) |\n        (ds.wavelength &gt;= 1340) & (ds.wavelength &lt;= 1480) |\n        (ds.wavelength &gt;= 1800) & (ds.wavelength &lt;= 1980) |\n        (ds.wavelength &gt; 2400), drop=True\n    )\n    # Use drop_sel() to remove those specific wavelength ranges\n    ds = ds.drop_sel(wavelength=wavelengths_to_drop)\n    \n    #normalise the data\n    l2_norm = np.sqrt((ds['reflectance'] ** 2).sum(dim='wavelength'))\n    ds['reflectance'] = ds['reflectance'] / l2_norm\n    \n     \n    #part 3 - apply the model over chunks\n    result = xr.apply_ufunc(\n        predict_on_chunk,\n        ds['reflectance'].chunk(dict(wavelength=-1)),\n        input_core_dims=[['wavelength']],#input dim with features\n        output_core_dims=[['class']],  # name for the new output dim\n        exclude_dims=set(('wavelength',)),  #dims to drop in result\n        output_sizes={'class': 9}, #length of the new dimension\n        output_dtypes=[np.float32],\n        dask=\"parallelized\",\n        kwargs={'model': best_model}\n    )\n    \n    #part 4 - post-processing\n    result = result.where((result &gt;= 0) & (result &lt;= 1), np.nan) #valid values\n    result = result.unstack('sample') #remove the stack\n    result = result.rio.set_spatial_dims(x_dim='easting',y_dim='northing') #set the spatial dims\n    result = result.rio.write_crs(ds_crs) #set the CRS\n    result = result.rio.clip(geometries) #clip to the protected areas and no data\n    result = result.transpose('class', 'northing', 'easting') #transpose the data rio expects it this way\n    \n    return result   \n\nLet’s test that it works on a single file before we run it through 100s of GB of data.\n\ntest  = predict_xr(earthaccess.open(single_granule),AVNG_sapad)\ntest\n\n\n\n\n\n\n\n\n\n\nfile: &lt;File-like object S3FileSystem, ornl-cumulus-prod-protected/bioscape/BioSCape_ANG_V02_L3_RFL_Mosaic/data/AVIRIS-NG_BIOSCAPE_V02_L3_36_11_RFL.nc&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'reflectance' (class: 9, northing: 2000, easting: 1999)&gt; Size: 144MB\ndask.array&lt;transpose, shape=(9, 2000, 1999), dtype=float32, chunksize=(9, 2000, 256), chunktype=numpy.ndarray&gt;\nCoordinates:\n  * easting      (easting) float64 16kB 7.9e+05 7.9e+05 7.9e+05 ... 8e+05 8e+05\n  * northing     (northing) float64 16kB 8.3e+05 8.3e+05 ... 8.2e+05 8.2e+05\n    spatial_ref  int64 8B 0\nDimensions without coordinates: classxarray.DataArray'reflectance'class: 9northing: 2000easting: 1999dask.array&lt;chunksize=(9, 2000, 255), meta=np.ndarray&gt;\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n137.26 MiB\n17.58 MiB\n\n\nShape\n(9, 2000, 1999)\n(9, 2000, 256)\n\n\nDask graph\n9 chunks in 32 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n                                         1999 2000 9\n\n\n\n\nCoordinates: (3)easting(easting)float647.9e+05 7.9e+05 ... 8e+05 8e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([790005., 790010., 790015., ..., 799985., 799990., 799995.],\n      shape=(1999,))northing(northing)float648.3e+05 8.3e+05 ... 8.2e+05 8.2e+05axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([830000., 829995., 829990., ..., 820015., 820010., 820005.],\n      shape=(2000,))spatial_ref()int640crs_wkt :PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ellipse Based\",SPHEROID[\"Unnamed\",6378137,298.257223562997]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",-30],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",-22],PARAMETER[\"standard_parallel_2\",-38],PARAMETER[\"false_easting\",1400000],PARAMETER[\"false_northing\",1300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223562997reference_ellipsoid_name :Unnamedlongitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Ellipse Basedhorizontal_datum_name :Ellipse Basedprojected_crs_name :unnamedgrid_mapping_name :albers_conical_equal_areastandard_parallel :(-22.0, -38.0)latitude_of_projection_origin :-30.0longitude_of_central_meridian :25.0false_easting :1400000.0false_northing :1300000.0spatial_ref :PROJCS[\"unnamed\",GEOGCS[\"Ellipse Based\",DATUM[\"Ellipse Based\",SPHEROID[\"Unnamed\",6378137,298.257223562997]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Albers_Conic_Equal_Area\"],PARAMETER[\"latitude_of_center\",-30],PARAMETER[\"longitude_of_center\",25],PARAMETER[\"standard_parallel_1\",-22],PARAMETER[\"standard_parallel_2\",-38],PARAMETER[\"false_easting\",1400000],PARAMETER[\"false_northing\",1300000],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :790002.5 5.0 0.0 830002.5 0.0 -5.0array(0)Indexes: (2)eastingPandasIndexPandasIndex(Index([790005.0, 790010.0, 790015.0, 790020.0, 790025.0, 790030.0, 790035.0,\n       790040.0, 790045.0, 790050.0,\n       ...\n       799950.0, 799955.0, 799960.0, 799965.0, 799970.0, 799975.0, 799980.0,\n       799985.0, 799990.0, 799995.0],\n      dtype='float64', name='easting', length=1999))northingPandasIndexPandasIndex(Index([830000.0, 829995.0, 829990.0, 829985.0, 829980.0, 829975.0, 829970.0,\n       829965.0, 829960.0, 829955.0,\n       ...\n       820050.0, 820045.0, 820040.0, 820035.0, 820030.0, 820025.0, 820020.0,\n       820015.0, 820010.0, 820005.0],\n      dtype='float64', name='northing', length=2000))Attributes: (0)\n\n\n\n# Again, recall the labels and LandType classes\nlabel_df\n\n\n\n\n\n\n\n\nLandType\nclass\n\n\n\n\n0\nBare ground/Rock\n0\n\n\n1\nMature Fynbos\n1\n\n\n2\nRecently Burnt Fynbos\n2\n\n\n3\nWetland\n3\n\n\n4\nForest\n4\n\n\n5\nPine\n5\n\n\n6\nEucalyptus\n6\n\n\n7\nWattle\n7\n\n\n8\nWater\n8\n\n\n\n\n\n\n\n\n# You can see here that we are mapping results for class 1, mature fynbos\ntest = test.rio.reproject(\"EPSG:4326\",nodata=np.nan)\nh = test.isel({'class':1}).hvplot(tiles=hv.element.tiles.EsriImagery(), \n                              project=True,rasterize=True,clim=(0,1),\n                              cmap='magma',frame_width=400,data_aspect=1,alpha=0.5)\nh\n\n\n\n\n\n  \n\n\n\n\nML models typically provide a single prediction of the most likely outcomes. You can also get probability-like scores (values from 0 to 1) from these models, but they are not true probabilities. If the model gives you a score of 0.6, that means it is more likely than a prediction of 0.5, and less likely than 0.7. However, it does not mean that in a large sample your prediction would be right 60 times out of 100. To get calibrated probabilities from our models, we have to apply additional steps. We can also get a set of predictions from models rather than a single prediction, which reflects the model’s true uncertainty using a technique called conformal predictions. Read more about conformal prediction for geospatial machine learning in this amazing paper:\nSingh, G., Moncrieff, G., Venter, Z., Cawse-Nicholson, K., Slingsby, J., & Robinson, T. B. (2024). Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction. Scientific Reports, 14(1), 16166.\n\n\nFinal steps of the full ML classification are time intensive and are not described in this workshop.\nSteps in BioSCape Cape Town Workshop Tutorial\n8.2.1.10. Merge and mosaic results\n\n\nCREDITS:\nFind all of the October 2025 BioSCape Data Workshop Materials/Notebooks\n\nhttps://ornldaac.github.io/bioscape_workshop_sa/intro.html\n\nThis Notebook is an adaption of Glenn Moncrieff’s BioSCape Data Workshop Notebook: Mapping invasive species using supervised machine learning and AVIRIS-NG - This Notebook accesses and uses an updated version of AVIRIS-NG data with improved corrections and that are in netCDF file formats\nGlenn’s lesson borrowed from:\n\nLand cover mapping example on Microsoft Planetary Computer",
    "crumbs": [
      "Tutorials",
      "4 NASA - Mapping Invasive Species Using Supervised Machine Learning and AVIRIS-NG"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html",
    "href": "neon/02_neon-refl-classification.html",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "",
    "text": "The National Ecological Observatory Network (NEON) Airborne Observation Platform (AOP) collects airborne remote sensing data, including hyperspectral reflectance data, over 81 sites across the United States and Puerto Rico. In this notebook we will show how to download and visualize reflectance data from NEON’s Smithsonian Environmental Research Center site (SERC) in Maryland. We will then demonstrate how to run a supervised classification using the NEON Observational System (OS) Vegetation Structure data as training data, and evaluate the model results.\n\n\n\nThe NEON Imaging Spectrometer (NIS) is an airborne imaging spectrometer built by JPL (AVIRIS-NG) and operated by the National Ecological Observatory Network’s (NEON) Airborne Observation Platform (AOP). NEON’s hyperspectral sensors collect measurements of sunlight reflected from the Earth’s surface in 426 narrow (~5 nm) spectral channels spanning wavelengths between ~ 380 - 2500 nm. NEON’s remote sensing data is intended to map and answer questions about a landscape, with ecological applications including identifying and classifying plant species and communities, mapping vegetation health, detecting disease or invasive species, and mapping droughts, wildfires, or other natural disturbances and their impacts.\nIn 2024, NEON started producing bidirectional reflectance data products (including BRDF and topographic corrections). These are currently available for AOP data collected between 2022-2025. For more details on this newly revised data product, please refer to the tutorial: Introduction to Bidirectional Hyperspectral Reflectance Data in Python.\nNEON surveys sites spanning the continental US, during peak phenological greenness, capturing each site 3 out of every 5 years, for most terrestrial sites. AOP’s Flight Schedules and Coverage provide’s more information about the current and past schedules.\nMore detailed information about NEON’s airborne sampling design can be found in the paper: Spanning scales: The airborne spatial and temporal sampling design of the National Ecological Observatory Network.\n\n\n\n\nNo Python setup requirements if connected to the workshop Openscapes cloud instance!\nLocal Only\n\nUsing your preferred command line interface (command prompt, terminal, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\n\n```cmd\nconda create -n neon_aop -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio geopandas jupyter jupyter_bokeh jupyterlab h5py spectral scikit-image scikit-learn seaborn neonutilities\n```\n\nFor MacOSX:\n\n```cmd\nconda create -n neon_aop -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter jupyter_bokeh jupyterlab h5py spectral scikit-image scikit-learn seaborn neonutilities\n```\n\n\n\n\nNEON API Token (optional, but strongly recommended), see NEON API Tokens Tutorial for more details on how to create and set up your token in Python (and R). Once you create your token (on the NEON User Accounts) page, this notebook will show you how to set it as an environment variable and use it for downloading AOP data.\n\n\n\n\nThe lesson shows how to programmatically download the NEON shapefiles, but you can also download them by clicking on the following links:\n\nAOP Flight Box Boundaries: AOP_FlightBoxes.zip\nTOS Sampling Boundaries: TOS_SamplingBoundaries.zip\n\n\n\n\n\nExplore NEON airborne and field (instrumented, observational) shapefiles to understand what colloated data are available\nUse the neonutilities package to determine available reflectance data and download\nUse a custom function to convert reflectance data into an xarray dataset\nCreate some interactive visualizations of reflectance data\nRun a random forest model to classify trees using reflectance data and data generated from vegetation structure (as the training data set)\nEvaluate classification model results\nUnderstand data QA considerations and potential steps to improve classification results\n\n\n\n\n\nSetup\nVisualize NEON AOP, OS, and IS shapefiles at SERC\nFind available NEON reflectance data at SERC and download\nRead in and visualize reflectance data interactively\nCreate a random forest model to predict the tree families from the reflectance spectra",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#setup",
    "href": "neon/02_neon-refl-classification.html#setup",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "1. Setup",
    "text": "1. Setup\n\n1.1 Import Python Packages\nIf not already installed, install the neonutilities and python-dotenv packages using pip as follows: - !pip install neonutilities - !pip install python-dotenv\n\n# Import required libraries, grouped by functionality\n# --- System and utility packages ---\nfrom datetime import timedelta\nimport dotenv\nimport os\nimport requests\nfrom zipfile import ZipFile\n\n# --- Data handling and scientific computing ---\nimport math\nimport numpy as np\nimport pandas as pd\n\n# --- Geospatial and multi-dimensional raster data ---\nimport geopandas as gpd\nimport h5py\nimport rasterio as rio  # work with geospatial raster data\nimport rioxarray as rxr  # work with raster arrays\nfrom shapely import geometry\nfrom shapely.geometry.polygon import orient\nfrom osgeo import gdal  # work with raster and vector geospatial data\nimport xarray as xr\n\n# --- Plotting and visualization ---\nimport holoviews as hv\nimport hvplot.xarray  # plot multi-dimensional arrays\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport folium\n\n# --- neonutilities ---\nimport neonutilities as nu\n\n\n\n1.2 Set your NEON Token\nDefine your token. You can set this up on your NEON user account page, https://data.neonscience.org/myaccount. Please refer to the NEON API Tokens Tutorial for more details on how to create and set up your token in Python (and R).\n\n# method 1: set the NEON_TOKEN directly in your code\nNEON_TOKEN='YOUR_TOKEN_HERE'\n\n# method 2: set the token as an environment variable using the dotenv package\ndotenv.set_key(dotenv_path=\".env\",\nkey_to_set=\"NEON_TOKEN\",#\nvalue_to_set=\"YOUR_TOKEN_HERE\")\n\n# to retrieve the token that you set as an environment variable, use:\nNEON_TOKEN=os.environ.get(\"NEON_TOKEN\")",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#visualize-neon-aop-os-and-is-shapefiles-at-serc",
    "href": "neon/02_neon-refl-classification.html#visualize-neon-aop-os-and-is-shapefiles-at-serc",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "2. Visualize NEON AOP, OS, and IS shapefiles at SERC",
    "text": "2. Visualize NEON AOP, OS, and IS shapefiles at SERC\nIn this next section, we will look at some of the NEON spatial data, honing in on our site of interest (SERC). We will look at the AOP flight box (the area over which the NEON AOP platform flies, including multiple priority boxes), the IS tower airshed, and the OS terrestrial sampling boundaries. This will provide an overview of how the NEON sites are set up, and the spatial overlap between the field and airborne data.\nFirst, let’s define a function that will download data from a url. We will use this to download shapefile boundares of the NEON AOP flight boxes, as well as the IS and OS shapefiles in order to see the spatial extent of the various data samples that NEON collects.\n\n# function to download data stored on the internet in a public url to a local file\ndef download_url(url,download_dir):\n    if not os.path.isdir(download_dir):\n        os.makedirs(download_dir)\n    filename = url.split('/')[-1]\n    r = requests.get(url, allow_redirects=True)\n    file_object = open(os.path.join(download_dir,filename),'wb')\n    file_object.write(r.content)\n\n\n2.1 NEON AOP flight box boundary\nDownload, Unzip, and Open the shapefile (.shp) containing the AOP flight box boundaries, which can also be downloaded from NEON Spatial Data and Maps. Read this shapefile into a geodataframe, explore the contents, and check the coordinate reference system (CRS) of the data.\n\n# Download and Unzip the NEON Flight Boundary Shapefile\naop_flight_boundary_url = \"https://www.neonscience.org/sites/default/files/AOP_flightBoxes_0.zip\"\n# Use download_url function to save the file to a directory\nos.makedirs('./data/shapefiles', exist_ok=True)\ndownload_url(aop_flight_boundary_url,'./data/shapefiles')\n# Unzip the file\nwith ZipFile(f\"./data/shapefiles/{aop_flight_boundary_url.split('/')[-1]}\", 'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles')\n\n\naop_flightboxes = gpd.read_file(\"./data/shapefiles/AOP_flightBoxes/AOP_flightboxesAllSites.shp\")\naop_flightboxes.head()\n\nNext, let’s examine the AOP flightboxes polygons at the SERC site.\n\nsite_id = 'SERC'\naop_flightboxes[aop_flightboxes.siteID == site_id]\n\nWe can see the site geodataframe consists of a single polygon, that we want to include in our study site (sometimes NEON sites may have more than one polygon, as there are sometimes multiple areas, with different priorities for collection).\n\n# write this to a new variable called \"site_polygon\"\nsite_aop_polygon = aop_flightboxes[aop_flightboxes.siteID == site_id]\n# subset to only include columns of interest\nsite_aop_polygon = site_aop_polygon[['domain','siteName','siteID','sampleType','flightbxID','priority','geometry']]\n# rename the flightbxID column to flightboxID for clarity\nsite_aop_polygon = site_aop_polygon.rename(columns={'flightbxID':'flightboxID'})\nsite_aop_polygon # display site polygon\n\nNext we can visualize our region of interest (ROI) and the exterior boundary polygon containing ROIs.\nNow let’s define a function that uses folium to display the bounding box polygon on a map. We will first use this function to visualize the AOP flight box polygon, and then we will use it to visualize the IS and OS polygons as well.\n\ndef plot_folium_shapes(\n    shapefiles,      # list of file paths or GeoDataFrames\n    styles=None,     # list of style dicts for each shapefile\n    names=None,      # list of names for each shapefile\n    map_center=None, # [lat, lon]\n    zoom_start=12\n):\n    import pyproj\n    # If no center is provided, use the centroid of the first shapefile (projected)\n    if map_center is None:\n        if isinstance(shapefiles[0], str):\n            gdf = gpd.read_file(shapefiles[0])\n        else:\n            gdf = shapefiles[0]\n        # Project to Web Mercator (EPSG:3857) for centroid calculation\n        gdf_proj = gdf.to_crs(epsg=3857)\n        centroid = gdf_proj.geometry.centroid.iloc[0]\n        # Convert centroid back to lat/lon\n        lon, lat = gpd.GeoSeries([centroid], crs=\"EPSG:3857\").to_crs(epsg=4326).geometry.iloc[0].coords[0]\n        map_center = [lat, lon]\n    \n    m = folium.Map(\n        location=map_center,\n        zoom_start=zoom_start,\n        tiles=None\n    )\n    folium.TileLayer(\n        tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',\n        attr='Google',\n        name='Google Satellite'\n    ).add_to(m)\n    \n    for i, shp in enumerate(shapefiles):\n        if isinstance(shp, str):\n            gdf = gpd.read_file(shp)\n        else:\n            gdf = shp\n        style = styles[i] if styles and i &lt; len(styles) else {}\n        layer_name = names[i] if names and i &lt; len(names) else f\"Shape {i+1}\"\n        folium.GeoJson(\n            gdf,\n            name=layer_name,\n            style_function=lambda x, style=style: style,\n            tooltip=layer_name\n        ).add_to(m)\n    \n    folium.LayerControl().add_to(m)\n    return m\n\n\nmap1 = plot_folium_shapes(\n    shapefiles=[site_aop_polygon],\n    names=['NEON AOP Flight Bounding Box']\n)\n\nmap1\n\n\n \n\nAOP flight box polygon at the SERC site.\n\n\n\n\n\n2.2 NEON OS terrestrial sampling boundaries\nWe will follow a similar process to download and visualize the NEON OS terrestrial sampling boundaries. The OS terrestrial sampling boundaries are also available as a shapefile, which can be downloaded from NEON Spatial Data and Maps page.\n\n# Download and Unzip the NEON Terrestrial Field Sampling Boundaries Shapefile\nneon_field_boundary_file = \"https://www.neonscience.org/sites/default/files/Field_Sampling_Boundaries_202503.zip\"\n# Use download_url function to save the file to the data directory\ndownload_url(neon_field_boundary_file,'./data/shapefiles')\n\n\n# Unzip the file\nwith ZipFile(f\"./data/shapefiles/Field_Sampling_Boundaries_202503.zip\", 'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles')\n\n\nneon_terr_bounds = gpd.read_file(\"./data/shapefiles/Field_Sampling_Boundaries/terrestrialSamplingBoundaries.shp\")\nneon_terr_bounds.head()\n\n\n# save the boundaries for the site to a new variable called \"site_terr_bounds\"\nsite_terr_bounds = neon_terr_bounds[neon_terr_bounds.siteID == site_id]\nsite_terr_bounds.head()\n\n\n\n2.3 NEON IS tower footprint boundaries\nLastly, we’ll download and read in the IS tower footprint shapefile, which represents the area of the airshed over which the IS tower collects data. This shapefile is available from the NEON Spatial Data and Maps page, but is pre-downloaded for convenience.\n\n# unzip the 90 percent footprint tower airshed file\nwith ZipFile(f\"./data/90percentfootprint.zip\", 'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles')\n\n\n# read in the neon tower airshed polygon shapefile and display\nneon_tower_airshed = gpd.read_file(\"./data/shapefiles/90percentfootprint/90percent_footprint.shp\")\nneon_tower_airshed.head()\n\n\n# save the boundaries for the site to a new variable called \"site_terr_bounds\"\nsite_tower_bounds = neon_tower_airshed[neon_tower_airshed.SiteID == site_id]\nsite_tower_bounds.head()\n\n\n\n2.4 Visualize AOP, OS, and IS boundaries together\nNow that we’ve read in all the shapefiles into geodataframes, we can visualize them all together as follows. We will use the plot_folium_shapes function defined above, and define a styles list of dictionaries specifying the color, so that we can display each polygon with a different color.\n\nneon_shapefiles = [site_aop_polygon, site_terr_bounds, site_tower_bounds]\n\n# define a list of styles for the polygons\n# each style is a dictionary with 'fillColor' and 'color' keys\nstyles = [\n    {'fillColor': '#228B22', 'color': '#228B22'}, # green\n    {'fillColor': '#00FFFFFF', 'color': '#00FFFFFF'}, # blue\n    {'fillColor': '#FF0000', 'color': '#FF0000'}, # red\n    {'fillColor': '#FFFF00', 'color': '#FFFF00'}, # yellow\n]\n\nmap2 = plot_folium_shapes(\n    shapefiles=neon_shapefiles,\n    names=['NEON AOP Flight Bounding Box', 'NEON Terrestrial Sampling Boundaries', 'NEON Tower Airshed'],\n    styles=styles\n)\n\nmap2\n\n\n \n\nAOP, OS, and IS polygons at the SERC site.\n\n\n\nAbove we can see the SOAP flightbox, and the exterior TOS boundary polygon which shows the extent of the area where observational data are collected.",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#find-available-neon-reflectance-data-and-download",
    "href": "neon/02_neon-refl-classification.html#find-available-neon-reflectance-data-and-download",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "3. Find available NEON reflectance data and download",
    "text": "3. Find available NEON reflectance data and download\nFinally we can look at the available NEON hyperspectral reflectance data, which are delivered as 1 km by 1 km hdf5 files (also called tiles) over the site. The next figure we make will make it clear why the files are called tiles. First, we will determine the available reflectance data, and then pull in some metadata shapefiles from another L3 AOP data product, derived from the lidar data.\nNEON hyperspectral reflectance data are currently available under two different revisions, as AOP is in the process of implementing a BRDF (Bidirectional Reflectance Distribution Function), but this has not been applied to the full archive of data yet. These data product IDs are DP3.30006.001 (directional surface reflectance), and DP3.30006.002 (bidirectional surface reflectance). The bidirectional surface reflectance data include BRDF and topographic corrections, which helps correct for differences in illumination throughout the flight.\n\n3.1 Find available data\nLet’s see what data are available at the SERC site for each of these data products using the neonutilities list_available_dates function as follows:\n\n# define the data product IDs for the reflectance data\nrefl_rev1_dpid = 'DP3.30006.001'\nrefl_rev2_dpid = 'DP3.30006.002'\n\n\nprint(f'Directional Reflectance Data Available at NEON Site {site_id}:')\nnu.list_available_dates(refl_rev1_dpid,site_id)\n\n\nprint(f'Bidirectional Reflectance Data Available at NEON Site {site_id}:')\nnu.list_available_dates(refl_rev2_dpid,site_id)\n\nThe dates provided are the year and month that the data were published (YYYY-MM). A single site may be collected over more than one month, so this publish date typically represents the month where the majority of the flight lines were collected. There are released directional reflectance data available from 2016 to 2021, and provisional bidirectional reflectance data available in 2022 and 2025. As of 2025, bidirectional data are only available provisionally because they were processed in 2024 (there is a year lag-time before data is released to allow for time to review for data quality issues).\nFor this exercise, we’ll look at the most recent data, from 2025. You may wish to consider other factors, for example if you collected field data in a certain year, you are looking at a year when there was a recent disturbance, or if you want to find the clearest weather data (data are not always collected in optimal sky conditions). For SERC, the most recent clear (&lt;10% cloud cover) weather collection to date was in 2017, so this directional reflectance data may be another good option to consider for your analysis.\nFor this lesson, we will use the 2025 bidirectional reflectance data, which is provisional.\n\nyear = '2025'\n\n\n\n3.2 Download NEON Lidar data using neonutilities by_file_aop\nWe can download the reflectance data either using the neonutilities function nu.by_file_aop, which downloads all tiles for the entire site for a given year, or nu.by_tile_aop. To figure out the inputs of these functions, you can type nu.by_tile_aop?, for example.\nAOP data are projected into a WGS84 coordinate system, with coordinates in UTM x, y. When using nu.by_tile_aop you need to specify the UTM easting and northing values for the tiles you want to download. If you’re not sure the extent of the site, you can use the function nu.get_aop_tile_extents. Let’s do that here, for the SOAP site collected in 2024.You will use the same token you defined at the beginning of the tutorial\n\nserc2025_utm_extents = nu.get_aop_tile_extents(refl_rev2_dpid, \n                                               site_id,\n                                               year,\n                                               token=NEON_TOKEN)\n\nThe AOP collection over SERC in 2025 extends from UTM 358000 - 370000 m (Easting) and 4298000 - 4312000 m (Northing). To display a list of the extents of every tile, you can print serc2025_utm_extents. This is sometimes useful when trying to determine the extents of irregularly shaped sites.\nWe can also look at the full extents by downloading one of the smaller lidar raster data products to start. The L3 lidar data products include metadata shapefiles that can be useful for understanding the spatial extents of the individual files that comprise the data product. To show how to look at these shapefiles, we can download the Canopy Height Model data (DP3.30015.001). The next cell shows how to do this:\n\n# download all CHM tiles (https://data.neonscience.org/data-products/DP3.30015.001)\nnu.by_file_aop(dpid='DP3.30015.001', # Ecosystem Structure / CHM \n               site=site_id,\n               year=year,\n               include_provisional=True,\n               token=NEON_TOKEN,\n               savepath='./data')\n\nThe function below displays all of the folders that we’ve downloaded. You can also use your File Explorer to look at the contents of what you have downloaded.\n\ndef find_data_subfolders(root_dir):\n    \"\"\"\n    Recursively finds subfolders within a directory that contain data (files)\n    and excludes subfolders that only contain other subfolders.\n\n    Args:\n        root_dir: The path to the root directory to search.\n\n    Returns:\n        A list of paths to the subfolders containing data.\n    \"\"\"\n    data_subfolders = []\n    for root, dirs, files in os.walk(root_dir):\n        # Check if the current directory has both subdirectories and files\n        if dirs and files:\n            # Iterate through subdirectories to find those that contain files\n            for dir_name in dirs:\n                dir_path = os.path.join(root, dir_name)\n                if any(os.path.isfile(os.path.join(dir_path, f)) for f in os.listdir(dir_path)):\n                    data_subfolders.append(dir_path)\n        # If the current directory has no subdirectories, but has files, we still want to keep the directory.\n        elif files:\n            if root != root_dir:  # Avoid adding the root directory itself if it has files\n                data_subfolders.append(root)\n\n    return data_subfolders\n\n\n# use the find_data_subfolders function to see what has been downloaded\nchm_subfolders = find_data_subfolders(r'./data/DP3.30015.001')\nchm_subfolders # display the subfolders\n\n\n# see the path of the zip file that was downloaded (ends in .zip):\nfor root, dirs, files in os.walk(r'./data/DP3.30015.001'):\n    for name in files:\n        if name.endswith('.zip'):\n            print(os.path.join(root, name))  # print file name\n\n\n# unzip the lidar tile boundary file\n# uncomment if you skipped the download section, and comment the next line (starting with \"with ZipFile\")\n# with ZipFile(f\"./data/shapefiles/2025_SERC_7_TileBoundary.zip\", 'r') as zip_ref: \nwith ZipFile(f\"./data/DP3.30015.001/neon-aop-provisional-products/2025/FullSite/D02/2025_SERC_7/Metadata/DiscreteLidar/TileBoundary/2025_SERC_7_TileBoundary.zip\",'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles/2025_SERC_7_TileBoundary')\n\n\n# read in the tile bounadry shapefile\naop_tile_boundaries = gpd.read_file(\"./data/shapefiles/2025_SERC_7_TileBoundary/shps/NEON_D02_SERC_DPQA_2025_merged_tiles_boundary.shp\")\n\n\n# append this last boundary file to the existing neon_shapefiles list\nneon_shapefiles.append(aop_tile_boundaries)\n\n# plot all NEON SERC shapefiles together\nmap3 = plot_folium_shapes(\n    shapefiles=neon_shapefiles,\n    names=['NEON AOP Flight Bounding Box', 'NEON Terrestrial Sampling Boundaries', 'NEON Tower Airshed', 'AOP Tile Boundaries'],\n    styles=styles,\n    zoom_start=12\n)\n\nmap3\n\n\n \n\nAOP, OS, and IS polygons at the SERC site.\n\n\n\nFrom the image above, you can see why the data are called “tiles”! The individual tiles make up a grid comprising the full site. These smaller areas make it easier to process the large data, and allow for batch processing instead of running an operation on a huge file, which might cause memory errors.\n\n\n3.3 Download NEON Reflectance Data using neonutilities by_tile_aop\nNow that we’ve explored the spatial extent of the NEON airborne data, as well as the OS terrestrial sampling plots and the IS tower airshed, we can start playing with the data! First, let’s download a single tile to start. For this exercise we’ll download the tile that encompasses the NEON tower, since there is typically more OS sampling in the NEON tower plots. The tile of interest for us is 365000_4305000 (note that AOP tiles are named by the SW or lower-left coordinate of the tile).\nYou can specify the download path using the savepath variable. Let’s set it to a data directory in line with the root directory where we’re working, in this case we’ll set it to ./data/NEON/refl.\nThe reflectance data are large in size (especially for an entire site’s worth of data), so by default the download functions will display the expected download size and ask if you want to proceed with the download (y/n). The reflectance tile downloaded below is ~ 660 MB in size, so make sure you have enough space on your local disk (or cloud platform) before downloading. If you want to download without being prompted to continue, you can set the input variable check_size=False.\nBy default the files will be downloaded following the same structure that they are stored in on Google Cloud Storage, so the actual data files are nested in sub-folders. We encourage you to navigate through the data/DP3.30006.002 folder, and explore the additional metadata (such as QA reports) that are downloaded along with the data.\n\n# download the tile that encompasses the NEON tower\nnu.by_tile_aop(dpid='DP3.30006.002',\n               site=site_id,\n               year=2025,\n               easting=364005,\n               northing=4305005,\n               include_provisional=True,\n               token=NEON_TOKEN,\n               savepath='./data')\n\nYou can either navigate to the download folder in File Explorer, or to programmatically see what files were downloaded, you can display the files as follows:\n\n# see all files that were downloaded (including data, metadata, and READMEs):\nfor root, dirs, files in os.walk(r'./data/DP3.30006.002'):\n    for name in files:\n        print(os.path.join(root, name))  # print file name\n\nYou can see there are several .txt and .csv files in addition to the .h5 data file (NEON_D02_SERC_DP3_364000_4305000_bidirectional_reflectance.h5). These include citation information: citation_DP3.30006.002_PROVISIONAL.txt, an issue log: issueLog_DP3.30006.002.csv, and a README: NEON.D02.SERC.DP3.30006.002.readme.20250719T050120Z.txt. We encourage you to look through these files, particularly the issue log, which conveys information about issues and the resolution for the data product in question. Make sure there is not a known issue with the data you downloaded, especially since it is provisional.\nIf you only want to see the names of the .h5 reflectance data you downloaded, you can modify the code as follows:\n\n# see only the .h5 files that were downloaded\nfor root, dirs, files in os.walk(r'./data/DP3.30006.002'):\n    for name in files:\n        if name.endswith('.h5'):\n            print(os.path.join(root, name))  # print file name\n\nSuccess! We’ve now downloaded a NEON bidirectional surface reflectance tile into our data directory.",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#read-in-and-visualize-reflectance-data-interactively",
    "href": "neon/02_neon-refl-classification.html#read-in-and-visualize-reflectance-data-interactively",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "4. Read in and visualize reflectance data interactively",
    "text": "4. Read in and visualize reflectance data interactively\n\n4.1 Convert Reflectance Data to an xarray Dataset\nThe function below will read in a NEON reflectance hdf5 dataset and export an xarray dataset. According to the xarray documentation, “xarray makes working with labelled multi-dimensional arrays in Python simple, efficient, and fun!” rioxarray is simply the rasterio xarray extension, so you can work with xarray for geospatial data.\n\ndef aop_h5refl2xarray(h5_filename):\n    \"\"\"\n    Reads a NEON AOP reflectance HDF5 file and returns an xarray.Dataset with reflectance and weather quality indicator data.\n\n    Parameters\n    ----------\n    h5_filename : str\n        Path to the NEON AOP reflectance HDF5 file.\n\n    Returns\n    -------\n    dsT : xarray.Dataset\n        An xarray Dataset containing:\n            - 'reflectance': DataArray of reflectance values (y, x, wavelengths)\n            - 'weather_quality_indicator': DataArray of weather quality indicator (y, x)\n            - Coordinates: y (UTM northing), x (UTM easting), wavelengths, fwhm, good_wavelengths\n            - Metadata attributes: projection, spatial_ref, EPSG, no_data_value, scale_factor, bad_band_window1, bad_band_window2, etc.\n    \"\"\"\n    import h5py\n    import numpy as np\n    import xarray as xr\n\n    with h5py.File(h5_filename) as hdf5_file:\n        print('Reading in ', h5_filename)\n        sitename = list(hdf5_file.keys())[0]\n        h5_refl_group = hdf5_file[sitename]['Reflectance']\n        refl_dataset = h5_refl_group['Reflectance_Data']\n        refl_array = refl_dataset[()].astype('float32')\n\n        # Transpose and flip reflectance data\n        refl_arrayT = np.transpose(refl_array, (1, 0, 2))\n        refl_arrayT = refl_array[::-1, :, :]\n\n        refl_shape = refl_arrayT.shape\n        wavelengths = h5_refl_group['Metadata']['Spectral_Data']['Wavelength'][:]\n        fwhm = h5_refl_group['Metadata']['Spectral_Data']['FWHM'][:]\n\n        # Weather Quality Indicator: transpose and flip to match reflectance\n        wqi_array = h5_refl_group['Metadata']['Ancillary_Imagery']['Weather_Quality_Indicator'][()]\n        wqi_arrayT = np.transpose(wqi_array, (1, 0))\n        wqi_arrayT = wqi_array[::-1, :]\n\n        # Collect metadata\n        metadata = {}\n        metadata['shape'] = refl_shape\n        metadata['no_data_value'] = float(refl_dataset.attrs['Data_Ignore_Value'])\n        metadata['scale_factor'] = float(refl_dataset.attrs['Scale_Factor'])\n        metadata['bad_band_window1'] = h5_refl_group.attrs['Band_Window_1_Nanometers']\n        metadata['bad_band_window2'] = h5_refl_group.attrs['Band_Window_2_Nanometers']\n        metadata['projection'] = h5_refl_group['Metadata']['Coordinate_System']['Proj4'][()].decode('utf-8')\n        metadata['spatial_ref'] = h5_refl_group['Metadata']['Coordinate_System']['Coordinate_System_String'][()].decode('utf-8')\n        metadata['EPSG'] = int(h5_refl_group['Metadata']['Coordinate_System']['EPSG Code'][()])\n\n        # Parse map info for georeferencing\n        map_info = str(h5_refl_group['Metadata']['Coordinate_System']['Map_Info'][()]).split(\",\")\n        pixel_width = float(map_info[5])\n        pixel_height = float(map_info[6])\n        x_min = float(map_info[3]); x_min = int(x_min)\n        y_max = float(map_info[4]); y_max = int(y_max)\n        x_max = x_min + (refl_shape[1]*pixel_width); x_max = int(x_max)\n        y_min = y_max - (refl_shape[0]*pixel_height); y_min = int(y_min)\n\n        # Calculate UTM coordinates for x and y axes\n        x_coords = np.linspace(x_min, x_max, num=refl_shape[1]).astype(float)\n        y_coordsT = np.linspace(y_min, y_max, num=refl_shape[0]).astype(float)\n\n        # Flag good/bad wavelengths (1=good, 0=bad)\n        good_wavelengths = np.ones_like(wavelengths)\n        for bad_window in [metadata['bad_band_window1'], metadata['bad_band_window2']]:\n            bad_indices = np.where((wavelengths &gt;= bad_window[0]) & (wavelengths &lt;= bad_window[1]))[0]\n            good_wavelengths[bad_indices] = 0\n        good_wavelengths[-10:] = 0\n\n        # Create xarray DataArray for reflectance\n        refl_xrT = xr.DataArray(\n            refl_arrayT,\n            dims=[\"y\", \"x\", \"wavelengths\"],\n            name=\"reflectance\",\n            coords={\n                \"y\": (\"y\", y_coordsT),\n                \"x\": (\"x\", x_coords),\n                \"wavelengths\": (\"wavelengths\", wavelengths),\n                \"fwhm\": (\"wavelengths\", fwhm),\n                \"good_wavelengths\": (\"wavelengths\", good_wavelengths)\n            }\n        )\n\n        # Create xarray DataArray for Weather Quality Indicator\n        wqi_xrT = xr.DataArray(\n            wqi_arrayT,\n            dims=[\"y\", \"x\"],\n            name=\"weather_quality_indicator\",\n            coords={\n                \"y\": (\"y\", y_coordsT),\n                \"x\": (\"x\", x_coords)\n            }\n        )\n\n        # Create xarray Dataset and add metadata as attributes\n        dsT = xr.Dataset({\n            \"reflectance\": refl_xrT,\n            \"weather_quality_indicator\": wqi_xrT\n        })\n        for key, value in metadata.items():\n            if key not in ['shape', 'extent', 'ext_dict']:\n                dsT.attrs[key] = value\n\n        return dsT\n\nNow that we’ve defined a function that reads in the reflectance hdf5 data and exports an xarray dataset, we can apply this function to our downloaded reflectance data. This should take around 15 seconds or so to run.\n\n%%time\n# uncomment the line below and comment the following one if running from the Openscapes platform and you skipped the download step\n# serc_refl_h5 = serc_refl_h5 = r'../../shared-public/data/neon/NEON_D02_SERC_DP3_364000_4305000_bidirectional_reflectance.h5'\nserc_refl_h5 = r'./data/DP3.30006.002/neon-aop-provisional-products/2025/FullSite/D02/2025_SERC_7/L3/Spectrometer/Reflectance/NEON_D02_SERC_DP3_364000_4305000_bidirectional_reflectance.h5'\nserc_refl_xr = aop_h5refl2xarray(serc_refl_h5)\n\nNext let’s define a function that updates the neon reflectance xarray dataset to apply the no data value (-9999), set the bad bands to NaN, and applies the CRS to make the xarray objet an rioxarray object. These could also be incorporated into the function above, but you may wish to work with unscaled reflectance data, for example, so we will keep these functions separate for now.\n\ndef update_neon_xr(neon_refl_ds):\n\n    # Set no data values (-9999) equal to np.nan\n    neon_refl_ds.reflectance.data[neon_refl_ds.reflectance.data == -9999] = np.nan\n    \n    # Scale by the reflectance scale factor\n    neon_refl_ds['reflectance'].data = ((neon_refl_ds['reflectance'].data) /\n                                        (neon_refl_ds.attrs['scale_factor']))\n    \n    # Set \"bad bands\" (water vapor absorption bands and noisy bands) to NaN\n    neon_refl_ds['reflectance'].data[:,:,neon_refl_ds['good_wavelengths'].data==0.0] = np.nan\n\n    neon_refl_ds.rio.write_crs(f\"epsg:{neon_refl_ds.attrs['EPSG']}\", inplace=True)\n    \n    return neon_refl_ds\n\nApply this function on our xarray dataset.\n\nserc_refl_xr = update_neon_xr(serc_refl_xr)\n\n\n\n4.2 Visualize the reflectance dataset\nDisplay the dataset. You can use the up and down arrows to the left of the table (e.g. to the left of Dimensions, Coordinates, Data variables, etc.) to explore each part of the dataset in more detail. You can also click on the icons to the right to see more details.\n\nserc_refl_xr\n\n\n# function to auto-scale to make RGB images more realistic\ndef gamma_adjust(rgb_ds, bright=0.2, white_background=False):\n    array = rgb_ds.reflectance.data\n    gamma = math.log(bright)/math.log(np.nanmean(array)) # Create exponent for gamma scaling - can be adjusted by changing 0.2 \n    scaled = np.power(np.nan_to_num(array,nan=1),np.nan_to_num(gamma,nan=1)).clip(0,1) # Apply scaling and clip to 0-1 range\n    if white_background == True:\n        scaled = np.nan_to_num(scaled, nan = 1) # Set NANs to 1 so they appear white in plots\n    rgb_ds.reflectance.data = scaled\n    return rgb_ds\n\n\n\n4.3 Plot the reflectance dataset\nNow let’s plot a true color (or RGB) image of the reflectance data as shown in the cell below.\n\n# Plot the RGB image of the SERC tower tile\nserc_refl_rgb = serc_refl_xr.sel(wavelengths=[650, 560, 470], method='nearest')\nserc_refl_rgb = gamma_adjust(serc_refl_rgb,bright=0.3,white_background=True)\n\nserc_rgb_plot = serc_refl_rgb.hvplot.rgb(y='y',x='x',bands='wavelengths',\n                         xlabel='UTM x',ylabel='UTM y',\n                         title='NEON AOP Reflectance RGB - SERC Tower Tile',\n                         frame_width=480, frame_height=480)\n\n# Set axis format to integer (no scientific notation)\nserc_rgb_plot = serc_rgb_plot.opts(\n    xformatter='%.0f',\n    yformatter='%.0f'\n)\n\nserc_rgb_plot\n\n\n\n4.4 Plot the weather quality indicator data\nWe can look at the weather conditions during the flight by displaying the weather_quality_indicator data array. This is a 2D array with values ranging from 1 to 3, where: 1 = &lt;10% cloud cover, 2 = 10-50% cloud cover, 3 = &gt;50% cloud cover. NEON uses a stop-light convention to indicate the weather and cloud conditions, where green (1) is good, yellow (2) is moderate, and red (3) is poor. The figure below shows some examples of these three conditions as captured by the flight operators during science flights.\n\n \n\nCloud cover percentage during AOP flights. Left: green (&lt;10%), Middle: yellow (10-50%), Right: red (&gt;50%).\n\n\n\nLet’s visualize this weather quality indicator data for this SERC tile using a transparent color on top of our RGB reflectance plot, following the same stop-light convention.\n\n# Prepare the WQI mask\nwqi = serc_refl_xr.weather_quality_indicator\n\n# Map WQI values to colors: 1=green, 2=yellow, 3=red, 0/other=transparent\nwqi_colors = ['#228B22', '#FFFF00', '#FF0000']\n# wqi_mask = wqi[wqi &gt; 0]  # mask out zeros or nodata\n\n# Use hvplot with categorical colormap and alpha (50% transparency)\nwqi_overlay = wqi.hvplot.image(\n    x='x', y='y', cmap=wqi_colors,\n    clim=(1, 3), colorbar=False, alpha=0.5, \n    xlabel='UTM x', ylabel='UTM y',\n    title='NEON AOP Reflectance Weather Quality - SERC Tower Tile',\n    frame_width=480, frame_height=480)\n\n# Overlay the RGB and WQI\n(serc_rgb_plot * wqi_overlay).opts(title=\"RGB + Weather Quality Indicator\").opts(xformatter='%.0f',\n                                                                                 yformatter='%.0f')\n\nThe cloud conditions for this tile are yellow, which indicates somewhere between 10-50% cloud cover, which is moderate. This is not ideal for reflectance data, but it is still usable. As we will use this data for classification, you would want to consider how the cloud cover may impact your results. You may wish to find a clear-weather (&lt;10% cloud cover) tile to run classification, or at a minimum compare results between the two to better understand how cloud cover impacts the model.\n\n\n4.5 Plot a false color image\nLet’s continue visualizing the data, next by making a False-Color image, which is a 3-band combination that shows you more than what you would see with the naked eye. For example, you can pull in SWIR or NIR bands to create an image that shows more information about vegetation health Here we will use a SWIR band (2000 nm), a NIR band (850 nm), and blue band (450 nm). Try some different band combinations on your own, remembering not to use bands that are flagged as bad (e.g. the last 10 bands, or those in the bad band windows between 1340-1445 nm and between 1790-1955 nm).\n\n# Plot a False-Color image of the SERC tower tile\nserc_refl_false_color = serc_refl_xr.sel(wavelengths=[2000, 850, 450], method='nearest')\nserc_refl_false_color = gamma_adjust(serc_refl_false_color,bright=0.3,white_background=True)\nserc_refl_false_color.hvplot.rgb(y='y',x='x',bands='wavelengths',\n                         xlabel='UTM x',ylabel='UTM y',\n                         title='NEON AOP Reflectance False Color Image - SERC Tower Tile',\n                         frame_width=480, frame_height=480).opts(xformatter='%.0f', yformatter='%.0f')\n\n\n\n4.6 Make an interactive spectral signature plot\nWe can also make an interactive plot that displays the spectral signature of the reflectance data for any pixel you click on. This is useful for exploring the spectral signature of different land cover types, and can help you identify which bands may be most useful for classification.\n\n# Interactive Points Plotting\n# Modified from https://github.com/auspatious/hyperspectral-notebooks/blob/main/03_EMIT_Interactive_Points.ipynb\nPOINT_LIMIT = 10\ncolor_cycle = hv.Cycle('Category20')\n\n# Create RGB Map\nmap = serc_refl_rgb.hvplot.rgb(x='x', y='y',\n                               bands='wavelengths',\n                               fontscale=1.5,\n                               xlabel='UTM x', ylabel='UTM y',\n                               frame_width=480, frame_height=480).opts(xformatter='%.0f', yformatter='%.0f')\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = serc_refl_rgb.x.values[int(len(serc_refl_rgb.x) / 2)]\nymid = serc_refl_rgb.y.values[int(len(serc_refl_rgb.y) / 2)]\n\nx0 = serc_refl_rgb.x.values[0]\ny0 = serc_refl_rgb.y.values[0]\n\n# first_point = ([xmid], [ymid], [0])\nfirst_point = ([x0], [y0], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        data = serc_refl_xr.sel(x=x, y=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return serc_refl_xr.sel(x=x,y=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths', color='black', frame_width=400)\n    # return emit_ds.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n    #                                                                         color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#supervised-classification-using-tos-vegetation-structure-data",
    "href": "neon/02_neon-refl-classification.html#supervised-classification-using-tos-vegetation-structure-data",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "5. Supervised Classification Using TOS Vegetation Structure Data",
    "text": "5. Supervised Classification Using TOS Vegetation Structure Data\nIn the last part of this lesson, we’ll go over an example of how to run a supervised classification using the reflectance data along with observational “vegetation structure” data. We will create a random forest model to classify the families of trees represented in this SERC tile, using species determined from the vegetation structure data product DP1.10098.001. See the notebook Make Training Data for Species Modeling from NEON TOS Vegetation Structure Dat to learn how the vegetation structure data were pre-processed to generate the training data file. In this notebook, we will just read in this file as a starting point.\nNote that this is a quick-and-dirty example, and there are many ways you could improve the classification results, such as using more training data (this uses only data within this AOP tile), filtering out sub-optimal data (e.g. data collected in &gt; 10 % cloud cover conditions, removing outliers (e.g. due to geospatial mis-match, shadowing, or other issues), tuning the model parameters, or using a different classification algorithm.\nLet’s get started, first by exploring the training data.\n\n5.1 Read in the training data\nFirst, read in the training data csv file (called serc_2025_training_data.csv) that was generated in the previous lesson. This file contains the training data for the random forest model, including the taxonId, family, and geographic coordinates (UTM easting and northing) of the training points. Note that there was not a lot of extensive pre-processing when creating this training data, so you may want to consider ways to assess and improve the training data quality before running the model.\n\nwoody_veg_data = pd.read_csv(r\"./data/serc_training_data.csv\")\nwoody_veg_data.head()\n\nWe can use the xarray sel method to select the reflectance data corresponding to the training points. This will return an xarray dataset with the reflectance values for each band at the training point locations. As a test, let’s plot the reflectance values for the first training point, which corresponds to an American Beech tree (Fagaceae family).\n\n# Define the coordinates of the first training data pixel\neasting = woody_veg_data.iloc[0]['adjEasting']\nnorthing = woody_veg_data.iloc[0]['adjNorthing']\n\n# Extract the reflectance data from serc_refl_xr for the specified coordinates\npixel_value = serc_refl_xr.sel(x=easting, y=northing, method='nearest')\npixel_value.reflectance\n\n# Plot the reflectance values for the pixel\nplt.plot(pixel_value['wavelengths'].values.flatten(), pixel_value['reflectance'].values.flatten(), 'o');\n\nAs another test, we can plot the refletance value for one of the water bodies that shows up in the reflectance data. In the interactive plot, hover your mouse over one of the water bodies to see the UTM x, y coordinates, and then set those as the easting and northing, as shown below.\n\n# Define the coordinates of the pixel over the pool in the NW corner of the site\neasting = 364750\nnorthing = 4305180\n\n# Extract and plot the reflectance data from serc_refl_xr specified coordinates\npixel_value = serc_refl_xr.sel(x=easting, y=northing, method='nearest')\nplt.plot(pixel_value['wavelengths'].values.flatten(), pixel_value['reflectance'].values.flatten(), 'o');\n\nYou can see that the spectral signature of water is quite different from that of vegetation.\nNow that we’ve extracted the pixel value for a single pixel, we can extract the reflectance values for all of the training data points. We will loop through the rows of the training dataframe and use the xarray.Dataset.sel method to select the reflectance values of the pixels corresponding to the same geographic location as the training data points, and then we will convert this into a pandas DataFrame for use in the random forest model.\n\n\n5.2 Inspect the training data\nIt is good practice to visually inspect the spectral signatures of the training data, for example, as shown above, to make sure you are executing the code correctly, and that there aren’t any major outliers (e.g. you might catch instances of geographic mismatch between the terrestrial location and the airborne data, or if there was a shadowing effect that caused the reflectance values to be very low).\n\n# Get the wavelengths as column names for reflectance\nwavelengths = serc_refl_xr.wavelengths.values\nwavelength_cols = [f'refl_{int(wl)}' for wl in wavelengths]\n\nrecords = []\n\nfor idx, row in woody_veg_data.iterrows():\n    # Find nearest pixel in xarray\n    y_val = serc_refl_xr.y.sel(y=row['adjNorthing'], method='nearest').item()\n    x_val = serc_refl_xr.x.sel(x=row['adjEasting'], method='nearest').item()\n    # Extract reflectance spectrum\n    refl = serc_refl_xr.reflectance.sel(y=y_val, x=x_val).values\n    # Build record: taxonID, easting, northing, reflectance values\n    record = {\n        'taxonID': row['taxonID'],\n        'family': row['family'],\n        'adjEasting': row['adjEasting'],\n        'adjNorthing': row['adjNorthing'],\n    }\n    # Add reflectance values with wavelength column names\n    record.update({col: val for col, val in zip(wavelength_cols, refl)})\n    records.append(record)\n\nNow create a dataframe from these records, and display. You can see that the reflectance values are in columns named refl_381, refl_386, etc., and the family is in the family column.\n\nreflectance_df = pd.DataFrame.from_records(records)\n# display the updated dataframe, which now includes the reflectance values for all \nreflectance_df\n\nDisplay the unique taxonIDs and families represented in this training data set:\n\nreflectance_df.taxonID.unique()\n\n\nreflectance_df.family.unique()\n\nNext we can manipulate the dataframe using melt to reshape the data and make it easier to display the reflectance spectra for each family. This is a helpful first step to visualizing the data and understanding what we’re working with before getting into the classification model.\nAfter re-shaping, we can make a figure to display what the spectra look like for the different families that were recorded as part of the vegetation structure data.\n\n# Melt (re-shape) the dataframe; wavelength columns start with 'refl_'\nmelted_df = reflectance_df.melt(\n    id_vars=['family', 'adjEasting', 'adjNorthing'],\n    value_vars=[col for col in reflectance_df.columns if col.startswith('refl_')],\n    var_name='wavelength',\n    value_name='reflectance'\n)\n\n# Convert 'wavelength' from 'refl_XXX' to integer\nmelted_df['wavelength'] = melted_df['wavelength'].str.replace('refl_', '').astype(int)\n\n# Create a summary dataframe that aggregates statistics (mean, min, and max)\nsummary_df = (\n    melted_df\n    .groupby(['family', 'wavelength'])\n    .reflectance\n    .agg(['mean', 'min', 'max'])\n    .reset_index()\n)\n\nplt.figure(figsize=(12, 7))\n\n# Create a color palette\npalette = sns.color_palette('hls', n_colors=summary_df['family'].nunique())\n\n# Plot the mean reflectance spectra for each family, filling with semi-transparent color between the min and max value\nfor i, (family, group) in enumerate(summary_df.groupby('family')):\n    # print(family)\n    if family in ['Fagaceae','Magnoliaceae','Hamamelidaceae','Juglandaceae','Aceraceae']:\n    # Plot mean line\n        plt.plot(group['wavelength'], group['mean'], label=family, color=palette[i])\n        # Plot min-max fill\n        plt.fill_between(\n            group['wavelength'],\n            group['min'],\n            group['max'],\n            color=palette[i],\n            alpha=0.2\n        )\n\nplt.xlabel('Wavelength')\nplt.ylabel('Reflectance')\nplt.title('Average Reflectance Spectra by Family \\n (with Min/Max Range)')\nplt.legend(title='family')\nplt.tight_layout()\nplt.show()\n\nWe can see that the spectral signatures for the different families have similar shapes, and there is a decent amount of spread in the reflectance values for each family. Some of this spread may be due to the cloud conditions during the time of acquisition. Reflectance values of one species may vary depending on how cloudy it was, or whether there was a cloud obscuring the sun during the collection. The random forest model may not be able to fully distinguish between the different families based on their spectral signatures, but we will see!\n\n\n5.3 Set up the Random Forest Model to Classify Tree Families\nWe can set up our random forest model by following the steps below:\n\nPrepare the training data by dropping the family column and setting the family column as the target variable. Remove the bad bands (NaN) from the reflectance predictor variables.\nSplit the data into training and testing sets.\nTrain the random forest model on the training data.\nEvaluate the model on the testing data.\nVisualize the results.\n\nWe will need to import scikit-learn (sklearn) packages in order to run the random forest model. If you don’t have these packages installed, you can install them using !pip install scikit-learn.\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\n\n5.4 Prepare and clean the training data\n\n# 1. Prepare the Data\n# Identify reflectance columns\nrefl_cols = [col for col in reflectance_df.columns if col.startswith('refl_')]\n\n# Remove rows with any NaN in reflectance columns (these are the 'bad bands')\nclean_df = reflectance_df.dropna(axis=1)\n# re-define refl_columns after removing the ones that are all NaN\nrefl_cols = [col for col in clean_df.columns if col.startswith('refl_')]\n\n\n# display the cleaned dataframe\nclean_df\n\nNote that we have &gt; 360 predictor variables (reflectance values for each band), and only 100 training points, so the model may not perform very well, due to over fitting. You can try increasing the number of training points by using more of the training data, or by using a different classification algorithm. Recall that we just pulled woody vegetation data from this tile that covers the tower, and there are also data collected throughout the rest of the TOS terrestrial sampling plots, so you could pull in more training data from the other tiles as well. You would likely not need all of the reflectance bands - for example, you could take every 2nd or 3rd band, or perform a PCA to reduce the number of bands. These are all things you could test as part of your model. For this lesson, we will include all of the valid reflectance bands for the sake of simplicity.\nThat said, we will need to remove some of the families that are poorly represented in the training data, as they will not be able to be predicted by the model. We can do this by filtering out families that have less than 10 training points. If you leave these in, the model will not be able to predict them, and will return an error when you try to evaluate the model.\n\n# determine the number of training data points for each family\nclean_df[['taxonID','family']].groupby('family').count().sort_values('taxonID', ascending=False)\n\n\n# Remove the rows where there are fewer than 10 samples\n# List of families to remove\nfamilies_to_remove = ['Rosaceae', 'Pinaceae', 'Ulmaceae']\n\n# Remove rows where family is in the families_to_remove list\nclean_df = clean_df[~clean_df['family'].isin(families_to_remove)].copy()\n\n\n\n5.5 Encode the target variable\nNext we need to encode the target variable (family) as integers, so that the model can work properly. Encoding is the process of converting from human-readable text (words / characters) to the byte representations used by computers. We can do this using the LabelEncoder from scikit-learn.\n\n# Encode the Target Variable (family)\n# Machine learning models require numeric targets. Use LabelEncoder:\nle = LabelEncoder()\nclean_df['family_encoded'] = le.fit_transform(clean_df['family'])\n# Display the cleaned dataframe after encoding the target variable\nclean_df[['taxonID','family','family_encoded','adjEasting','adjNorthing','refl_381','refl_2461']].head()\n\n\n# Confirm that the number of unique encodings is the same as the number of unique families, as a sanity check\nclean_df['family_encoded'].nunique()\n\n\n\n5.6 Split the data into training and testing sets\nIn this next step, we will split the data into training and testing sets. We will use 80% of the data for training and 20% for testing (this is the default split). This is a common practice in machine learning to test the performance of the model, and to ensure that the model is able to generalize to new data (e.g. you’re not over-fitting).\n\n# Split Data into Train/Test Sets\nX = clean_df[refl_cols].values\ny = clean_df['family_encoded'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n\n\n5.7 Create a Random Forest Classifier Object\n\n# Create a Random Forest Classifier object and fit it to the training data\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n\n\n5.8 Evaluate the Model\n\n# Determine the accuracy scores based off of the test set\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nWhat do these accuracy metrics mean?\n\nPrecision: Of the items predicted as a given class, what fraction were actually that class?\n(Precision = True Positives / (True Positives + False Positives))\nRecall: Of all actual items of a given class, what fraction were correctly predicted?\n(Recall = True Positives / (True Positives + False Negatives))\nF1-score: The harmonic mean of precision and recall. It balances both metrics.\n(F1 = 2 * (Precision * Recall) / (Precision + Recall))\nSupport: The number of true instances of each class in the dataset (i.e., how many samples belong to each class).\n\nThese metrics are commonly used to evaluate classification models. Ideally we would be closer to 1 for the precision, recall, and f1-scores, which would indicate that the model is performing well. The support values indicate how many training points were used for each family, and you can see that some families have very few training points, which is likely negatively impacting the model performance.",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#discussion",
    "href": "neon/02_neon-refl-classification.html#discussion",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "Discussion",
    "text": "Discussion\nGreat job! You now should have a fairly good grasp on working with NEON airborne and field datasets together.\nWhat are some things you could do to improve the classification results?\nModels are only as good as the underlying training data - so the better the training data (more + higher quality training data points) the better your results will be.\nYou could consider some of the following options:\n\nIncrease the number of training points: Use more training data from the other plots (and use more AOP tiles). You could also collect your own additional data within the NEON site.\nFilter out sub-optimal data: Remove training points that were collected in poor weather conditions (e.g. &gt; 10% cloud cover), or that are outliers (e.g. due to spatial mis-match, shadowing, or other issues).\nAverage the reflectance values over an entire tree crown: In this example, we just pulled the reflectance values from a single pixel, but you could average the reflectance values over an entire tree crown to get a more representative value for each tree. If part of the tree is in shadow, you may want to remove that pixel from the average.\nTune the model parameters: You can adjust the hyperparameters of the random forest model, such as the number of trees, maximum depth, and minimum samples per leaf, to improve performance.\nUse a different classification algorithm: Random forest is a good starting point, but you could also try other algorithms such as support vector machines, gradient boosting, or neural networks to see if they perform better.",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#next-steps",
    "href": "neon/02_neon-refl-classification.html#next-steps",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "Next Steps",
    "text": "Next Steps\nIn this example, we just scratched the surface of what you can do with NEON reflectance data. Here are some next steps you could take to further explore and analyze the data:\n\nApply the model to the entire reflectance dataset: Use the trained model to predict the tree families for all pixels in the reflectance dataset, and visualize the results. You may wish to include more training data for non-tree species, since the AOP data also captures non-vegetation such as water bodies, buildings, roads, etc.\nTry out the same model on SERC AOP data that was acquired in better weather conditions: Use the reflectance data from SERC 2017, which was collected in clearer weather conditions, to see if the model performs better. Note that you may need to make some minor modifications to the aop_h5refl2xarray function to accommodate the slightly different data structure of the directional reflectance data product (2019 data is not yet available with BRDF and topographic corrections, as of August 2025).\nExplore other NEON sites: Use the neonutilities package to explore reflectance data from other NEON sites, and compare the spectral signatures of different land cover types.\nAdd in other NEON AOP datasets: In this lesson, we only looked at the reflectance data. How might other NEON data products compliment this analysis? For example, you could look at the lidar data to get information about the structure of the vegetation, for example the Canopy Height Model (CHM) or the Digital Surface Model (DSM). You could also look at the AOP imagery data.\nUse the reflectance data for other applications: The reflectance data can be used for a variety of applications, such as mapping vegetation health, detecting disease or invasive species, and mapping droughts, wildfires, or other natural disturbances and their impacts. You could use a similar approach to explore some of these applications.\n\n\n# Example Challenge Solution: Apply the model to the full AOP reflectance data tile at SERC\n\n# 1. Prepare the data:\n\n# Extract the reflectance array from your xarray Dataset:\nrefl = serc_refl_xr['reflectance'].values  # shape: (y, x, bands)\n# Remove the bad bands so that we can apply the model, which only uses 363 bands\ngood_bands = serc_refl_xr['good_wavelengths'].values.astype(bool)\nrefl_good = refl[:, :, good_bands]         # shape: (y, x, n_good_bands)\n\n# 2. Reshape for prediction:\nnrows, ncols, nbands = refl_good.shape\nrefl_2d = refl_good.reshape(-1, nbands)\n\n# 3. Apply the model:\n# Use the trained random forest model (e.g., rf_model) to predict values for every pixel\npreds = clf.predict(refl_2d)\n\n# 4. Reshape predictions back to image (y, x):\npred_map = preds.reshape(nrows, ncols)\n\n# 5. Create an xarray DataArray for mapping, using the coordinates from your original data:\npred_xr = xr.DataArray(\n    pred_map,\n    dims=('y', 'x'),\n    coords={'y': serc_refl_xr['y'], 'x': serc_refl_xr['x']},\n    name='classification_prediction'\n)\n\n# 6. Plot the map, using hvplot to visualize:\n# pred_xr.hvplot.image(x='x', y='y', cmap='tab20', title='Random Forest Classification Map')\n\n\nclasses = le.classes_\nprint('classes:', classes)\n\nclass_labels = dict(enumerate(classes))\nprint('class labels:',class_labels)\n\n\n# Convert your prediction DataArray to a categorical type with labels:\npred_xr_labeled = pred_xr.copy()\npred_xr_labeled = pred_xr_labeled.assign_coords(\n    family=(('y', 'x'), np.vectorize(class_labels.get)(pred_xr.values))\n)\n\n\n# Plot the classification map, using hvplot to visualize:\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\n# Plot using hvplot with the family coordinate as the color dimension:\n\nfamily_codes = np.arange(7)\nfamily_names = classes\n# Choose 7 distinct colors (can use tab10, Set1, or your own)\ncolors = plt.get_cmap('tab10').colors[:7]  # 7 distinct colors from tab10\n\n# Create a mapping from code to color\ncode_to_color = {code: colors[i] for i, code in enumerate(family_codes)}\n\ncmap = ListedColormap([code_to_color[code] for code in family_codes])\n\npred_xr_labeled.hvplot.image(\n    x='x', y='y', groupby=[], color='family', cmap=cmap,\n    title='Random Forest Classification Map', frame_width=600, frame_height=600\n).opts(xformatter='%.0f', yformatter='%.0f')\n\n\n# Optional: Create a custom colorbar for the classification map\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\n\nfig, ax = plt.subplots(figsize=(6, 1))\nfig.subplots_adjust(bottom=0.5)\n\n# Create a colormap and norm\ncmap = ListedColormap([code_to_color[code] for code in family_codes])\nnorm = BoundaryNorm(np.arange(-0.5, 7.5, 1), cmap.N)\n\n# Create colorbar\ncb = plt.colorbar(\n    plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n    cax=ax, orientation='horizontal', ticks=family_codes\n)\ncb.ax.set_xticklabels(family_names,fontsize=6)\ncb.set_label('Family')\nplt.show()\n\n\nAcknowledgements\nMuch of this tutorial was inspired by and adapated from lessons in the NASA VITALS GitHub Repository. Thank you!",
    "crumbs": [
      "Tutorials",
      "2 NEON - Reflectance Visualization and Classification"
    ]
  },
  {
    "objectID": "resources/additional_resources.html",
    "href": "resources/additional_resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "We recommend the following tutorial series and tutorials for working with NEON AOP data. These provide example workflows at a range of levels from beginner to intermediate/advanced, in various programming languages including Python, R, and Google Earth Engine (JavaScript and Python).\n\n\n\nIntroduction to Hyperspectral Remote Sensing Data in Python\nNotebooks for working with NEON and EMIT Hyperspectral Data\n\n\n\n\n\nIntroduction to Hyperspectral Remote Sensing Data in R\nIntroduction to Light Detection and Ranging (LiDAR) in R\n\n\n\n\n\nDownload and Explore NEON Data\nUsing an API Token when Accessing NEON Data with neonUtilities\nCompare tree height measured from the ground to a Lidar-based Canopy Height Model\n\n\n\n\nNEON GEE Publisher Catalog\n\n\n\nIntro to AOP Data in Google Earth Engine (GEE) Tutorial Series\nRandom Forest Species Classification using AOP and TOS data in GEE\nPrincipal Component Analysis of AOP Hyperspectral Data in GEE\n\n\n\n\n\nIntro to AOP Datasets in Google Earth Engine (GEE) using Python\nIntro to AOP Hyperspectral Data in Google Earth Engine (GEE) using Python geemap\nExploring NEON AOP remote sensing and GBIF occurrence data in Google Earth Engine Python (geemap) to assess the impacts of a wildfire",
    "crumbs": [
      "Resources",
      "Additional Resources"
    ]
  },
  {
    "objectID": "resources/additional_resources.html#neon",
    "href": "resources/additional_resources.html#neon",
    "title": "Additional Resources",
    "section": "",
    "text": "We recommend the following tutorial series and tutorials for working with NEON AOP data. These provide example workflows at a range of levels from beginner to intermediate/advanced, in various programming languages including Python, R, and Google Earth Engine (JavaScript and Python).\n\n\n\nIntroduction to Hyperspectral Remote Sensing Data in Python\nNotebooks for working with NEON and EMIT Hyperspectral Data\n\n\n\n\n\nIntroduction to Hyperspectral Remote Sensing Data in R\nIntroduction to Light Detection and Ranging (LiDAR) in R\n\n\n\n\n\nDownload and Explore NEON Data\nUsing an API Token when Accessing NEON Data with neonUtilities\nCompare tree height measured from the ground to a Lidar-based Canopy Height Model\n\n\n\n\nNEON GEE Publisher Catalog\n\n\n\nIntro to AOP Data in Google Earth Engine (GEE) Tutorial Series\nRandom Forest Species Classification using AOP and TOS data in GEE\nPrincipal Component Analysis of AOP Hyperspectral Data in GEE\n\n\n\n\n\nIntro to AOP Datasets in Google Earth Engine (GEE) using Python\nIntro to AOP Hyperspectral Data in Google Earth Engine (GEE) using Python geemap\nExploring NEON AOP remote sensing and GBIF occurrence data in Google Earth Engine Python (geemap) to assess the impacts of a wildfire",
    "crumbs": [
      "Resources",
      "Additional Resources"
    ]
  },
  {
    "objectID": "resources/additional_resources.html#nasa-airborne",
    "href": "resources/additional_resources.html#nasa-airborne",
    "title": "Additional Resources",
    "section": "NASA Airborne",
    "text": "NASA Airborne\n\nAirborne and Field Data Resource Center\nNASA Airborne Science Data Tutorials\nBioSCape\nSHIFT\nVSWIR Imaging and Thermal Applications, Learning, and Science Repository",
    "crumbs": [
      "Resources",
      "Additional Resources"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html",
    "href": "setup/setup_instructions.html",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "The how-tos and tutorials in this repository require a NASA Earthdata account, an installation of Git, and a compatible Python Environment. Resources in this repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace.\nFor local Python environment setup we recommend using mamba to manage Python packages. To install mamba, download miniforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\n\nThese Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate ornl_daac_neon\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact ORNL DAAC User Services."
  },
  {
    "objectID": "setup/setup_instructions.html#python-environment-setup",
    "href": "setup/setup_instructions.html#python-environment-setup",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "These Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate ornl_daac_neon\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact ORNL DAAC User Services."
  }
]