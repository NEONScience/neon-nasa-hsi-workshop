[
  {
    "objectID": "setup/workshop_setup.html",
    "href": "setup/workshop_setup.html",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "If you plan to use this repository with the Openscapes 2i2c JupyterHub Cloud Workspace there are no additional setup requirements for the Python environment. All packages needed are included unless specified within a notebook, in which case a cell will be dedicated to installing the necessary Python libraries using the appropriate package manager.\nAfter completing the prerequisites you will have access to the Openscapes 2i2c JupyterHub cloud workspace. Click here to start JupyterLab. Use your email and the provided password to sign in. This password will be provided in the workshop. If you’re interested in using the 2i2c cloud workspace outside of the workshop, please contact us.\nAfter signing in you will be prompted for some server options:\nBe sure to select the radio button for Python and a size of 14.8 GB RAM and up to 3.75 CPUs.\nAt this point you can use the terminal to clone the repository.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#cloning-the-neon-data-skills-repository",
    "href": "setup/workshop_setup.html#cloning-the-neon-data-skills-repository",
    "title": "Cloud Workspace Setup",
    "section": "Cloning the NEON-Data-Skills Repository",
    "text": "Cloning the NEON-Data-Skills Repository\nIf you plan to edit or contribute to the NEON-Data-Skills repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.\nIf you just want to work with the notebooks or modules, you can simply clone the repository.\nTo clone the repository, navigate to the directory where you want to store the repository on your local machine, then type the following:\ngit clone https://github.com/NEONScience/NEON-Data-Skills.git",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#troubleshooting",
    "href": "setup/workshop_setup.html#troubleshooting",
    "title": "Cloud Workspace Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWe recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/prerequisites.html",
    "href": "setup/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nTo follow along during the workshop, or to run through the notebooks contained within the repository using the Openscapes 2i2c Cloud JupyterHub (cloud workspace), the following are required. All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov/users/new\nRemember your username and password; you will need them to download or access data during the workshop and beyond.\n\nNEON User accout and API token\n\nCreate a NEON User account (if you don’t already have one) following the instructions here: https://www.neonscience.org/about/user-accounts/\nCreate an API token and save this; you will use this to download and access NEON data during the workshop and beyond. Instructions on creating an API token can be found here: https://www.neonscience.org/resources/learning-hub/tutorials/neon-api-tokens-tutorial\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com/join. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form. You will receive an email invitation to join the organization on GitHub. You must join to gain access to the workspace.\n\n\n.netrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All workshop participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.",
    "crumbs": [
      "Setup Instructions",
      "Prerequisites"
    ]
  },
  {
    "objectID": "notebooks/01_neon-hyperspectral-functions.html",
    "href": "notebooks/01_neon-hyperspectral-functions.html",
    "title": "Read in and visualize hyperspectral data in Python",
    "section": "",
    "text": "In this tutorial, you will learn how to efficiently read in hdf5 data and metadata, plot a single band and rgb band combinations of a reflectance data tile using Python functions created for working with and visualizing NEON AOP hyperspectral data.\nThis tutorial uses the Level 3 Spectrometer orthorectified surface directional reflectance - mosaic data product.\nWe can combine any three bands from the NEON reflectance data to make an RGB image that will depict different information about the Earth’s surface. A natural color image, made with bands from the red, green, and blue wavelengths looks close to what we would see with the naked eye. We can also choose band combinations from other wavelenghts, and map them to the red, blue, and green colors to highlight different features. A false color image is made with one or more bands from a non-visible portion of the electromagnetic spectrum that are mapped to red, green, and blue colors. These images can display other information about the landscape that is not easily seen with a natural color image.\nThe NASA Goddard Media Studio video “Peeling Back Landsat’s Layers of Data” gives a good quick overview of natural and false color band combinations. Note that the Landsat multispectral sensor collects information from 11 bands, while NEON AOP hyperspectral data captures information spanning 426 bands!",
    "crumbs": [
      "Python Notebooks",
      "1 NEON - Working with Hyperspectral Data using Functions"
    ]
  },
  {
    "objectID": "notebooks/01_neon-hyperspectral-functions.html#load-function-module",
    "href": "notebooks/01_neon-hyperspectral-functions.html#load-function-module",
    "title": "Read in and visualize hyperspectral data in Python",
    "section": "Load Function Module",
    "text": "Load Function Module\nFirst we can import the required packages and the neon_aop_hyperspectral module, which includes a number of functions which we will use to read in the hyperspectral hdf5 data as well as visualize the data.\n\nimport os\nimport sys\nimport time\nimport h5py\nimport requests\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThis next function is a handy way to download the Python module and data that we will be using for this lesson. This uses the requests package.\n\n# function to download data stored on the internet in a public url to a local file\ndef download_url(url,download_dir):\n    if not os.path.isdir(download_dir):\n        os.makedirs(download_dir)\n    filename = url.split('/')[-1]\n    r = requests.get(url, allow_redirects=True)\n    file_object = open(os.path.join(download_dir,filename),'wb')\n    file_object.write(r.content)\n\nDownload the module from its location on GitHub, add the python_modules to the path and import the neon_aop_hyperspectral.py module.\n\nmodule_url = \"https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/Python/AOP/aop_python_modules/neon_aop_hyperspectral.py\"\ndownload_url(module_url,'../python_modules')\n# os.listdir('../python_modules') #optionally show the contents of this directory to confirm the file downloaded\n\nsys.path.insert(0, '../python_modules')\n# import the neon_aop_hyperspectral module, the semicolon supresses an empty plot from displaying\nimport neon_aop_hyperspectral as neon_hs;\n\nThe first function we will use is aop_h5refl2array. We encourage you to look through the code to understand what it is doing behind the scenes. This function automates the steps required to read AOP hdf5 reflectance files into a Python numpy array. This function also cleans the data: it sets any no data values within the reflectance tile to nan (not a number) and applies the reflectance scale factor so the final array that is returned represents unitless scaled reflectance, with values ranging between 0 and 1 (0-100%).\nIf you forget what this function does, or don’t want to scroll up to read the docstrings, remember you can use help or ? to display the associated docstrings.\n\nhelp(neon_hs.aop_h5refl2array)\n# neon_hs.aop_h5refl2array? #uncomment for an alternate way to show the help\n\nHelp on function aop_h5refl2array in module neon_aop_hyperspectral:\n\naop_h5refl2array(h5_filename, raster_type_: Literal['Cast_Shadow', 'Data_Selection_Index', 'GLT_Data', 'Haze_Cloud_Water_Map', 'IGM_Data', 'Illumination_Factor', 'OBS_Data', 'Radiance', 'Reflectance', 'Sky_View_Factor', 'to-sensor_Azimuth_Angle', 'to-sensor_Zenith_Angle', 'Visibility_Index_Map', 'Weather_Quality_Indicator'], only_metadata=False)\n    read in NEON AOP reflectance hdf5 file and return the un-scaled \n    reflectance array, associated metadata, and wavelengths\n           \n    Parameters\n    ----------\n        h5_filename : string\n            reflectance hdf5 file name, including full or relative path\n        raster : string\n            name of raster value to read in; this will typically be the reflectance data, \n            but other data stored in the h5 file can be accessed as well\n            valid options: \n                Cast_Shadow (ATCOR input)\n                Data_Selection_Index\n                GLT_Data\n                Haze_Cloud_Water_Map (ATCOR output)\n                IGM_Data\n                Illumination_Factor (ATCOR input)\n                OBS_Data \n                Reflectance\n                Radiance\n                Sky_View_Factor (ATCOR input)\n                to-sensor_Azimuth_Angle\n                to-sensor_Zenith_Angle\n                Visibility_Index_Map: sea level values of visibility index / total optical thickeness\n                Weather_Quality_Indicator: estimated percentage of overhead cloud cover during acquisition\n    \n    Returns \n    --------\n    raster_array : ndarray\n        array of reflectance values\n    metadata: dictionary \n        associated metadata containing\n            bad_band_window1 (tuple)\n            bad_band_window2 (tuple)\n            bands: # of bands (float)\n            data ignore value: value corresponding to no data (float)\n            epsg: coordinate system code (float)\n            map info: coordinate system, datum & ellipsoid, pixel dimensions, and origin coordinates (string)\n            reflectance scale factor: factor by which reflectance is scaled (float)\n    wavelengths: array\n            wavelength values, in nm\n    --------\n    Example Execution:\n    --------\n    refl, refl_metadata = aop_h5refl2array('NEON_D02_SERC_DP3_368000_4306000_reflectance.h5','Reflectance')\n\n\n\nNow that we have an idea of how this function works, let’s try it out. First, let’s download a file. For this tutorial, we will use requests to download from the public link where the data is stored on the cloud (Google Cloud Storage). This downloads to a data folder in the working directory, but you can download it to a different location if you prefer.\n\n# define the data_url to point to the cloud storage location of the the hyperspectral hdf5 data file\ndata_url = \"https://storage.googleapis.com/neon-aop-products/2021/FullSite/D03/2021_DSNY_6/L3/Spectrometer/Reflectance/NEON_D03_DSNY_DP3_454000_3113000_reflectance.h5\"\n\n\n# download the h5 data and display how much time it took to download (uncomment 1st and 3rd lines)\n# start_time = time.time()\ndownload_url(data_url,'.\\data')\n# print(\"--- It took %s seconds to download the data ---\" % round((time.time() - start_time),1))\n\n\n# display the contents in the ./data folder to confirm the download completed\nos.listdir('./data')\n\n['NEON_D03_DSNY_DP3_454000_3113000_reflectance.h5']\n\n\n\n# read the h5 reflectance file (including the full path) to the variable h5_file_name\nh5_file_name = data_url.split('/')[-1]\nh5_tile = os.path.join(\".\\data\",h5_file_name)\nprint(f'h5_tile: {h5_tile}')\n\nh5_tile: .\\data\\NEON_D03_DSNY_DP3_454000_3113000_reflectance.h5\n\n\nNow that we’ve specified our reflectance tile, we can call aop_h5refl2array to read in the reflectance tile as a python array called refl , the metadata into a dictionary called refl_metadata, and the wavelengths into an array.\n\n# read in the reflectance data using the aop_h5refl2array function, this may also take a bit of time\nstart_time = time.time()\nrefl, refl_metadata, wavelengths = neon_hs.aop_h5refl2array(h5_tile,'Reflectance')\nprint(\"--- It took %s seconds to read in the data ---\" % round((time.time() - start_time),0))\n\nReading in  .\\data\\NEON_D03_DSNY_DP3_454000_3113000_reflectance.h5\n--- It took 7.0 seconds to read in the data ---\n\n\n\n# display the reflectance metadata dictionary contents\nrefl_metadata\n\n{'shape': (1000, 1000, 426),\n 'no_data_value': -9999.0,\n 'scale_factor': 10000.0,\n 'bad_band_window1': array([1340, 1445]),\n 'bad_band_window2': array([1790, 1955]),\n 'projection': b'+proj=UTM +zone=17 +ellps=WGS84 +datum=WGS84 +units=m +no_defs',\n 'EPSG': 32617,\n 'res': {'pixelWidth': 1.0, 'pixelHeight': 1.0},\n 'extent': (454000.0, 455000.0, 3113000.0, 3114000.0),\n 'ext_dict': {'xMin': 454000.0,\n  'xMax': 455000.0,\n  'yMin': 3113000.0,\n  'yMax': 3114000.0},\n 'source': '.\\\\data\\\\NEON_D03_DSNY_DP3_454000_3113000_reflectance.h5'}\n\n\n\n# display the first 5 values of the wavelengths\nwavelengths[:5]\n\narray([383.884 , 388.8917, 393.8995, 398.9072, 403.915 ], dtype=float32)\n\n\nWe can use the shape method to see the dimensions of the array we read in. Use this method to confirm that the size of the reflectance array makes sense given the hyperspectral data cube, which is 1000 meters x 1000 meters x 426 bands.\n\nrefl.shape\n\n(1000, 1000, 426)",
    "crumbs": [
      "Python Notebooks",
      "1 NEON - Working with Hyperspectral Data using Functions"
    ]
  },
  {
    "objectID": "notebooks/01_neon-hyperspectral-functions.html#plot_aop_refl-plot-a-single-band-of-the-reflectance-data",
    "href": "notebooks/01_neon-hyperspectral-functions.html#plot_aop_refl-plot-a-single-band-of-the-reflectance-data",
    "title": "Read in and visualize hyperspectral data in Python",
    "section": "plot_aop_refl: plot a single band of the reflectance data",
    "text": "plot_aop_refl: plot a single band of the reflectance data\nNext we’ll use the function plot_aop_refl to plot a single band of reflectance data. You can use help to understand the required inputs and data types for each of these; only the band and spatial extent are required inputs, the rest are optional inputs. If specified, these optional inputs allow you to set the range color values, specify the axis, add a title, colorbar, colorbar title, and change the colormap (default is to plot in greyscale).\n\nband56 = refl[:,:,55]\n\n\nneon_hs.plot_aop_refl(band56/refl_metadata['scale_factor'],\n                      refl_metadata['extent'],\n                      colorlimit=(0,0.3),\n                      title='DSNY Tile Band 56',\n                      cmap_title='Reflectance',\n                      colormap='gist_earth')",
    "crumbs": [
      "Python Notebooks",
      "1 NEON - Working with Hyperspectral Data using Functions"
    ]
  },
  {
    "objectID": "notebooks/01_neon-hyperspectral-functions.html#rgb-plots---band-stacking",
    "href": "notebooks/01_neon-hyperspectral-functions.html#rgb-plots---band-stacking",
    "title": "Read in and visualize hyperspectral data in Python",
    "section": "RGB Plots - Band Stacking",
    "text": "RGB Plots - Band Stacking\nIt is often useful to look at several bands together. We can extract and stack three reflectance bands in the red, green, and blue (RGB) spectrums to produce a color image that looks like what we see with our eyes; this is your typical camera image. In the next part of this tutorial, we will learn to stack multiple bands and make a geotif raster from the compilation of these bands. We can see that different combinations of bands allow for different visualizations of the remotely-sensed objects and also conveys useful information about the chemical makeup of the Earth’s surface.\nWe will select bands that fall within the visible range of the electromagnetic spectrum (400-700 nm) and at specific points that correspond to what we see as red, green, and blue.\n\n \n\nNEON Imaging Spectrometer bands and their respective wavelengths. Source: National Ecological Observatory Network (NEON)\n\n\n\nFor this exercise, we’ll first use the function stack_rgb to extract the bands we want to stack. This function uses splicing to extract the nth band from the reflectance array, and then uses the numpy function stack to create a new 3D array (1000 x 1000 x 3) consisting of only the three bands we want.\n\n# pull out the true-color band combinations\nrgb_bands = (58,34,19) # set the red, green, and blue bands\n\n# stack the 3-band combinations (rgb and cir) using stack_rgb function\nrgb_unscaled = neon_hs.stack_rgb(refl,rgb_bands)\n\n# apply the reflectance scale factor\nrgb = rgb_unscaled/refl_metadata['scale_factor']\n\nWe can display the red, green, and blue band center wavelengths, whose indices were defined above. To confirm that these band indices correspond to wavelengths in the expected portion of the spectrum, we can print out the wavelength values in nanometers.\n\nprint('Center wavelengths:')\nprint('Band 58: %.1f' %(wavelengths[57]),'nm')\nprint('Band 33: %.1f' %(wavelengths[33]),'nm')\nprint('Band 19: %.1f' %(wavelengths[18]),'nm')\n\nCenter wavelengths:\nBand 58: 669.3 nm\nBand 33: 549.1 nm\nBand 19: 474.0 nm",
    "crumbs": [
      "Python Notebooks",
      "1 NEON - Working with Hyperspectral Data using Functions"
    ]
  },
  {
    "objectID": "notebooks/01_neon-hyperspectral-functions.html#plot_aop_rgb-plot-an-rgb-band-combination",
    "href": "notebooks/01_neon-hyperspectral-functions.html#plot_aop_rgb-plot-an-rgb-band-combination",
    "title": "Read in and visualize hyperspectral data in Python",
    "section": "plot_aop_rgb: plot an RGB band combination",
    "text": "plot_aop_rgb: plot an RGB band combination\nNext, we can use the function plot_aop_rgb to plot the band stack as follows:\n\n# plot the true color image (rgb)\nneon_hs.plot_aop_rgb(rgb,\n                     refl_metadata['extent'],\n                     plot_title='DSNY Reflectance RGB Image')",
    "crumbs": [
      "Python Notebooks",
      "1 NEON - Working with Hyperspectral Data using Functions"
    ]
  },
  {
    "objectID": "notebooks/01_neon-hyperspectral-functions.html#false-color-image---color-infrared-cir",
    "href": "notebooks/01_neon-hyperspectral-functions.html#false-color-image---color-infrared-cir",
    "title": "Read in and visualize hyperspectral data in Python",
    "section": "False Color Image - Color Infrared (CIR)",
    "text": "False Color Image - Color Infrared (CIR)\nWe can also create an image from bands outside of the visible spectrum. An image containing one or more bands outside of the visible range is called a false-color image. Here we’ll use the green and blue bands as before, but we replace the red band with a near-infrared (NIR) band.\nFor more information about non-visible wavelengths, false color images, and some frequently used false-color band combinations, refer to NASA’s Earth Observatory page.\n\ncir_bands = (90,34,19)\nprint('Band 90 Center Wavelength = %.1f' %(wavelengths[89]),'nm')\nprint('Band 34 Center Wavelength = %.1f' %(wavelengths[33]),'nm')\nprint('Band 19 Center Wavelength = %.1f' %(wavelengths[18]),'nm')\n\ncir = neon_hs.stack_rgb(refl,cir_bands)\nneon_hs.plot_aop_rgb(cir,\n                     refl_metadata['extent'],\n                     ls_pct=20,\n                     plot_title='DSNY Color Infrared Image')\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nBand 90 Center Wavelength = 829.6 nm\nBand 34 Center Wavelength = 549.1 nm\nBand 19 Center Wavelength = 474.0 nm",
    "crumbs": [
      "Python Notebooks",
      "1 NEON - Working with Hyperspectral Data using Functions"
    ]
  },
  {
    "objectID": "notebooks/01_neon-hyperspectral-functions.html#references",
    "href": "notebooks/01_neon-hyperspectral-functions.html#references",
    "title": "Read in and visualize hyperspectral data in Python",
    "section": "References",
    "text": "References\nKekesi, Alex et al.   “NASA | Peeling Back Landsat’s Layers of Data”.  https://svs.gsfc.nasa.gov/vis/a010000/a011400/a011491/. Published on Feb 24, 2014.\nRiebeek, Holli.  “Why is that Forest Red and that Cloud Blue? How to Interpret a False-Color Satellite Image”  https://earthobservatory.nasa.gov/Features/FalseColor/",
    "crumbs": [
      "Python Notebooks",
      "1 NEON - Working with Hyperspectral Data using Functions"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "",
    "text": "NASA’s airborne science program and the NSF-funded National Ecological Observatory Network’s (NEON’s) Airborne Observation Platform (AOP) offer complementary remote sensing datasets ideal for carrying out large-scale ecological research. Both facilities operate similar airborne imaging spectrometers to collect visible to shortwave infrared (VSWIR) hyperspectral data, supporting regional ecosystem studies.\nThe Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC) archives data from NASA-funded ecological campaigns focusing on diverse environments such as river deltas and wetlands, the arctic, and tropics across multiple continents (North and Central America, South Africa). NEON’s AOP gathers high-resolution hyperspectral imagery, lidar, and RGB photography at 81 U.S. sites, offering repeat data spanning 2-10 years, with collections starting in 2013.\nThis workshop introduces NEON and NASA airborne and field datasets through live-coding exercises presented as Python Jupyter Notebook tutorials, demonstrating data access, exploration, and analysis. Participants will learn to apply these datasets to answer ecological research questions, gaining insights into regional and landscape areas of interest.\nThis workshop is hosted by National Ecological Observatory Network (NEON), NASA Land Processes Distributed Activate Archive Center (LP DAAC) and NASA Jet Propulsion Laboratory (JPL) with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance. Instructions for setting up the Python environment locally are provided in the setup instructions.",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n8:00 AM\nIntroduction: NEON and NASA Airborne and Field Data\nBridget Hass and Michele Thornton\n\n\n8:05 AM\nOverview of NEON Airborne Observation Platform\nBridget Hass\n\n\n8:30 AM\nNotebook 1: Download and Explore NEON AOP Hyperspectral Data\nBridget Hass\n\n\n9:00 AM\nNotebook 2: Pairing NEON AOP Hyperspectral and Field Data\nBridget Hass\n\n\n9:30 AM\nBreak\n\n\n\n9:35 AM\nOverview of Recent NASA Airborne Missions (SHIFT, BioSCape, AUVELO)\nMichele Thornton\n\n\n10:00 AM\nNotebook 3: Discovery and Analysis of VegPlot and AVIRIS Instrument Data\nMichele Thornton\n\n\n10:55 AM\nDiscussion and Wrap Up\nAll",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "Contact Info",
    "text": "Contact Info\nNEON AOP\nOrganization: National Ecological Observatory Network Airborne Observation Platform (NEON AOP)1\nWebsite: https://neonscience.org/\nContact: https://www.neonscience.org/about/contact-us/\n1NEON is a project fully funded by the National Science Foundation and operated by Battelle.\nORNL DAAC\nOrganization: NASA Earthdata Data Center, Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)\nWebsite: https://www.earthdata.nasa.gov/centers/ornl-daac\nContact:\nNASA Earthdata Forum: https://forum.earthdata.nasa.gov/\nORNL DAAC - uso@daac.ornl.gov",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.\n\n\n\n\nBe respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.\n\n\n\n\nThe following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.\n\n\n\n\nIf you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.\n\n\n\nViolations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-commitment",
    "href": "CODE_OF_CONDUCT.html#our-commitment",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#expected-behavior",
    "href": "CODE_OF_CONDUCT.html#expected-behavior",
    "title": "Code of Conduct",
    "section": "",
    "text": "Be respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "href": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "title": "Code of Conduct",
    "section": "",
    "text": "The following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#reporting-violations",
    "href": "CODE_OF_CONDUCT.html#reporting-violations",
    "title": "Code of Conduct",
    "section": "",
    "text": "If you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Code of Conduct",
    "section": "",
    "text": "Violations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html",
    "href": "background/nasa_neon_comparison.html",
    "title": "Comparing NEON and NASA Airborne Campaigns",
    "section": "",
    "text": "While NASA and NEON operate similar imaging spectrometer instruments, there are a number of differences in two airborne campaigns and datasets. This section provides a high-level overview of some of the similarities and differences between the two.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#comparison-of-nasa-aviris-v.-neon-aop-imaging-spectrometer-datasets",
    "href": "background/nasa_neon_comparison.html#comparison-of-nasa-aviris-v.-neon-aop-imaging-spectrometer-datasets",
    "title": "Comparing NEON and NASA Airborne Campaigns",
    "section": "Comparison of NASA AVIRIS v. NEON AOP Imaging Spectrometer Datasets",
    "text": "Comparison of NASA AVIRIS v. NEON AOP Imaging Spectrometer Datasets\n\n\n\n\n\n\n\n\nParameter\nNASA AVIRIS\nNEON AOP\n\n\n\n\nWhere?\nWorldwide\n80 sites throughout the continental U.S., Alaska, Hawaii, and Puerto Rico\n\n\nWhen?\n…\n2013 - present; Each site is collected 3 out of every 5 years\n\n\nWhy?\nSpecific research questions\nLong-term ecological monitoring of the United States",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-processing",
    "href": "background/nasa_neon_comparison.html#data-processing",
    "title": "Comparing NEON and NASA Airborne Campaigns",
    "section": "Data Processing",
    "text": "Data Processing\n\nData products\nNEON data are processed to Level 2 (flightline) and Level 3 (tiled mosaic) data products, and include a number of derived data products such as vegetation and water indices, LAI, fPAR, and Albedo. NASA data are processed to Level 1 (flightline) radiance.\n\n\nAtmospheric correction:\nNEON uses ATCOR-4 for the atmospheric correction. NASA uses ISOFIT …",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-storage-and-access",
    "href": "background/nasa_neon_comparison.html#data-storage-and-access",
    "title": "Comparing NEON and NASA Airborne Campaigns",
    "section": "Data Storage and Access",
    "text": "Data Storage and Access\n\nNEON\nNEON data are stored on Google Cloud Storage (GCS) and are accessible via the NEON Data Portal. A subset of the L3 data products are also available on Google Earth Engine.\nNEON provides an API for downloading from the Data Portal, and has developed tools in R (neonUtilities) and Python (neonutilities) for downloading NEON data, and wrangling OS and IS data.\n\n\nNASA\nNASA airborne data are stored on AWS and can be accessed through the ORNL DAAC. NASA provides tools including Earthdata Search to help data users discover and download datasets.\nThis workshop will provide more details on accessing NEON and NASA airborne datasets in the Jupyter Notebooks section.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/neon_background.html",
    "href": "background/neon_background.html",
    "title": "NEON Airborne and Field Datasets",
    "section": "",
    "text": "NEON is a continental-scale observation facility designed to collect long-term open-access ecological data to better understand the complexities of Earth’s ecosystems and how they are changing. NEON uses cutting-edge sensor networks, instrumentation, observational sampling, natural history archive facilities and remote sensing methods and technologies to collect data on plants, animals, soil, nutrients, freshwater and the atmosphere.\nNEON operates 81 field sites strategically located across 20 ecoclimatic Domains across the United States, including 47 terrestrial sites and 34 freshwater aquatic sites. When logistically possible, aquatic and terrestrial field sites are colocated (i.e. in close proximity) to support understanding of linkages across terrestrial and aquatic ecosystems and their interactions with the atmosphere. For example, Domain 08, the Ozarks Complex, has three co-located sets of terrestrial and aquatic field sites. These sites are situated along the same watershed system, creating a unique opportunity to study hydrology, nutrient transport, and biogeochemical cycling through the watershed.\n\n\n\nNEON Field Sites Map; Green: Terrestrial Sites, Blue: Aquatic Sites",
    "crumbs": [
      "Background",
      "NEON Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#what-is-neon",
    "href": "background/neon_background.html#what-is-neon",
    "title": "NEON Airborne and Field Datasets",
    "section": "",
    "text": "NEON is a continental-scale observation facility designed to collect long-term open-access ecological data to better understand the complexities of Earth’s ecosystems and how they are changing. NEON uses cutting-edge sensor networks, instrumentation, observational sampling, natural history archive facilities and remote sensing methods and technologies to collect data on plants, animals, soil, nutrients, freshwater and the atmosphere.\nNEON operates 81 field sites strategically located across 20 ecoclimatic Domains across the United States, including 47 terrestrial sites and 34 freshwater aquatic sites. When logistically possible, aquatic and terrestrial field sites are colocated (i.e. in close proximity) to support understanding of linkages across terrestrial and aquatic ecosystems and their interactions with the atmosphere. For example, Domain 08, the Ozarks Complex, has three co-located sets of terrestrial and aquatic field sites. These sites are situated along the same watershed system, creating a unique opportunity to study hydrology, nutrient transport, and biogeochemical cycling through the watershed.\n\n\n\nNEON Field Sites Map; Green: Terrestrial Sites, Blue: Aquatic Sites",
    "crumbs": [
      "Background",
      "NEON Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#neon-airborne-observation-platform-aop",
    "href": "background/neon_background.html#neon-airborne-observation-platform-aop",
    "title": "NEON Airborne and Field Datasets",
    "section": "NEON Airborne Observation Platform (AOP)",
    "text": "NEON Airborne Observation Platform (AOP)\n\n\n\nNEON Airborne Remote Sensing\n\n\nAirborne remote sensing surveys are conducted over NEON field sites during peak greenness and provide quantitative information on land cover and changes to ecological structure and chemistry, including the presence and effects of invasive species. The surveys are supported by the NEON Airborne Observation Platform (AOP), a suite of earth observation instruments installed into a Twin Otter aircraft designed to collect high-resolution remote sensing data at low altitude. AOP was designed to collect regional-scale landscape information at the NEON field sites. The AOP maps areas where NEON’s observational and instrumented sampling is occurring and allows relationships to be drawn between NEON’s detailed in-situ observations to the broader environmental and ecological conditions.\n\nAOP Payload Sensors\nThe AOP consists of three complete and comparable instrument payloads. Typically, two of the payloads are dedicated to collections of the NEON field sites while the third is dedicated to NEON’s Research Support services which support externally driven research. The primary sensors on each payload include\n\nA discrete and full-waveform lidar to provide three-dimensional structural information of the landscape,\nAn imaging spectrometer to allow discrimination of land cover types and chemical content of vegetation,\nA high-resolution digital camera to provide spatially accurate and detailed contextual information, and\nA GPS antenna and receiver and Inertial Measurement Unit (IMU) to provide high-accuracy positioning and orientation of the aircraft.\n\n\n\nAOP Data Products\nThe AOP produces approximately 30 data products. The products are separated into categories of Level 1, Level 2, and Level 3 (L1, L2, L3). L1 represents the least processed data products. Additional processing steps are required to transition the L1 data to the derived L2 and L3 data. Broadly, the L1 and L2 products are provided by individual aircraft flight line, while L3 products are provided in 1 km by 1 km tiles. Generally, the data volume for L1 products is the highest and decreases for L2 and L3 products. Details of the different products within each Level can be found in the individual webpages for each sensor. All AOP data products can be found on the NEON Data Portal, and a subset of the L3 data products are available on Google Earth Engine.\n\nImaging Spectrometer Data Products\nLevel 1 (L1) products include at-sensor radiance and surface reflectance which are distributed by flightline. The image data is georeferenced to the ITRF00 datum and projected into the appropriate UTM zone, and provided at 1 m spatial resolution. Both the radiance and reflectance image data are stored in an HDF5 file format that includes extensive metadata and data quality information. The HDF5 format was selected because of the flexibility it allows in storing associated metadata.\nLevel 2 (L2) products are derived from the L1 surface reflectance and are produced at the same spatial resolution (1 m), datum and map projection as the Level 1 products. The L2 products include a suite of spectral indices designed to strategically combine bands to highlight vegetation characteristics such as photosynthetic activity or water content. For example, NDVI (Normalized Difference Vegetation Index) is a well-known and commonly used vegetation index which combines information from the NIR and Red regions to estimate vegetative greenness and can be used as a proxy for plant health. The L2 products also include fPAR (fraction of photosynthetically active radiation) and LAI (leaf area index), products further derived from vegetation indices. Additionally, a surface Albedo product that estimates the integrated reflectance of all the NIS bands into a single value is also provided. All L2 products are distributed by flightline in a GeoTIFF (gtiff) format. Currently, all vegetation indices, water indices, fPAR, and LAI are delivered with associated simulated error images.\nLevel 3 (L3) products include mosaics of all L1 and L2 products, excluding at-sensor radiance, and are distributed as 1 km x 1 km tiles instead of flightlines. Tiles are created by making a full mosaic of all the data and sub-setting the 1 km x 1 km tiles. The tiles are designed so their boundaries are set to even 1000 m UTM coordinate intervals. During the mosaic generation, the algorithm preferentially selects pixels that were collected under the best weather conditions in regions with multiple potential pixels due to flightline overlap. If weather conditions were equivalent, pixels acquired nearest to nadir of the image acquisition are selected. Generally, this will correspond to pixels that are nearest to the center of the flightline. The tiles are created at the same spatial resolution (1 m) as the L1 and L2 products are in delivered in gtiff format, with the exception of the surface reflectance, which is delivered in HDF5 format.\n\n\nBRDF and topographic corrections\nStarting in 2024, NEON began producing BRDF (Bidirectional Reflectance Distribution Function) and topographic corrected reflectance data, which include “bidirectional” in the name, and end with revision .002 in the Data Product IDs. As of 2025, these bidirectional reflectance are currently only available for data collected between 2022-2024. NEON is beginning to back-process earlier years (pre-2022) to apply the BRDF and topographic corrections. Please look at the data availability charts for each product on the data portal to determine whether the bidirectional data are available. Eventually, only bidirectional data products will be delivered, with the exception of the Level 1 Spectrometer orthorectified surface directional reflectance (DP1.30006.001), which will continue to be delivered, so that researchers who wish to carry out their own BRDF, topographic, or other corrections may do so.\nThe table below shows a full list of the spectrometer-derived data products, including the corresponding bidirectional reflectance data products, if applicable.\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nBRDF-Corrected DPID\n\n\n\n\nSpectrometer orthorectified at-sensor radiance\nL1\nDP1.30008.001\n\n\n\nSpectrometer orthorectified surface (bi)directional reflectance\nL1\nDP1.30006.001\nDP1.30006.002\n\n\nAlbedo - spectrometer - flightline\nL2\nDP2.30011.001\nDP2.30011.002\n\n\nLAI - spectrometer - flightline\nL2\nDP2.30012.001\nDP2.30012.002\n\n\nfPAR - spectrometer - flightline\nL2\nDP2.30014.001\nDP2.30014.002\n\n\nCanopy water indices - flightline\nL2\nDP2.30019.001\nDP2.30019.002\n\n\nVegetation indices - spectrometer - flightline\nL2\nDP2.30026.001\nDP2.30026.002\n\n\nAlbedo - spectrometer - mosaic\nL3\nDP3.30011.001\nDP3.30011.002\n\n\nLAI - Spectrometer - mosaic\nL3\nDP3.30012.001\nDP3.30012.002\n\n\nfPAR - spectrometer - mosaic\nL3\nDP3.30014.001\nDP3.30014.002\n\n\nCanopy water indices - mosaic\nL3\nDP3.30019.001\nDP3.30019.002\n\n\nVegetation indices - spectrometer - mosaic\nL3\nDP3.30026.001\nDP3.30026.002\n\n\n\nIn addition to the spectrometer-derived data products, NEON generates 5 lidar-derived products and 2 RGB camera data products. These are summarized in the tables below.\n\n\nLiDAR Data Products\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nATBD Document #\n\n\n\n\nLiDAR Slant Range Waveform\nL1\nDP1.30001.001\nNEON.DOC.001293\n\n\nDiscrete Return LiDAR Point Cloud\nL1\nDP1.30003.001\nNEON.DOC.001292, NEON.DOC.001288\n\n\nEcosystem Structure\nL3\nDP3.30015.001\nNEON.DOC.002387\n\n\nElevation – LiDAR\nL3\nDP3.30024.001\nNEON.DOC.002390\n\n\nSlope and Aspect – LiDAR\nL3\nDP3.30025.001\nNEON.DOC.003791\n\n\n\n\n\nRGB Camera Products\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nATBD Document #\n\n\n\n\nHigh-resolution orthorectified camera imagery\nL1\nDP1.30010.001\nNEON.DOC.001211vB\n\n\nHigh-resolution orthorectified camera imagery mosaic\nL3\nDP3.30010.001\nNEON.DOC.005052vB",
    "crumbs": [
      "Background",
      "NEON Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#neon-field-data",
    "href": "background/neon_background.html#neon-field-data",
    "title": "NEON Airborne and Field Datasets",
    "section": "NEON Field Data",
    "text": "NEON Field Data\nIn addition to the AOP remote sensing data, NEON also provides Observational Sampling (OS) data and Instrumented Sampling (IS) data at terrestrial and aquatic sites. The field and instrumented sampling are briefly described below, but we encourage exploring the NEON website further for a more detailed understanding of the sensors and data products provided by the OS and IS groups.\n\nObservational Sampling\n\n\n\nNEON Observational Samples\n\n\nNEON field scientists collect a broad variety of observations and samples at terrestrial and aquatic field sites at regular intervals throughout the year. The data and samples collected by NEON’s Aquatic Observation System (AOS) and Terrestrial Observation System (TOS) are designed to provide standardized, continentally distributed observations of organisms, biogeochemistry, and physical properties.\n\n\nInstrumented Sampling\n\n\n\nNEON Instrumented Sampling\n\n\nNEON deploys automated instruments to collect meteorological, soil, phenological, surface water, and groundwater data at NEON field sites.\nWhere logistically possible, NEON colocated aquatic sites with terrestrial sites (21 in total) to support an understanding of linkages across atmospheric, terrestrial, and aquatic ecosystems. The suite of OS, IS, and AOP data provide an unparalleled opportunity to study ecosystem-level change over time in the United States.",
    "crumbs": [
      "Background",
      "NEON Background"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "Please submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\nWe want your help! Even if you’re not a coder! There are several ways you can contribute to this repository:\n\nReport an Issue or make a recommendation\nUpdate code, documentation, notebooks, or other files (even fixing typos)\nPropose a new notebook\n\nIn the sections below we outline how to approach each of these types of contributions. If you’re new to GitHub, you can sign up here. There are a bunch of great resources on the GitHub Quickstart page. The GitHub Cheatsheet is also quite helpful, even for experienced users. Please reach out to lpdaac@usgs.gov with questions or concerns.\n\n\nIf you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!\n\n\n\nTo contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\n\n\n\n\nIn the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a community_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the community_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.\n\n\n\nThese contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "href": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "If you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "href": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "To contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "href": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "In the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a community_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the community_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#attribution",
    "href": "CONTRIBUTING.html#attribution",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "These contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "neon-nasa.html",
    "href": "neon-nasa.html",
    "title": "NEON and NASA Airborne and Field Tutorials",
    "section": "",
    "text": "Please view the NEON NASA Workshop Page for workshop details. Resources in this repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace. For local Python environment setup instructions please see the Setup Instructions.\nWelcome to the NEON - NASA Airborne Hyperspectral Data Resources Repository! This repository provides Python Jupyter notebooks to help the community work with visible to short-wave infrared imaging spectroscopy data from NEON’s Airbonre Observation Platform and missions carried out with the NASA AVIRIS sensor. These complimentary hyperspectral datasets provide an opportunity to …\nIn the interest of open science, this repository has been made public, but is still under active development. Make sure to consult the change log for the most recent changes to the repository. Contributions from all parties are welcome.",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "neon-nasa.html#contact-info",
    "href": "neon-nasa.html#contact-info",
    "title": "NEON and NASA Airborne and Field Tutorials",
    "section": "Contact Info",
    "text": "Contact Info\nNEON AOP\nOrganization: National Ecological Observatory Network Airborne Observation Platform (NEON AOP)1\nWebsite: https://neonscience.org/\nContact: https://www.neonscience.org/about/contact-us/\n1NEON is a project fully funded by the National Science Foundation and operated by Battelle.\nORNL DAAC\nOrganization: NASA Earthdata Data Center, Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)\nWebsite: https://www.earthdata.nasa.gov/centers/ornl-daac\nContact:\nNASA Earthdata Forum: https://forum.earthdata.nasa.gov/\nORNL DAAC - uso@daac.ornl.gov\nDate last modified: 06-17-2025",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "",
    "text": "In this introductory tutorial, we demonstrate how to read NEON AOP bidirectional hyperspectral reflectance (Level 3, tiled - DP3.30006.002) data in Python. For a more general introduction to Hyperspectral remote sensing data in Python, please refer to the related lesson: NEON AOP Hyperspectral Data in HDF5 format with Python, which works with the previous revision of the same reflectance data product.\nIn Spring 2024, AOP started producing revised (.002) spectrometer data products, which incorporate Bidirectional Reflectance Distribution Function (BRDF) and topographic corrections. Airborne hyperspectral data acquired between 2022 - 2024 have been processed with these corrections, and downstream Level 2 and Level 3 derived spectrometer data products (eg. vegetation and water indices, fPAR, LAI, etc.) are now generated from this bidirectional (BRDF-corrected) reflectance data. The L1 directional reflectance data will still be available under the original .001 revision # (DP1.30006.001). Eventually, all previous years of data (2013-2021) will also be re-processed to apply the BRDF and topographic corrections. Updates on this progress will be posted as Data Notifications on the NEON Data Portal.\nThe new bidirectional data includes some slight modifications to the H5 contents, including some additional fields specific to the BRDF corrections. This tutorial outlines the major differences and highlights information you may want to incorporate when working with this revised data product. The tutorial also covers fundamental steps of reading in and exploring the HDF5 (h5) format that the reflectance data is delivered in. You will learn skills to explore and visualize the spectral data, and learn to make some functions for streamlining this process. .",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#brdf-and-topographic-corrections",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#brdf-and-topographic-corrections",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "BRDF and Topographic Corrections",
    "text": "BRDF and Topographic Corrections\n\nBRDF Correction\nObjects appear differently when viewed from different angles, and when illuminated from different directions. The Bidirectional Reflectance Distribution Function (BRDF) describes the directional dependence of the reflected energy of a target as a function of illumination and viewing geometry. It also depends on the wavelength and structural and optical properties of the surface. In short, the BRDF correction helps to improve continuity in brightness levels between flightlines, and helps minimize the view and illumination angle effects.\n\n\nTopographic Correction\nSteep mountain slopes can significantly affect the remote sensing of vegetation. In areas with complex terrain, slopes facing the sun receive more light and appear brighter than slopes facing away from the sun. The irradiation on a slope varies strongly with the slope azimuth relative to the sun, and the reflectance of the slope varies with the angles of incidence and exitance relative to the slope normal. The topographic correction involves standardizing the imagery for these two effects based on the slope of the terrain and its relative position with the sun.\n\n\nFlexBRDF\nNEON followed the FlexBRDF approach to perform the topographic and BRDF corrections, following Queally et al. 2022. Details of the implementation are provided in the Topographic and BRDF Corrections ATBD, which can be downloaded from the link at the bottom of the previous section. Section 4.2 in the linked document provides a short summary of the approach. The BRDF correction is applied using the University of Wisconsin Environmental Spectroscopy Lab’s Python-based open-source software HyTools.\n\n \n\nExample dataset showing the a) original and b) BRDF-corrected datasets at the NEON CPER site; c) shows the difference in RMSE between the data processed with and without the BRDF correction.",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#changes-to-bidirectional-reflectance-hdf5-files",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#changes-to-bidirectional-reflectance-hdf5-files",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "Changes to Bidirectional Reflectance HDF5 Files",
    "text": "Changes to Bidirectional Reflectance HDF5 Files\nIf you’re working with the bidirectional reflectance data for the first time, we encourage you to start by exploring the HDF5 file in HDFView, a free software program that can be downloaded from the HDF Group Download Page. This provides a more interactive way to view the contents of the hdf5 files. The figure below shows a DP3.30006.002 and DP3.30006.001 file in HDFView for comparison.\n\n\n\n\n\n\n\n\n\n\nHDFView of the new DP3.30006.002 (left) and original DP3.30006.001 (right). Highlighted fields on the left image indicate new or updated fields in the .002 revision.\n\nAs you can see, there are some differences in the HDF5 structure between the .001 (directional) and .002 (bidirectional) datasets. The major changes are summarized below, with additional details outlined in the table.\n\nAddition of an Acquisition Date Ancillary Image.\nUpdated Weather Quality Index Ancillary Image.\nUpdated Logs to use file names from the L1 reflectance, including the flight line #, with addition of the raw file name to the top level.\nConsistency in fields for Ancillary Rasters.\nConsistency in NO DATA values across Ancillary Rasters.\nAddition of BRDF metadata.\n\n\n\n\n\n\n\n\n\nHDF Group/Field\nBidirectional Reflectance (DP3.30006.002)\nDirectional Reflectance (DP3.30006.001)\n\n\n\n\nAncillary_Imagery/Acquisition_Date\nNew field added, including the date (YYMMDD) of each pixel\nInformation is derived from Data Selection Index\n\n\nAncillary_Imagery/Weather_Quality_Indicator\nCloud cover % data provided as 1/2/3 where 1 = &lt;10%, 2 = 10-50%, 3=&gt;50%\nCloud cover data provided as R/G/B color combinations where Green = &lt;10%, Yellow = 10-50%, and Red = &gt;50% cloud cover\n\n\nLogs\nLog file names reflect the Flight Line #, corresonding to the L1 Reflectance Tile\nLog file names reflect the time stamp of the flight line\n\n\nLogs\nBRDF logs added (BRDF_COEFFS_JSON_for_Hytools and BRDF_Config_JSON_for_Hytools)\nNo BRDF logs\n\n\n\nNow that you have a high-level picture of what has changed between the directional and bidirectional reflectance revisions, let’s take a more detailed look at the contents of the bidirectional reflectance data in Python.\nIn Python, you can look inside the HDF5 dataset with the h5py visititems function. The list_dataset function defined below displays all datasets stored in the hdf5 file and their locations within the hdf5 file:\n\nSet up\nFirst let’s import the required packages:\n\nimport os, shutil\nimport json\nimport h5py\nfrom neonutilities import list_available_dates, get_aop_tile_extents, by_tile_aop\nimport numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\n\n\n\nDownload the Bidirectional Reflectance Data\nYou can download the bidirectional reflectance data tile using the Python neonutilities function by_tile_aop as follows. For more details on any of the neonutilities functions, you can use help, eg. type help(by_tile_aop). If you aren’t sure what data are currently available, or the extent of data available, you can use the functions list_available_dates and get_aop_tile_extents as shown below.\n\nlist_available_dates('DP3.30006.002','LIRO')\n\nPROVISIONAL Available Dates: 2022-06\n\n\nThere are provisional data available in 2022 (2022-06 is the YYYY-MM date, so 2022-06 means those data are available in June 2022). If you don’t want to download all of the data, you can use by_tile_aop to download data encompassing specified coordinates. L3 data are provided as mosaicked 1km x 1km tiles, where the UTM coordinates of the SW corner is specified in the file name. To first determine what tiles are available, you can use the get_aop_tile_extents function, as shown in the next cell.\n\nliro_2022_refl_exts = get_aop_tile_extents('DP3.30006.002','LIRO',2022)\n\nEasting Bounds: (289000, 292000)\nNorthing Bounds: (5094000, 5101000)\n\n\nThis shows the Easting and Northing Bounds (minimum and maximum values). Type print(liro_2022_refl_exts) to display a complete list of all the UTM coordinates of the tiles. Not all of the AOP flight boxes are rectangular in shape, so for these sites that have an irregular polygon shape, it may help to see the full list of available tiles. Likewise, in some years, AOP may not obtain complete coverage of a site, due to poor weather or other logistical constraints.\nNext let’s use by_tile_aop to download a bidirectional reflectance tile. We highly encourage using a token for larger AOP downloads, such as the reflectance data. Refer to the Using an API Token when Accessing NEON Data with neonUtilities tutorial to set up a User Account and Token, if you haven’t already done so.\nBy default, this function will display the total size of the data to be download and ask you if you want to proceed. Type y for yes to continue with the download. Optionally, you can set the input parameter check_size=False if you want to download data regardless of the size. The download may take up to a minute or two to complete.\nNEON_TOKEN=\"YOUR_TOKEN_HERE\"\n\nby_tile_aop(dpid='DP3.30006.002',\n            site='LIRO',\n            year=2022,\n            easting=290001,\n            northing=5097001,\n            include_provisional=True,\n            savepath='./data',\n            token=NEON_TOKEN)\n\nProvisional data are included. To exclude provisional data, use input parameter include_provisional=False.\n\n\nContinuing will download 2 files totaling approximately 669.3 MB. Do you want to proceed? (y/n)  y\n\n\nDownloading 2 files totaling approximately 669.3 MB\n\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:57&lt;00:00, 28.51s/it]\n\n\nThe reflectance data tile is now downloaded into the ‘./data’ directory and maintains a path structure as the data is stored on Google Cloud Storage (GCS). You can use the code cell below to walk through all the directories and display where the .h5 file was downloaded.\n\nfor root, dirs, files in os.walk(\"data\"):\n    for file in files:\n        if file.endswith(\".h5\"):\n             print(os.path.join(root, file))\n\ndata\\NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5\ndata\\DP3.30006.002\\neon-aop-provisional-products\\2022\\FullSite\\D05\\2022_LIRO_3\\L3\\Spectrometer\\Reflectance\\NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5\n\n\nYou may wish to move the .h5 file to a more convenient path - for example, you could move all the .h5 files (in this case just one) downloaded to the data folder, for simplicity. You can do that as follows:\n\n# Walk through the 'data' directory\nfor root, dirs, files in os.walk(\"data\"):\n    for file in files:\n        if file.endswith(\".h5\"):\n            # Construct the full file path\n            file_path = os.path.join(root, file)\n            print(file_path)\n            \n            # Construct the destination path\n            destination_path = os.path.join(\"data\", file)\n            \n            # Move the file to the 'data' directory\n            shutil.move(file_path, destination_path)\n            print(f\"Moved {file} to {destination_path}\")\n\ndata\\NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5\nMoved NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5 to data\\NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5\ndata\\DP3.30006.002\\neon-aop-provisional-products\\2022\\FullSite\\D05\\2022_LIRO_3\\L3\\Spectrometer\\Reflectance\\NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5\nMoved NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5 to data\\NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#read-in-the-bidirectional-reflectance-.h5-dataset",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#read-in-the-bidirectional-reflectance-.h5-dataset",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "Read in the bidirectional reflectance .h5 dataset",
    "text": "Read in the bidirectional reflectance .h5 dataset\nTo start, make sure the NEON surface bidirectional reflectance data (DP3.30006.002) is downloaded (see instructions at the top of this lesson) and located in the data folder under your working directory. You can change the path, but make sure to update the script to point to where you’ve saved this file.\n\n# display the contents in the ./data folder to confirm the file is in the correct location\nos.listdir('./data')\n\n['DP3.30006.002',\n 'NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5']\n\n\nLet’s explore the hyperspectral reflectance data. Note that if the h5 file is stored in a different directory than where you are running your notebook, you need to include the path (either relative or absolute) to the directory where that data file is stored. Use os.path.join to create the full path of the file.\n\n# Note that you may need to update this filepath for your local machine\nh5_file = h5py.File('./data/NEON_D05_LIRO_DP3_290000_5097000_bidirectional_reflectance.h5','r')\n\n\n#list_dataset lists the names of datasets in an hdf5 file\ndef list_dataset(name,node):\n    if isinstance(node, h5py.Dataset):\n        print(name)\n\nh5_file.visititems(list_dataset)\n\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Acquisition_Date\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Aerosol_Optical_Thickness\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Aspect\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Cast_Shadow\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Dark_Dense_Vegetation_Classification\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Data_Selection_Index\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Haze_Cloud_Water_Map\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Illumination_Factor\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Path_Length\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Sky_View_Factor\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Slope\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Smooth_Surface_Elevation\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Visibility_Index_Map\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Water_Vapor_Column\nLIRO/Reflectance/Metadata/Ancillary_Imagery/Weather_Quality_Indicator\nLIRO/Reflectance/Metadata/Coordinate_System/Coordinate_System_String\nLIRO/Reflectance/Metadata/Coordinate_System/EPSG Code\nLIRO/Reflectance/Metadata/Coordinate_System/Map_Info\nLIRO/Reflectance/Metadata/Coordinate_System/Proj4\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/ATCOR_Input_file\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/ATCOR_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/BRDF_COEFFS_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/BRDF_Config_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/Shadow_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/Skyview_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/Solar_Azimuth_Angle\nLIRO/Reflectance/Metadata/Logs/L004-1_20220623/Solar_Zenith_Angle\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/ATCOR_Input_file\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/ATCOR_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/BRDF_COEFFS_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/BRDF_Config_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/Shadow_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/Skyview_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/Solar_Azimuth_Angle\nLIRO/Reflectance/Metadata/Logs/L005-1_20220617/Solar_Zenith_Angle\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/ATCOR_Input_file\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/ATCOR_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/BRDF_COEFFS_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/BRDF_Config_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/Shadow_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/Skyview_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/Solar_Azimuth_Angle\nLIRO/Reflectance/Metadata/Logs/L005-1_20220623/Solar_Zenith_Angle\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/ATCOR_Input_file\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/ATCOR_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/BRDF_COEFFS_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/BRDF_Config_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/Shadow_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/Skyview_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/Solar_Azimuth_Angle\nLIRO/Reflectance/Metadata/Logs/L006-1_20220617/Solar_Zenith_Angle\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/ATCOR_Input_file\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/ATCOR_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/BRDF_COEFFS_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/BRDF_Config_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/Shadow_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/Skyview_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/Solar_Azimuth_Angle\nLIRO/Reflectance/Metadata/Logs/L006-1_20220623/Solar_Zenith_Angle\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/ATCOR_Input_file\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/ATCOR_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/BRDF_COEFFS_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/BRDF_Config_JSON_for_Hytools\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/Shadow_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/Skyview_Processing_Log\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/Solar_Azimuth_Angle\nLIRO/Reflectance/Metadata/Logs/L007-1_20220623/Solar_Zenith_Angle\nLIRO/Reflectance/Metadata/Spectral_Data/FWHM\nLIRO/Reflectance/Metadata/Spectral_Data/Wavelength\nLIRO/Reflectance/Metadata/to-sensor_Azimuth_Angle\nLIRO/Reflectance/Metadata/to-sensor_Zenith_Angle\nLIRO/Reflectance/Reflectance_Data\n\n\nYou can see that there is a lot of information stored inside this reflectance hdf5 file. Most of this information is metadata (data about the reflectance data), for example, this file stores input parameters used in the atmospheric correction.\nFor this introductory lesson, we will explore some of these datasets, including the reflectance data (hyperspectral cube), and the corresponding geospatial information, stored in Metadata/Coordinate_System.\n\nReflectance/Reflectance_Data\nReflectance/Metadata/Coordinate_System/\nReflectance/Metadata/Spectral_Data/Wavelength\n\nWe will also highlight some of the new and updated datasets that differ from the previous h5 format for the directional reflectance (DP3.30006.001). These include:\n\nReflectance/Metadata/Ancillary_Imagery/Acqusition_Date\nReflectance/Metadata/Ancillary_Imagery/Weather_Quality_Indicator\nReflectance/Metadata/Logs/L004-1_20220623/BRDF_COEFFS_JSON_for_Hytools\nReflectance/Metadata/Logs/L004-1_20220623/BRDF_Config_JSON_for_Hytools\n\nThe function below pulls out some spatial information about the dataset that will come in handy for plotting.\n\ndef get_spatial_info(hdf5_file):\n    # get the site name\n    site_name = str(list(hdf5_file.items())).split(\"'\")[1]\n    product_type = str(list(hdf5_file[site_name].items())).split(\"'\")[1] # this is the Reflectance\n    base_loc = hdf5_file[site_name][product_type]\n    refl_shape = base_loc['Reflectance_Data'].shape\n    # create a metadata dictionary to store the relevant map information\n    metadata = {}\n    metadata['projection'] = base_loc['Metadata']['Coordinate_System']['Proj4'][()]\n    metadata['EPSG'] = int(base_loc['Metadata']['Coordinate_System']['EPSG Code'][()])\n    map_info = base_loc['Metadata']['Coordinate_System']['Map_Info'][()]\n    map_info_split = str(map_info).split(\",\")\n    # extract the resolution & convert to floating decimal number\n    pixel_width = float(map_info_split[5])\n    pixel_height = float(map_info_split[6])\n    # extract the upper left-hand corner coordinates from map_info and cast to float\n    x_min = float(map_info_split[3]) \n    y_max = float(map_info_split[4])\n    # calculate the x_max and yMin values from the dimensions\n    x_max = x_min + (refl_shape[1]*pixel_width) #xMax = left edge + (# of columns * x_resolution)\\n\",\n    y_min = y_max - (refl_shape[0]*pixel_height) #yMin = top edge - (# of rows * y_resolution)\\n\",\n    metadata['extent'] = (x_min,x_max,y_min,y_max)\n    return metadata\n\nRun this function on the LIRO dataset to see what it returns:\n\nmap_info = get_spatial_info(h5_file)\nmap_info\n\n{'projection': b'+proj=UTM +zone=16 +ellps=WGS84 +datum=WGS84 +units=m +no_defs',\n 'EPSG': 32616,\n 'extent': (290000.0, 291000.0, 5097000.0, 5098000.0)}\n\n\nNow that we have the spatial information, let’s start looking at the data. To start, we can pull out all the Ancillary_Imagery data into a Python dictionary as follows:\n\n# create a dictionary containing the Ancillary Images\nsitename = str(list(h5_file.items())).split(\"'\")[1] # this is LIRO for this example, but if you use another dataset, this is more generic\nancillary_image_names = list(h5_file[f'{sitename}/Reflectance/Metadata/Ancillary_Imagery'].keys())\nancillary_image_paths = [f'{sitename}/Reflectance/Metadata/Ancillary_Imagery/{im}' for im in ancillary_image_names]\nancillary_images = [h5_file[path][()] for path in ancillary_image_paths]\nanc_image_dict = dict(zip(ancillary_image_names,ancillary_images))\n# display the dictionary\nanc_image_dict\n\n{'Acquisition_Date': array([[20220623, 20220623, 20220623, ..., 20220623, 20220623, 20220623],\n        [20220623, 20220623, 20220623, ..., 20220623, 20220623, 20220623],\n        [20220623, 20220623, 20220623, ..., 20220623, 20220623, 20220623],\n        ...,\n        [20220623, 20220623, 20220623, ..., 20220623, 20220623, 20220623],\n        [20220623, 20220623, 20220623, ..., 20220623, 20220623, 20220623],\n        [20220623, 20220623, 20220623, ..., 20220623, 20220623, 20220623]]),\n 'Aerosol_Optical_Thickness': array([[194., 194., 194., ..., 200., 200., 200.],\n        [194., 194., 194., ..., 200., 200., 200.],\n        [194., 194., 194., ..., 200., 200., 200.],\n        ...,\n        [161., 161., 161., ..., 211., 211., 211.],\n        [161., 161., 161., ..., 211., 211., 211.],\n        [161., 161., 161., ..., 211., 211., 211.]], dtype=float32),\n 'Aspect': array([[261.08093 , 246.32469 , 240.37682 , ...,  86.42367 ,  93.01279 ,\n          90.      ],\n        [245.40097 , 226.64857 , 225.40874 , ...,  83.29016 ,  81.027374,\n          66.80141 ],\n        [256.24622 , 226.46211 , 226.57805 , ...,  98.972626,  98.972626,\n         123.69006 ],\n        ...,\n        [149.98001 , 149.26245 , 147.871   , ...,  41.82997 ,  40.090603,\n          39.70494 ],\n        [150.28685 , 150.06934 , 148.8032  , ...,  43.94467 ,  42.518238,\n          42.363697],\n        [150.07767 , 149.9158  , 148.82988 , ...,  47.395676,  46.086636,\n          45.983833]], dtype=float32),\n 'Cast_Shadow': array([[1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.]], dtype=float32),\n 'Dark_Dense_Vegetation_Classification': array([[3, 3, 3, ..., 1, 1, 1],\n        [3, 3, 3, ..., 1, 1, 1],\n        [3, 3, 3, ..., 1, 1, 1],\n        ...,\n        [3, 3, 3, ..., 2, 2, 2],\n        [3, 3, 3, ..., 2, 2, 2],\n        [3, 3, 3, ..., 2, 2, 2]], dtype=uint8),\n 'Data_Selection_Index': array([[12, 12, 12, ...,  9,  9,  9],\n        [12, 12, 12, ...,  9,  9,  9],\n        [12, 12, 12, ...,  9,  9,  9],\n        ...,\n        [12, 12, 12, ...,  9,  9,  9],\n        [12, 12, 12, ...,  9,  9,  9],\n        [12, 12, 12, ...,  9,  9,  9]]),\n 'Haze_Cloud_Water_Map': array([[3, 3, 3, ..., 1, 1, 1],\n        [3, 3, 3, ..., 1, 1, 1],\n        [3, 3, 3, ..., 1, 1, 1],\n        ...,\n        [3, 3, 3, ..., 2, 2, 2],\n        [3, 3, 3, ..., 2, 2, 2],\n        [3, 3, 3, ..., 2, 2, 2]], dtype=uint8),\n 'Illumination_Factor': array([[84., 85., 85., ..., 86., 86., 86.],\n        [85., 87., 86., ..., 86., 86., 86.],\n        [86., 87., 87., ..., 86., 86., 86.],\n        ...,\n        [94., 95., 95., ..., 78., 77., 77.],\n        [95., 95., 95., ..., 80., 79., 79.],\n        [95., 95., 96., ..., 82., 81., 81.]], dtype=float32),\n 'Path_Length': array([[ 987.98065,  987.52844,  987.3978 , ..., 1052.3306 , 1052.151  ,\n         1052.053  ],\n        [ 988.1091 ,  987.31824,  987.32806, ..., 1052.3868 , 1052.1865 ,\n         1051.9873 ],\n        [ 987.90106,  987.3085 ,  987.0074 , ..., 1052.3856 , 1052.165  ,\n         1051.966  ],\n        ...,\n        [1007.4887 , 1008.27515, 1015.2912 , ..., 1049.5427 , 1049.8715 ,\n         1049.3566 ],\n        [1008.6032 , 1019.3234 , 1014.8984 , ..., 1048.8473 , 1049.2275 ,\n         1046.6539 ],\n        [1011.15393, 1016.7114 , 1013.91504, ..., 1048.599  , 1042.2389 ,\n         1042.2389 ]], dtype=float32),\n 'Sky_View_Factor': array([[98., 98., 98., ..., 97., 97., 97.],\n        [98., 98., 98., ..., 97., 97., 97.],\n        [98., 98., 98., ..., 97., 97., 97.],\n        ...,\n        [95., 95., 95., ..., 90., 90., 91.],\n        [95., 95., 95., ..., 93., 93., 91.],\n        [95., 94., 94., ..., 93., 93., 93.]], dtype=float32),\n 'Slope': array([[5.1402011e+00, 4.7354989e+00, 6.0021877e+00, ..., 1.4015521e-02,\n         1.6634010e-02, 6.9941138e-03],\n        [4.3225102e+00, 4.3750796e+00, 5.8721676e+00, ..., 1.4964992e-02,\n         1.6816808e-02, 6.6581978e-03],\n        [2.8365092e+00, 2.7115109e+00, 4.3466454e+00, ..., 1.6816808e-02,\n         1.6816808e-02, 6.3044089e-03],\n        ...,\n        [1.0582998e+01, 1.1100612e+01, 1.1690731e+01, ..., 2.7058205e+01,\n         2.6974607e+01, 2.7148523e+01],\n        [1.1586640e+01, 1.2123251e+01, 1.2760822e+01, ..., 2.5439777e+01,\n         2.5439209e+01, 2.5682121e+01],\n        [1.2407374e+01, 1.2923250e+01, 1.3561447e+01, ..., 2.3443769e+01,\n         2.3602884e+01, 2.3984230e+01]], dtype=float32),\n 'Smooth_Surface_Elevation': array([[524.9416 , 525.02014, 525.0957 , ..., 495.1146 , 495.11423,\n         495.114  ],\n        [524.9064 , 524.96277, 525.0171 , ..., 495.11484, 495.11447,\n         495.11426],\n        [524.8794 , 524.9146 , 524.94904, ..., 495.1147 , 495.11432,\n         495.1141 ],\n        ...,\n        [518.654  , 518.55975, 518.45404, ..., 510.5763 , 510.24155,\n         509.9204 ],\n        [518.4886 , 518.3865 , 518.27515, ..., 510.93674, 510.61002,\n         510.29437],\n        [518.2981 , 518.1863 , 518.0675 , ..., 511.26312, 510.9449 ,\n         510.6319 ]], dtype=float32),\n 'Visibility_Index_Map': array([[26., 26., 26., ..., 27., 27., 27.],\n        [26., 26., 26., ..., 27., 27., 27.],\n        [26., 26., 26., ..., 27., 27., 27.],\n        ...,\n        [20., 20., 20., ..., 29., 29., 29.],\n        [20., 20., 20., ..., 29., 29., 29.],\n        [20., 20., 20., ..., 29., 29., 29.]], dtype=float32),\n 'Water_Vapor_Column': array([[2353., 2352., 2350., ..., 2349., 2349., 2349.],\n        [2351., 2351., 2349., ..., 2349., 2349., 2349.],\n        [2351., 2350., 2349., ..., 2349., 2349., 2349.],\n        ...,\n        [2154., 2153., 2152., ..., 2254., 2252., 2245.],\n        [2152., 2151., 2150., ..., 2261., 2258., 2252.],\n        [2151., 2150., 2148., ..., 2268., 2265., 2259.]], dtype=float32),\n 'Weather_Quality_Indicator': array([[2, 2, 2, ..., 1, 1, 1],\n        [2, 2, 2, ..., 1, 1, 1],\n        [2, 2, 2, ..., 1, 1, 1],\n        ...,\n        [2, 2, 2, ..., 1, 1, 1],\n        [2, 2, 2, ..., 1, 1, 1],\n        [2, 2, 2, ..., 1, 1, 1]])}",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#weather-quality-indicator",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#weather-quality-indicator",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "Weather Quality Indicator",
    "text": "Weather Quality Indicator\nOne of the most critical factors impacting the data quality are the weather conditions during the flight. While AOP strives to collect in optimal conditions, this is not always possible.\nThe weather quality indicator includes information about the cloud conditions during the flight, as reported by the flight operators, where 1 corresponds to &lt; 10% cloud cover, 2 corresponds to 10-50% cloud cover, and 3 corresponds to &gt; 50% cloud cover. We recommend using only clear-sky data (1) for a typical analysis, as it comprises the highest quality reflectance data. In higher cloud-cover conditions, clouds could be obscuring the sun and the level of uncertainty in the reflectance values will increase.\nPreviously, in rev .001, the Weather Quality Indicator was provided as an RGB band combination, corresponding to AOP’s “stop-light” color convention, where green (0,255,0) = 0-10%, yellow (255,255,0) = 10-50%, and red (255,0,0) = 50-100%. This has been simplified in rev .002, to use a simpler 1,2,3 convention.\nLet’s start by plotting the weather quality data (cloud conditions) for the LIRO tile. The code below has some extra formatting features to apply colors using the stoplight color codes, just for visualization.\n\n# Plot the Weather Quality Indicator with 1 as green, 2 as yellow, and 3 as red\nwqi_data = anc_image_dict['Weather_Quality_Indicator']\n\n# Create a colormap\ncmap = plt.cm.colors.ListedColormap(['green', 'yellow', 'red'])\n\n# Create a normalize object the describes the limits of each color\nbounds = [0.5, 1.5, 2.5, 3.5]\nnorm = plt.cm.colors.BoundaryNorm(bounds, cmap.N)\n\n# Plot the data\nplt.imshow(wqi_data, extent=map_info['extent'], cmap=cmap, norm=norm)\nax = plt.gca()\nax.ticklabel_format(useOffset=False, style='plain'); #do not use scientific notation for ticklabels\nrotatexlabels = plt.setp(ax.get_xticklabels(),rotation=90); #rotate x tick labels 90 degrees\n\n# Create a colorbar\ncbar = plt.colorbar()\ncbar.set_ticks([1, 2, 3])\ncbar.set_ticklabels(['1 (&lt; 10% cloud cover)', '2 (10-50% cloud cover)', '3 (&gt; 50% cloud cover)'])\n\nplt.show()\n\n\n\n\n\n\n\n\nThe mosaicking routine which generates the tiled reflectance data first selects the cloud-free pixels, then chooses the nadir-most pixels. In a single tile, there may be different cloud conditions represented, as shown in this example.",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#acquisition-date",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#acquisition-date",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "Acquisition Date",
    "text": "Acquisition Date\nYou may be interested in the date at which a given pixel was acquired, in order to link the data with ground data, or satellite data, for example. Previously, the acquisition date had to be determined from the Data_Selection_Index dataset. The acquisition date is now being provided more directly (and the Data Selection Index is still provided, and is useful if you would like to pull more detail than just the date). This Data Selection Index links each pixel with the flight log that was used to generate that pixel.\nYou can see the unique flight dates that are represented in this tile as follows:\n\nliro_flight_dates = anc_image_dict['Acquisition_Date']\nprint(f'Flight dates in this tile: {np.unique(liro_flight_dates[()])}')\n\nFlight dates in this tile: [20220617 20220623]",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#plotting-other-ancillary-imagery",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#plotting-other-ancillary-imagery",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "Plotting Other Ancillary Imagery",
    "text": "Plotting Other Ancillary Imagery\nThe function below is a more generic function for plotting any of the ancillary datasets. You may wish to customize it, as the code chunks above have, to highlight the information more clearly.\n\ndef plot_ancillary_data(dataset,colorlimit,title):\n    plot = plt.imshow(dataset,extent=map_info['extent'],cmap='Greys',clim=colorlimit); ax = plt.gca()\n    ax.ticklabel_format(useOffset=False, style='plain'); #do not use scientific notation for ticklabels\n    rotatexlabels = plt.setp(ax.get_xticklabels(),rotation=90); #rotate x tick labels 90 degrees\n    cbar = plt.colorbar(plot,aspect=40); \n    cbar.set_label('',rotation=90,labelpad=20)\n    plt.title(title);\n\nTry this out on the Slope dataset, or others of your choice.\n\nplot_ancillary_data(anc_image_dict['Slope'],colorlimit=(0,45),title='Slope')",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#brdf-correction-parameters",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#brdf-correction-parameters",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "BRDF Correction Parameters",
    "text": "BRDF Correction Parameters\nNext let’s take a quick look at the new BRDF Logs. These provide information about the coefficients and configuration files used in applying the BRDF correction as outlined in the BRDF correction ATBD.\n\nBRDF Configuration JSON\nTable 2 in the BRDF correction ATBD lists the definitions for the user-defined parameters that go into the FlexBRDF approach, which is also the same information provided in the config.json file. This information is stored within the reflectance metadata in the “Logs” for each flightline, and can be accessed in Python as follows:\n\n# BRDF Configuration\nbrdf_config = h5_file['LIRO/Reflectance/Metadata/Logs/L004-1_20220623/BRDF_Config_JSON_for_Hytools'][()]\n# use json.loads to read this in as a dictionary object\nbrdf_config_dict = json.loads(brdf_config.decode(\"utf-8\"))\n# display just the first 3 items to see the file contents\nlist(brdf_config_dict.items())[:3]\n\n[('bad_bands', [[300, 400], [1337, 1430], [1800, 1960], [2450, 2600]]),\n ('file_type', 'envi'),\n ('input_files',\n  ['D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_160816_directional_reflectance',\n   'D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_161211_directional_reflectance',\n   'D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_161637_directional_reflectance',\n   'D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_162028_directional_reflectance',\n   'D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_162431_directional_reflectance',\n   'D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_162845_directional_reflectance',\n   'D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_163305_directional_reflectance',\n   'D:\\\\2022\\\\FullSite\\\\D05\\\\2022_LIRO_3\\\\L1\\\\Spectrometer\\\\Reflectance\\\\2022062313\\\\NEON_D05_LIRO_DP1_20220623_163632_directional_reflectance'])]\n\n\n\n\nBRDF Coefficients JSON\nThe BRDF coefficients provided in the BRDF_COEFFS_JSON_for_Hytools refer to f_iso, f_geo, and f_vol in Equation 11 in the BRDF correction ATBD. BRDF effects (and the value of those three coefficients) vary by vegetation type and by wavelength. In the absence of a site-specific landcover map, the FlexBRDF approach uses NDVI as a proxy for characterizing the vegetation type. To characterize the vegetation diversity, it stratifies the entire range of NDVI values for a site into 18 bins dynamically such that each bin has roughly the same number of pixels (# of bins and dynamic binning approach are user-defined parameters in the config file). This binning is carried out separately for each of the 426 wavelength bands. The coefficients file provides the value of three coefficients used for each of the 18 NDVI bins for each of the 426 bands. The Python code chunk below demonstrates how to access these coefficients for a single flightline.\n\n# BRDF Coefficients\nbrdf_coeffs = h5_file['LIRO/Reflectance/Metadata/Logs/L004-1_20220623/BRDF_COEFFS_JSON_for_Hytools'][()]\n# use json.loads to read this in as a dictionary object\nbrdf_coeffs_dict = json.loads(brdf_coeffs.decode(\"utf-8\"))\n# display just the first 10 items to see the beginning of the file contents\nlist(brdf_coeffs_dict.items())[:10]\n# brdf_coeffs_dict # optionally display the full dictionary\n\n[('solar_zn_type', 'scene'),\n ('type', 'flex'),\n ('grouped', True),\n ('geometric', 'li_sparse'),\n ('volume', 'ross_thick'),\n ('b/r', 10),\n ('h/b', 2),\n ('sample_perc', 0.25),\n ('interp_kind', 'linear'),\n ('calc_mask',\n  [['ndi', {'band_1': 850, 'band_2': 660, 'min': 0.1, 'max': 1.0}]])]\n\n\nThis BRDF correction information may be useful to gain a better understanding of the BRDF implementation. Optionally, you may wish to carry out your own BRDF correction in HyTools with different parameters, starting from NEON’s L1 reflectance data, so this would be the place to check what was used in the NEON data products.",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#reflectance-data",
    "href": "notebooks/02_neon-l3-refl-h5-brdf-corrected.html#reflectance-data",
    "title": "Introduction to Bidirectional Hyperspectral Reflectance Data in Python",
    "section": "Reflectance Data",
    "text": "Reflectance Data\nFinally, let’s read in and plot the reflectance data.\n\nrefl_array = h5_file['LIRO/Reflectance/Reflectance_Data']\n\nThe function below stacks three bands of the reflectance data and rescales from 0 to 255 so it can be displayed properly.\n\ndef stack_rgb(refl_array,bands):\n    red = refl_array[:,:,bands[0]-1]\n    green = refl_array[:,:,bands[1]-1]\n    blue = refl_array[:,:,bands[2]-1]\n    stacked_rgb = np.stack((red,green,blue),axis=2)\n    rescaled_rgb = ((stacked_rgb - stacked_rgb.min()) * (1/(stacked_rgb.max() - stacked_rgb.min()) * 255)).astype('uint8')\n    return rescaled_rgb\n\nRun this function on the reflectance data, using bands in the visible portion of the spectrum (Red, Green, and Blue, bands 58, 34, and 19). This will create a “True Color Image”.\n\nrefl_rgb = stack_rgb(refl_array,(58,34,19))\nplt.imshow(refl_rgb,extent=map_info['extent']);\n\n\n\n\n\n\n\n\nYou can see this image is a little dark. The function below incorporates the skimage.exposure to rescale, or lighten up the data. It also adds in some other handy plotting features such as displaying the full UTM y coordinates on the y axis, rotating the x axis labels, and adding a plot title.\n\ndef plot_refl_rgb(rgb_array,ext,ls_pct=5,plot_title=''):   \n    p_low, p_high = np.percentile(rgb_array[~np.isnan(rgb_array)], (ls_pct,100-ls_pct))\n    img_rescale = exposure.rescale_intensity(rgb_array, in_range=(p_low,p_high))\n    img_rescale = exposure.rescale_intensity(rgb_array, in_range=(p_low,p_high))\n    plt.imshow(img_rescale,extent=ext)\n    plt.title(plot_title); \n    ax = plt.gca(); ax.ticklabel_format(useOffset=False, style='plain') \n    rotatexlabels = plt.setp(ax.get_xticklabels(),rotation=90) \n\n\nrefl_rgb = stack_rgb(refl_array,(58,34,19))\nplot_refl_rgb(refl_rgb,ext=map_info['extent'],ls_pct=.1,plot_title=\"LIRO Reflectance RGB Image\")\n\n\n\n\n\n\n\n\nYou can also plot a different 3-band combination, like a Color Infrared (CIR) image.\n\nrefl_cir = stack_rgb(refl_array,(90,34,19))\nplot_refl_rgb(refl_cir,ext=map_info['extent'],ls_pct=.5,plot_title=\"LIRO Reflectance Color Infrared Image\")",
    "crumbs": [
      "Python Notebooks",
      "2 NEON - BRDF-Corrected Hyperspectral Data"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html",
    "href": "setup/setup_instructions.html",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "The how-tos and tutorials in this repository require a NASA Earthdata account, an installation of Git, and a compatible Python Environment. Resources in this repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace.\nFor local Python environment setup we recommend using mamba to manage Python packages. To install mamba, download miniforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\n\nThese Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate ornl_daac_neon\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact ORNL DAAC User Services.",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html#python-environment-setup",
    "href": "setup/setup_instructions.html#python-environment-setup",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "These Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate ornl_daac_neon\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact ORNL DAAC User Services.",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  }
]