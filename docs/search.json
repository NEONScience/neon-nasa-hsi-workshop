[
  {
    "objectID": "setup/workshop_setup.html",
    "href": "setup/workshop_setup.html",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "If you plan to use this repository with the Openscapes 2i2c JupyterHub Cloud Workspace there are no additional setup requirements for the Python environment. All packages needed are included unless specified within a notebook, in which case a cell will be dedicated to installing the necessary Python libraries using the appropriate package manager.\nAfter completing the prerequisites you will have access to the Openscapes 2i2c JupyterHub cloud workspace. Click here to start JupyterLab. Use your email and the provided password to sign in. This password will be provided in the workshop. If you’re interested in using the 2i2c cloud workspace outside of the workshop, please contact us.\nAfter signing in you will be prompted for some server options:\nBe sure to select the radio button for Python and a size of 14.8 GB RAM and up to 3.75 CPUs.\nAt this point you can use the terminal to clone the repository.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#cloning-the-neon-data-skills-repository",
    "href": "setup/workshop_setup.html#cloning-the-neon-data-skills-repository",
    "title": "Cloud Workspace Setup",
    "section": "Cloning the NEON-Data-Skills Repository",
    "text": "Cloning the NEON-Data-Skills Repository\nIf you plan to edit or contribute to the NEON-Data-Skills repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.\nIf you just want to work with the notebooks or modules, you can simply clone the repository.\nTo clone the repository, navigate to the directory where you want to store the repository on your local machine, then type the following:\ngit clone https://github.com/NEONScience/NEON-Data-Skills.git",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#troubleshooting",
    "href": "setup/workshop_setup.html#troubleshooting",
    "title": "Cloud Workspace Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWe recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/prerequisites.html",
    "href": "setup/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nTo follow along during the workshop, or to run through the notebooks contained within the repository using the Openscapes 2i2c Cloud JupyterHub (cloud workspace), the following are required. All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov/users/new\nRemember your username and password; you will need them to download or access data during the workshop and beyond.\n\nNEON User accout and API token\n\nCreate a NEON User account (if you don’t already have one) following the instructions here: https://www.neonscience.org/about/user-accounts/\nCreate an API token and save this; you will use this to download and access NEON data during the workshop and beyond. Instructions on creating an API token can be found here: https://www.neonscience.org/resources/learning-hub/tutorials/neon-api-tokens-tutorial\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com/join. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form. You will receive an email invitation to join the organization on GitHub. You must join to gain access to the workspace.\n\n\n.netrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All workshop participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.",
    "crumbs": [
      "Setup Instructions",
      "Prerequisites"
    ]
  },
  {
    "objectID": "neon/02_neon-refl-classification.html",
    "href": "neon/02_neon-refl-classification.html",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "",
    "text": "The National Ecological Observatory Network (NEON) Airborne Observation Platform (AOP) collects airborne remote sensing data, including hyperspectral reflectance data, over 81 sites across the United States and Puerto Rico. In this notebook we will show how to download and visualize reflectance data from NEON’s Smithsonian Environmental Research Center site (SERC) in Maryland. We will then demonstrate how to run a supervised classification using the NEON Observational System (OS) Vegetation Structure data as training data, and evaluate the model results.\n\n\n\nThe NEON Imaging Spectrometer (NIS) is an airborne imaging spectrometer built by JPL (AVIRIS-NG) and operated by the National Ecological Observatory Network’s (NEON) Airborne Observation Platform (AOP). NEON’s hyperspectral sensors collect measurements of sunlight reflected from the Earth’s surface in 426 narrow (~5 nm) spectral channels spanning wavelengths between ~ 380 - 2500 nm. NEON’s remote sensing data is intended to map and answer questions about a landscape, with ecological applications including identifying and classifying plant species and communities, mapping vegetation health, detecting disease or invasive species, and mapping droughts, wildfires, or other natural disturbances and their impacts.\nIn 2024, NEON started producing bidirectional reflectance data products (including BRDF and topographic corrections). These are currently available for AOP data collected between 2022-2025. For more details on this newly revised data product, please refer to the tutorial: Introduction to Bidirectional Hyperspectral Reflectance Data in Python.\nNEON surveys sites spanning the continental US, during peak phenological greenness, capturing each site 3 out of every 5 years, for most terrestrial sites. AOP’s Flight Schedules and Coverage provide’s more information about the current and past schedules.\nMore detailed information about NEON’s airborne sampling design can be found in the paper: Spanning scales: The airborne spatial and temporal sampling design of the National Ecological Observatory Network.\n\n\n\n\nNo Python setup requirements if connected to the workshop Openscapes cloud instance!\nLocal Only\n\nUsing your preferred command line interface (command prompt, terminal, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\n\n```cmd\nconda create -n neon_aop -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas jupyter_bokeh h5py spectral scikit-image scikit-learn jupyterlab seaborn\n```\n\nFor MacOSX:\n\n```cmd\nconda create -n neon_aop -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter jupyter_bokeh h5py spectral scikit-image scikit-learn seaborn jupyterlab\n```\n\n\n\n\nNEON API Token (optional, but strongly recommended), see NEON API Tokens Tutorial for more details on how to create and set up your token in Python (and R). Once you create your token (on the NEON User Accounts) page, this notebook will show you how to set it as an environment variable and use it for downloading AOP data.\n\n\n\n\nThe lesson shows how to programmatically download the NEON shapefiles, but you can also download them by clicking on the following links:\n\nAOP Flight Box Boundaries: AOP_FlightBoxes.zip\nTOS Sampling Boundaries: TOS_SamplingBoundaries.zip\n\n\n\n\n\nExplore NEON airborne and field (instrumented, observational) shapefiles to understand what colloated data are available\nUse the neonutilities package to determine available reflectance data and download\nUse a custom function to convert reflectance data into an xarray dataset\nCreate some interactive visualizations of reflectance data\nRun a random forest model to classify trees using reflectance data and data generated from vegetation structure (as the training data set)\nEvaluate classification model results\nUnderstand data QA considerations and potential steps to improve classification results\n\n\n\n\n\nSetup\nVisualize NEON AOP, OS, and IS shapefiles at SERC\nFind available NEON reflectance data at SERC and download\nRead in and visualize reflectance data interactively\nCreate a random forest model to predict the tree families from the reflectance spectra"
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#setup",
    "href": "neon/02_neon-refl-classification.html#setup",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "1. Setup",
    "text": "1. Setup\n\n1.1 Import Python Packages\nIf not already installed, install the neonutilities and python-dotenv packages using pip as follows: - !pip install neonutilities - !pip install python-dotenv\n\n# Import required libraries, grouped by functionality\n# --- System and utility packages ---\nfrom datetime import timedelta\nimport dotenv\nimport os\nimport requests\n#import sys\nfrom zipfile import ZipFile\n\n# --- Data handling and scientific computing ---\nimport math\nimport numpy as np\nimport pandas as pd\n\n# --- Geospatial and multi-dimensional raster data ---\nimport geopandas as gpd\nimport h5py\nimport rasterio as rio  # work with geospatial raster data\nimport rioxarray as rxr  # work with raster arrays\nfrom shapely import geometry\nfrom shapely.geometry.polygon import orient\nfrom osgeo import gdal  # work with raster and vector geospatial data\nimport xarray as xr\n\n# --- Plotting and visualization ---\nimport holoviews as hv\nimport hvplot.xarray  # plot multi-dimensional arrays\n# import hvplot.pandas  # plot DataFrames/Series\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport folium\n# import folium.plugins\nfrom branca.element import Figure\nfrom IPython.display import display\nfrom skimage import io\n\n# --- neonutilities ---\nimport neonutilities as nu\n\n\n\n1.2 Set your NEON Token\nDefine your token. You can set this up on your NEON user account page, https://data.neonscience.org/myaccount. Please refer to the NEON API Tokens Tutorial for more details on how to create and set up your token in Python (and R).\n\n# method 1: set the NEON_TOKEN directly in your code\nNEON_TOKEN='YOUR_TOKEN_HERE'\n\nmethod 2: set the token as an environment variable using the dotenv package\ndotenv.set_key(dotenv_path=\".env\",\nkey_to_set=\"NEON_TOKEN\",\nvalue_to_set=\"YOUR_TOKEN_HERE\")"
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#visualize-neon-aop-os-and-is-shapefiles-at-serc",
    "href": "neon/02_neon-refl-classification.html#visualize-neon-aop-os-and-is-shapefiles-at-serc",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "2. Visualize NEON AOP, OS, and IS shapefiles at SERC",
    "text": "2. Visualize NEON AOP, OS, and IS shapefiles at SERC\nIn this next section, we will look at some of the NEON spatial data, honing in on our site of interest (SERC). We will look at the AOP flight box (the area over which the NEON AOP platform flies, including multiple priority boxes), the IS tower airshed, and the OS terrestrial sampling boundaries. This will provide an overview of how the NEON sites are set up, and the spatial overlap between the field and airborne data.\nFirst, let’s define a function that will download data from a url. We will use this to download shapefile boundares of the NEON AOP flight boxes, as well as the IS and OS shapefiles in order to see the spatial extent of the various data samples that NEON collects.\n\n# function to download data stored on the internet in a public url to a local file\ndef download_url(url,download_dir):\n    if not os.path.isdir(download_dir):\n        os.makedirs(download_dir)\n    filename = url.split('/')[-1]\n    r = requests.get(url, allow_redirects=True)\n    file_object = open(os.path.join(download_dir,filename),'wb')\n    file_object.write(r.content)\n\n\n2.1 NEON AOP flight box boundary\nDownload, Unzip, and Open the shapefile (.shp) containing the AOP flight box boundaries, which can also be downloaded from NEON Spatial Data and Maps. Read this shapefile into a geodataframe, explore the contents, and check the coordinate reference system (CRS) of the data.\n\n# Download and Unzip the NEON Flight Boundary Shapefile\naop_flight_boundary_url = \"https://www.neonscience.org/sites/default/files/AOP_flightBoxes_0.zip\"\n# Use download_url function to save the file to a directory\nos.makedirs('./data/shapefiles', exist_ok=True)\ndownload_url(aop_flight_boundary_url,'./data/shapefiles')\n# Unzip the file\nwith ZipFile(f\"./data/shapefiles/{aop_flight_boundary_url.split('/')[-1]}\", 'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles')\n\n\naop_flightboxes = gpd.read_file(\"./data/shapefiles/AOP_flightBoxes/AOP_flightboxesAllSites.shp\")\naop_flightboxes.head()\n\nNext, let’s examine the AOP flightboxes polygons at the SERC site.\n\nsite_id = 'SERC'\naop_flightboxes[aop_flightboxes.siteID == site_id]\n\nWe can see the site geodataframe consists of a single polygon, that we want to include in our study site (sometimes NEON sites may have more than one polygon, as there are sometimes multiple areas, with different priorities for collection).\n\n# write this to a new variable called \"site_polygon\"\nsite_aop_polygon = aop_flightboxes[aop_flightboxes.siteID == site_id]\n# subset to only include columns of interest\nsite_aop_polygon = site_aop_polygon[['domain','siteName','siteID','sampleType','flightbxID','priority','geometry']]\n# rename the flightbxID column to flightboxID for clarity\nsite_aop_polygon = site_aop_polygon.rename(columns={'flightbxID':'flightboxID'})\nsite_aop_polygon # display site polygon\n\nNext we can visualize our region of interest and the exterior boundary polygon containing ROIs. First add a function to help reformat bounding box coordinates to work with leaflet notation.\n\n# Function to convert a bounding box for use in leaflet notation\ndef convert_bounds(bbox, invert_y=False):\n    \"\"\"\n    Helper method for changing bounding box representation to leaflet notation\n\n    ``(lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))``\n    \"\"\"\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\nNow let’s define a function that uses folium to display the bounding box polygon on a map. We will first use this function to visualize the AOP flight box polygon, and then we will use it to visualize the IS and OS polygons as well.\n\ndef plot_folium_shapes(\n    shapefiles,      # list of file paths or GeoDataFrames\n    styles=None,     # list of style dicts for each shapefile\n    names=None,      # list of names for each shapefile\n    map_center=None, # [lat, lon]\n    zoom_start=12\n):\n    import pyproj\n    # If no center is provided, use the centroid of the first shapefile (projected)\n    if map_center is None:\n        if isinstance(shapefiles[0], str):\n            gdf = gpd.read_file(shapefiles[0])\n        else:\n            gdf = shapefiles[0]\n        # Project to Web Mercator (EPSG:3857) for centroid calculation\n        gdf_proj = gdf.to_crs(epsg=3857)\n        centroid = gdf_proj.geometry.centroid.iloc[0]\n        # Convert centroid back to lat/lon\n        lon, lat = gpd.GeoSeries([centroid], crs=\"EPSG:3857\").to_crs(epsg=4326).geometry.iloc[0].coords[0]\n        map_center = [lat, lon]\n    \n    m = folium.Map(\n        location=map_center,\n        zoom_start=zoom_start,\n        tiles=None\n    )\n    folium.TileLayer(\n        tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}',\n        attr='Google',\n        name='Google Satellite'\n    ).add_to(m)\n    \n    for i, shp in enumerate(shapefiles):\n        if isinstance(shp, str):\n            gdf = gpd.read_file(shp)\n        else:\n            gdf = shp\n        style = styles[i] if styles and i &lt; len(styles) else {}\n        layer_name = names[i] if names and i &lt; len(names) else f\"Shape {i+1}\"\n        folium.GeoJson(\n            gdf,\n            name=layer_name,\n            style_function=lambda x, style=style: style,\n            tooltip=layer_name\n        ).add_to(m)\n    \n    folium.LayerControl().add_to(m)\n    return m\n\n\n\nmap1 = plot_folium_shapes(\n    shapefiles=[site_aop_polygon],\n    names=['NEON AOP Flight Bounding Box']\n)\n\nmap1\n\n\n \n\nAOP flight box polygon at the SERC site.\n\n\n\n\n\n2.2 NEON OS terrestrial sampling boundaries\nWe will follow a similar process to download and visualize the NEON OS terrestrial sampling boundaries. The OS terrestrial sampling boundaries are also available as a shapefile, which can be downloaded from NEON Spatial Data and Maps page.\n\n# Download and Unzip the NEON Terrestrial Field Sampling Boundaries Shapefile\nneon_field_boundary_file = \"https://www.neonscience.org/sites/default/files/Field_Sampling_Boundaries_202503.zip\"\n# Use download_url function to save the file to the data directory\ndownload_url(neon_field_boundary_file,'./data/shapefiles')\n\n\n# Unzip the file\nwith ZipFile(f\"./data/shapefiles/Field_Sampling_Boundaries_202503.zip\", 'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles')\n\n\nneon_terr_bounds = gpd.read_file(\"./data/shapefiles/Field_Sampling_Boundaries/terrestrialSamplingBoundaries.shp\")\nneon_terr_bounds.head()\n\n\n# save the boundaries for the site to a new variable called \"site_terr_bounds\"\nsite_terr_bounds = neon_terr_bounds[neon_terr_bounds.siteID == site_id]\nsite_terr_bounds.head()\n\n\n\n2.3 NEON IS tower footprint boundaries\nLastly, we’ll download and read in the IS tower footprint shapefile, which represents the area of the airshed over which the IS tower collects data. This shapefile is available from the NEON Spatial Data and Maps page, but is pre-downloaded for convenience.\n\n# Unzip the 90 percent footprint tower airshed file\nwith ZipFile(f\"./data/90percentfootprint.zip\", 'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles')\n\n\n\nneon_tower_airshed = gpd.read_file(\"./data/shapefiles/90percentfootprint/90percent_footprint.shp\")\nneon_tower_airshed.head()\n\n\n# save the boundaries for the site to a new variable called \"site_terr_bounds\"\nsite_tower_bounds = neon_tower_airshed[neon_tower_airshed.SiteID == site_id]\nsite_tower_bounds.head()\n\n\n\n2.4 Visualize AOP, OS, and IS boundaries together\nNow that we’ve read in all the shapefiles into geodataframes, we can visualize them all together as follows. We will use the plot_folium_shapes function defined above, and define a styles list of dictionaries specifying the color, so that we can display each polygon with a different color.\n\nneon_shapefiles = [site_aop_polygon, site_terr_bounds, site_tower_bounds]\n\n# define a list of styles for the polygons\n# each style is a dictionary with 'fillColor' and 'color' keys\nstyles = [\n    {'fillColor': '#228B22', 'color': '#228B22'}, # green\n    {'fillColor': '#00FFFFFF', 'color': '#00FFFFFF'}, # blue\n    {'fillColor': '#FF0000', 'color': '#FF0000'}, # red\n    {'fillColor': '#FFFF00', 'color': '#FFFF00'}, # yellow\n]\n\nmap2 = plot_folium_shapes(\n    shapefiles=neon_shapefiles,\n    names=['NEON AOP Flight Bounding Box', 'NEON Terrestrial Sampling Boundaries', 'NEON Tower Airshed'],\n    styles=styles\n)\n\nmap2\n\n\n \n\nAOP, OS, and IS polygons at the SERC site.\n\n\n\nAbove we can see the SOAP flightbox, and the exterior TOS boundary polygon which shows the extent of the area where observational data are collected."
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#find-available-neon-reflectance-data-and-download",
    "href": "neon/02_neon-refl-classification.html#find-available-neon-reflectance-data-and-download",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "3. Find available NEON reflectance data and download",
    "text": "3. Find available NEON reflectance data and download\nFinally we can look at the available NEON hyperspectral reflectance data, which are delivered as 1 km by 1 km hdf5 files (also called tiles) over the site. The next figure we make will make it clear why the files are called tiles. First, we will determine the available reflectance data, and then pull in some metadata shapefiles from another L3 AOP data product, derived from the lidar data.\nNEON hyperspectral reflectance data are currently available under two different revisions, as AOP is in the process of implementing a BRDF (Bidirectional Reflectance Distribution Function), but this has not been applied to the full archive of data yet. These data product IDs are DP3.30006.001 (directional surface reflectance), and DP3.30006.002 (bidirectional surface reflectance). The bidirectional surface reflectance data include BRDF and topographic corrections, which helps correct for differences in illumination throughout the flight.\n\n3.1 Find available data\nLet’s see what data are available at the SERC site for each of these data products using the neonutilities list_available_dates function as follows:\n\n# define the data product IDs for the reflectance data\nrefl_rev1_dpid = 'DP3.30006.001'\nrefl_rev2_dpid = 'DP3.30006.002'\n\n\nprint(f'Directional Reflectance Data Available at NEON Site {site_id}:')\nnu.list_available_dates(refl_rev1_dpid,site_id)\n\n\nprint(f'Bidirectional Reflectance Data Available at NEON Site {site_id}:')\nnu.list_available_dates(refl_rev2_dpid,site_id)\n\nThe dates provided are the year and month that the data were published (YYYY-MM). A single site may be collected over more than one month, so this publish date typically represents the month where the majority of the flight lines were collected. There are released directional reflectance data available from 2016 to 2021, and provisional bidirectional reflectance data available in 2022 and 2025. As of 2025, bidirectional data are only available provisionally because they were processed in 2024 (there is a year lag-time before data is released to allow for time to review for data quality issues).\nFor this exercise, we’ll look at the most recent data, from 2025. You may wish to consider other factors, for example if you collected field data in a certain year, you are looking at a year when there was a recent disturbance, or if you want to find the clearest weather data (data are not always collected in optimal sky conditions). For SERC, the most recent clear (&lt;10% cloud cover) weather collection to date was in 2017, so this directional reflectance data may be another good option to consider for your analysis.\nFor this lesson, we will use the 2025 bidirectional reflectance data, which is provisional.\n\nyear = '2025'\n\n\n\n3.2 Download NEON Lidar data using neonutilities by_file_aop\nWe can download the reflectance data either using the neonutilities function nu.by_file_aop, which downloads all tiles for the entire site for a given year, or nu.by_tile_aop. To figure out the inputs of these functions, you can type nu.by_tile_aop?, for example.\nAOP data are projected into a WGS84 coordinate system, with coordinates in UTM x, y. When using nu.by_tile_aop you need to specify the UTM easting and northing values for the tiles you want to download. If you’re not sure the extent of the site, you can use the function nu.get_aop_tile_extents. Let’s do that here, for the SOAP site collected in 2024. First set up your NEON token as follows, replacing the \"YOUR TOKEN HERE\" string with the token copied from the NEON “My Account” page. This will help speeed up downloads, and is strongly recommended (and will likely soon be required) for interacting with the NEON data via the API.\ndotenv.set_key(dotenv_path=\".env\",\nkey_to_set=\"NEON_TOKEN\",\nvalue_to_set=\"YOUR TOKEN HERE\")\n\nserc2025_utm_extents = nu.get_aop_tile_extents(refl_rev2_dpid, \n                                               site_id,\n                                               year,\n                                               token=os.environ.get(\"NEON_TOKEN\"))\n\nThe AOP collection over SERC in 2025 extends from UTM 358000 - 370000 m (Easting) and 4298000 - 4312000 m (Northing). To display a list of the extents of every tile, you can print serc2025_utm_extents. This is sometimes useful when trying to determine the extents of irregularly shaped sites.\nWe can also look at the full extents by downloading one of the smaller lidar raster data products to start. The L3 lidar data products include metadata shapefiles that can be useful for understanding the spatial extents of the individual files that comprise the data product. To show how to look at these shapefiles, we can download the Canopy Height Model data (DP3.30015.001). The next cell shows how to do this:\n\n# download all CHM tiles (https://data.neonscience.org/data-products/DP3.30015.001)\nnu.by_file_aop(dpid='DP3.30015.001', # Ecosystem Structure / CHM \n               site=site_id,\n               year=year,\n               include_provisional=True,\n               token='NEON_TOKEN',\n               savepath='./data')\n\n\n# Unzip the lidar tile boundary file\nwith ZipFile(f\"./data/shapefiles/2025_SERC_7_TileBoundary.zip\", 'r') as zip_ref:\n    zip_ref.extractall('./data/shapefiles/2025_SERC_7_TileBoundary')\n\n\naop_tile_boundaries = gpd.read_file(\"./data/shapefiles/2025_SERC_7_TileBoundary/shps/NEON_D02_SERC_DPQA_2025_merged_tiles_boundary.shp\")\naop_tile_boundaries.head()\n\n\n# append this last boundary file to the existing neon_shapefiles list\nneon_shapefiles.append(aop_tile_boundaries)\n\n# plot all shapefiles together\nmap3 = plot_folium_shapes(\n    shapefiles=neon_shapefiles,\n    names=['NEON AOP Flight Bounding Box', 'NEON Terrestrial Sampling Boundaries', 'NEON Tower Airshed', 'AOP Tile Boundaries'],\n    styles=styles,\n    zoom_start=12\n)\n\nmap3\n\n\n \n\nAOP, OS, and IS polygons at the SERC site.\n\n\n\nFrom the image above, you can see why the data are called “tiles”! The individual tiles make up a grid comprising the full site. These smaller areas make it easier to process the large data, and allow for batch processing instead of running an operation on a huge file, which might cause memory errors.\n\n\n3.3 Download NEON Reflectance Data using neonutilities by_tile_aop\nNow that we’ve explored the spatial extent of the NEON airborne data, as well as the OS terrestrial sampling plots and the IS tower airshed, we can start playing with the data! First, let’s download a single tile to start. For this exercise we’ll download the tile that encompasses the NEON tower, since there is typically more OS sampling in the NEON tower plots. The tile of interest for us is 365000_4305000 (note that AOP tiles are named by the SW or lower-left coordinate of the tile).\nYou can specify the download path using the savepath variable. Let’s set it to a data directory in line with the root directory where we’re working, in this case we’ll set it to ./data/NEON/refl.\nThe reflectance data are large in size (especially for an entire site’s worth of data), so by default the download functions will display the expected download size and ask if you want to proceed with the download (y/n). The reflectance tile downloaded below is ~ 660 MB in size, so make sure you have enough space on your local disk (or cloud platform) before downloading. If you want to download without being prompted to continue, you can set the input variable check_size=False.\nBy default the files will be downloaded following the same structure that they are stored in on Google Cloud Storage, so the actual data files are nested in sub-folders. We encourage you to navigate through the data/DP3.30006.002 folder, and explore the additional metadata (such as QA reports) that are downloaded along with the data.\n\n# download the tile that encompasses the NEON tower\nnu.by_tile_aop(dpid='DP3.30006.002',\n               site=site_id,\n               year=2025,\n               easting=364005,\n               northing=4305005,\n               include_provisional=True,\n               token='NEON_TOKEN',\n               check_size=False,\n               savepath='./data')\n\nYou can either navigate to the download folder in File Explorer, or to programmatically see what files were downloaded, you can display the files as follows:\n\n# see all files that were downloaded (including data, metadata, and READMEs):\nfor root, dirs, files in os.walk(r'data\\DP3.30006.002'):\n    for name in files:\n        print(os.path.join(root, name))  # print file name\n\nYou can see there are several .txt and .csv files in addition to the .h5 data file (NEON_D02_SERC_DP3_364000_4305000_bidirectional_reflectance.h5). These include citation information: citation_DP3.30006.002_PROVISIONAL.txt, an issue log: issueLog_DP3.30006.002.csv, and a README: NEON.D02.SERC.DP3.30006.002.readme.20250719T050120Z.txt. We encourage you to look through these files, particularly the issue log, which conveys information about issues and the resolution for the data product in question. Make sure there is not a known issue with the data you downloaded, especially since it is provisional.\nIf you only want to see the names of the .h5 reflectance data you downloaded, you can modify the code as follows:\n\n# see only the .h5 files that were downloaded\nfor root, dirs, files in os.walk(r'.\\data\\DP3.30006.002'):\n    for name in files:\n        if name.endswith('.h5'):\n            print(os.path.join(root, name))  # print file name\n\nSuccess! We’ve now downloaded a NEON bidirectional surface reflectance tile into our data directory."
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#read-in-and-visualize-reflectance-data-interactively",
    "href": "neon/02_neon-refl-classification.html#read-in-and-visualize-reflectance-data-interactively",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "4. Read in and visualize reflectance data interactively",
    "text": "4. Read in and visualize reflectance data interactively\n\n4.1 Convert Reflectance Data to an xarray Dataset\nThe function below will read in a NEON reflectance hdf5 dataset and export an xarray dataset. According to the xarray documentation, “xarray makes working with labelled multi-dimensional arrays in Python simple, efficient, and fun!” rioxarray is simply the rasterio xarray extension, so you can work with xarray for geospatial data.\n\ndef aop_h5refl2xarray(h5_filename):\n    \"\"\"\n    Reads a NEON AOP reflectance HDF5 file and returns an xarray.Dataset with reflectance and weather quality indicator data.\n\n    Parameters\n    ----------\n    h5_filename : str\n        Path to the NEON AOP reflectance HDF5 file.\n\n    Returns\n    -------\n    dsT : xarray.Dataset\n        An xarray Dataset containing:\n            - 'reflectance': DataArray of reflectance values (y, x, wavelengths)\n            - 'weather_quality_indicator': DataArray of weather quality indicator (y, x)\n            - Coordinates: y (UTM northing), x (UTM easting), wavelengths, fwhm, good_wavelengths\n            - Metadata attributes: projection, spatial_ref, EPSG, no_data_value, scale_factor, bad_band_window1, bad_band_window2, etc.\n    \"\"\"\n    import h5py\n    import numpy as np\n    import xarray as xr\n\n    with h5py.File(h5_filename) as hdf5_file:\n        print('Reading in ', h5_filename)\n        sitename = list(hdf5_file.keys())[0]\n        h5_refl_group = hdf5_file[sitename]['Reflectance']\n        refl_dataset = h5_refl_group['Reflectance_Data']\n        refl_array = refl_dataset[()].astype('float32')\n\n        # Transpose and flip reflectance data\n        refl_arrayT = np.transpose(refl_array, (1, 0, 2))\n        refl_arrayT = refl_array[::-1, :, :]\n\n        refl_shape = refl_arrayT.shape\n        wavelengths = h5_refl_group['Metadata']['Spectral_Data']['Wavelength'][:]\n        fwhm = h5_refl_group['Metadata']['Spectral_Data']['FWHM'][:]\n\n        # Weather Quality Indicator: transpose and flip to match reflectance\n        wqi_array = h5_refl_group['Metadata']['Ancillary_Imagery']['Weather_Quality_Indicator'][()]\n        wqi_arrayT = np.transpose(wqi_array, (1, 0))\n        wqi_arrayT = wqi_array[::-1, :]\n\n        # Collect metadata\n        metadata = {}\n        metadata['shape'] = refl_shape\n        metadata['no_data_value'] = float(refl_dataset.attrs['Data_Ignore_Value'])\n        metadata['scale_factor'] = float(refl_dataset.attrs['Scale_Factor'])\n        metadata['bad_band_window1'] = h5_refl_group.attrs['Band_Window_1_Nanometers']\n        metadata['bad_band_window2'] = h5_refl_group.attrs['Band_Window_2_Nanometers']\n        metadata['projection'] = h5_refl_group['Metadata']['Coordinate_System']['Proj4'][()].decode('utf-8')\n        metadata['spatial_ref'] = h5_refl_group['Metadata']['Coordinate_System']['Coordinate_System_String'][()].decode('utf-8')\n        metadata['EPSG'] = int(h5_refl_group['Metadata']['Coordinate_System']['EPSG Code'][()])\n\n        # Parse map info for georeferencing\n        map_info = str(h5_refl_group['Metadata']['Coordinate_System']['Map_Info'][()]).split(\",\")\n        pixel_width = float(map_info[5])\n        pixel_height = float(map_info[6])\n        x_min = float(map_info[3]); x_min = int(x_min)\n        y_max = float(map_info[4]); y_max = int(y_max)\n        x_max = x_min + (refl_shape[1]*pixel_width); x_max = int(x_max)\n        y_min = y_max - (refl_shape[0]*pixel_height); y_min = int(y_min)\n\n        # Calculate UTM coordinates for x and y axes\n        x_coords = np.linspace(x_min, x_max, num=refl_shape[1]).astype(float)\n        y_coordsT = np.linspace(y_min, y_max, num=refl_shape[0]).astype(float)\n\n        # Flag good/bad wavelengths (1=good, 0=bad)\n        good_wavelengths = np.ones_like(wavelengths)\n        for bad_window in [metadata['bad_band_window1'], metadata['bad_band_window2']]:\n            bad_indices = np.where((wavelengths &gt;= bad_window[0]) & (wavelengths &lt;= bad_window[1]))[0]\n            good_wavelengths[bad_indices] = 0\n        good_wavelengths[-10:] = 0\n\n        # Create xarray DataArray for reflectance\n        refl_xrT = xr.DataArray(\n            refl_arrayT,\n            dims=[\"y\", \"x\", \"wavelengths\"],\n            name=\"reflectance\",\n            coords={\n                \"y\": (\"y\", y_coordsT),\n                \"x\": (\"x\", x_coords),\n                \"wavelengths\": (\"wavelengths\", wavelengths),\n                \"fwhm\": (\"wavelengths\", fwhm),\n                \"good_wavelengths\": (\"wavelengths\", good_wavelengths)\n            }\n        )\n\n        # Create xarray DataArray for Weather Quality Indicator\n        wqi_xrT = xr.DataArray(\n            wqi_arrayT,\n            dims=[\"y\", \"x\"],\n            name=\"weather_quality_indicator\",\n            coords={\n                \"y\": (\"y\", y_coordsT),\n                \"x\": (\"x\", x_coords)\n            }\n        )\n\n        # Create xarray Dataset and add metadata as attributes\n        dsT = xr.Dataset({\n            \"reflectance\": refl_xrT,\n            \"weather_quality_indicator\": wqi_xrT\n        })\n        for key, value in metadata.items():\n            if key not in ['shape', 'extent', 'ext_dict']:\n                dsT.attrs[key] = value\n\n        return dsT\n\nNow that we’ve defined a function that reads in the reflectance hdf5 data and exports an xarray dataset, we can apply this function to our downloaded reflectance data. This should take around 15 seconds or so to run.\n\n%%time\n# serc_refl_h5 = r'./data/NEON_D02_SERC_DP3_364000_4305000_bidirectional_reflectance.h5'\nserc_refl_h5 = r'./data/DP3.30006.002/neon-aop-provisional-products/2025/FullSite/D02/2025_SERC_7/L3/Spectrometer/Reflectance/NEON_D02_SERC_DP3_364000_4305000_bidirectional_reflectance.h5'\nserc_refl_xr = aop_h5refl2xarray(serc_refl_h5)\n\nNext let’s define a function that updates the neon reflectance xarray dataset to apply the no data value (-9999), set the bad bands to NaN, and applies the CRS to make the xarray objet an rioxarray object. These could also be incorporated into the function above, but you may wish to work with unscaled reflectance data, for example, so we will keep these functions separate for now.\n\ndef update_neon_xr(neon_refl_ds):\n\n    # Set no data values (-9999) equal to np.nan\n    neon_refl_ds.reflectance.data[neon_refl_ds.reflectance.data == -9999] = np.nan\n    \n    # Scale by the reflectance scale factor\n    neon_refl_ds['reflectance'].data = ((neon_refl_ds['reflectance'].data) /\n                                        (neon_refl_ds.attrs['scale_factor']))\n    \n    # Set \"bad bands\" (water vapor absorption bands and noisy bands) to NaN\n    neon_refl_ds['reflectance'].data[:,:,neon_refl_ds['good_wavelengths'].data==0.0] = np.nan\n\n    neon_refl_ds.rio.write_crs(f\"epsg:{neon_refl_ds.attrs['EPSG']}\", inplace=True)\n    \n    return neon_refl_ds\n\nApply this function on our xarray dataset. This should take a few seconds to run.\n\nserc_refl_xr = update_neon_xr(serc_refl_xr)\n\n\n\n4.2 Explore the reflectance dataset\nDisplay the dataset. You can use the up and down arrows to the left of the table (e.g. to the left of Dimensions, Coordinates, Data variables, etc.) to explore each part of the dataset in more detail. You can also click on the icons to the right to see more details.\n\nserc_refl_xr\n\n\n# function to auto-scale to make RGB images more realistic\ndef gamma_adjust(rgb_ds, bright=0.2, white_background=False):\n    array = rgb_ds.reflectance.data\n    gamma = math.log(bright)/math.log(np.nanmean(array)) # Create exponent for gamma scaling - can be adjusted by changing 0.2 \n    scaled = np.power(np.nan_to_num(array,nan=1),np.nan_to_num(gamma,nan=1)).clip(0,1) # Apply scaling and clip to 0-1 range\n    if white_background == True:\n        scaled = np.nan_to_num(scaled, nan = 1) # Set NANs to 1 so they appear white in plots\n    rgb_ds.reflectance.data = scaled\n    return rgb_ds\n\n\n\n4.3 Plot the reflectance dataset\nNow let’s plot a true color (or RGB) image of the reflectance data as shown in the cell below.\n\n# Plot the RGB image of the SERC tower tile\nserc_refl_rgb = serc_refl_xr.sel(wavelengths=[650, 560, 470], method='nearest')\nserc_refl_rgb = gamma_adjust(serc_refl_rgb,bright=0.3,white_background=True)\n\nserc_rgb_plot = serc_refl_rgb.hvplot.rgb(y='y',x='x',bands='wavelengths',\n                         xlabel='UTM x',ylabel='UTM y',\n                         title='NEON AOP Reflectance RGB - SERC Tower Tile',\n                         frame_width=480, frame_height=480)\n\n# Set axis format to integer (no scientific notation)\nserc_rgb_plot = serc_rgb_plot.opts(\n    xformatter='%.0f',\n    yformatter='%.0f'\n)\n\nserc_rgb_plot\n\n\n\n4.4 Plot the weather quality indicator data\nWe can look at the weather conditions during the flight by displaying the weather_quality_indicator data array. This is a 2D array with values ranging from 1 to 3, where: 1 = &lt;10% cloud cover, 2 = 10-50% cloud cover, 3 = &gt;50% cloud cover. NEON uses a stop-light convention to indicate the weather and cloud conditions, where green (1) is good, yellow (2) is moderate, and red (3) is poor. The figure below shows some examples of these three conditions as captured by the flight operators during science flights.\n\n \n\nCloud cover percentage during AOP flights. Left: green (&lt;10%), Middle: yellow (10-50%), Right: red (&gt;50%).\n\n\n\nLet’s visualize this weather quality indicator data for this SERC tile using a transparent color on top of our RGB reflectance plot, following the same stop-light convention.\n\n# Prepare the WQI mask\nwqi = serc_refl_xr.weather_quality_indicator\n\n# Map WQI values to colors: 1=green, 2=yellow, 3=red, 0/other=transparent\nwqi_colors = ['#228B22', '#FFFF00', '#FF0000']\n# wqi_mask = wqi[wqi &gt; 0]  # mask out zeros or nodata\n\n# Use hvplot with categorical colormap and alpha (50% transparency)\nwqi_overlay = wqi.hvplot.image(\n    x='x', y='y', cmap=wqi_colors,\n    clim=(1, 3), colorbar=False, alpha=0.5, \n    xlabel='UTM x', ylabel='UTM y',\n    title='NEON AOP Reflectance Weather Quality - SERC Tower Tile',\n    frame_width=480, frame_height=480)\n\n# Overlay the RGB and WQI\n(serc_rgb_plot * wqi_overlay).opts(title=\"RGB + Weather Quality Indicator\").opts(xformatter='%.0f',\n                                                                                 yformatter='%.0f')\n\nThe cloud conditions for this tile are yellow, which indicates somewhere between 10-50% cloud cover, which is moderate. This is not ideal for reflectance data, but it is still usable. As we will use this data for classification, you would want to consider how the cloud cover may impact your results. You may wish to find a clear-weather (&lt;10% cloud cover) tile to run classification, or at a minimum compare results between the two to better understand how cloud cover impacts the model.\n\n\n4.5 Plot a false color image\nLet’s continue visualizing the data, next by making a False-Color image, which is a 3-band combination that shows you more than what you would see with the naked eye. For example, you can pull in SWIR or NIR bands to create an image that shows more information about vegetation health Here we will use a SWIR band (2000 nm), a NIR band (850 nm), and blue band (450 nm). Try some different band combinations on your own, remembering not to use bands that are flagged as bad (e.g. the last 10 bands, or those in the bad band windows between 1340-1445 nm and between 1790-1955 nm).\n\n# Plot a False-Color image of the SERC tower tile\nserc_refl_false_color = serc_refl_xr.sel(wavelengths=[2000, 850, 450], method='nearest')\nserc_refl_false_color = gamma_adjust(serc_refl_false_color,bright=0.3,white_background=True)\nserc_refl_false_color.hvplot.rgb(y='y',x='x',bands='wavelengths',\n                         xlabel='UTM x',ylabel='UTM y',\n                         title='NEON AOP Reflectance False Color Image - SERC Tower Tile',\n                         frame_width=480, frame_height=480).opts(xformatter='%.0f', yformatter='%.0f')\n\n\n\n4.6 Make an interactive spectral signature plot\nWe can also make an interactive plot that displays the spectral signature of the reflectance data for any pixel you click on. This is useful for exploring the spectral signature of different land cover types, and can help you identify which bands may be most useful for classification.\n\n# Interactive Points Plotting\n# Modified from https://github.com/auspatious/hyperspectral-notebooks/blob/main/03_EMIT_Interactive_Points.ipynb\nPOINT_LIMIT = 10\ncolor_cycle = hv.Cycle('Category20')\n\n# Create RGB Map\nmap = serc_refl_rgb.hvplot.rgb(x='x', y='y',\n                               bands='wavelengths',\n                               fontscale=1.5,\n                               xlabel='UTM x', ylabel='UTM y',\n                               frame_width=480, frame_height=480).opts(xformatter='%.0f', yformatter='%.0f')\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = serc_refl_rgb.x.values[int(len(serc_refl_rgb.x) / 2)]\nymid = serc_refl_rgb.y.values[int(len(serc_refl_rgb.y) / 2)]\n\nx0 = serc_refl_rgb.x.values[0]\ny0 = serc_refl_rgb.y.values[0]\n\n# first_point = ([xmid], [ymid], [0])\nfirst_point = ([x0], [y0], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        data = serc_refl_xr.sel(x=x, y=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return serc_refl_xr.sel(x=x,y=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths', color='black', frame_width=400)\n    # return emit_ds.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n    #                                                                         color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)"
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#supervised-classification-using-tos-vegetation-structure-data",
    "href": "neon/02_neon-refl-classification.html#supervised-classification-using-tos-vegetation-structure-data",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "5. Supervised Classification Using TOS Vegetation Structure Data",
    "text": "5. Supervised Classification Using TOS Vegetation Structure Data\nIn the last part of this lesson, we’ll go over an example of how to run a supervised classification using the reflectance data along with observational “vegetation structure” data. We will create a random forest model to classify the families of trees represented in this SERC tile, using species determined from the vegetation structure data product (https://data.neonscience.org/data-products/DP1.10098.001)[DP1.10098.001] See the notebook __ to see how the vegetation structure data were pre-processed to generate the training data file. In this notebook, we will just read in this file as a starting point.\nNote that this is a quick-and-dirty example, and there are many ways you could improve the classification results, such as using more training data (this uses only data within this AOP tile), filtering out sub-optimal data (e.g. data collected in &gt; 10 % cloud cover conditions, removing outliers (e.g. due to spatial mis-match, shadowing, or other issues), tuning the model parameters, or using a different classification algorithm.\nLet’s get started, first by exploring the training data.\n\n5.1 Read in the training data\nFirst, read in the training data csv file (called serc_2025_training_data.csv) that was generated in the previous lesson. This file contains the training data for the random forest model, including the taxonId, family, and geographic coordinates (UTM easting and northing) of the training points. Note that there was not a lot of extensive pre-processing when creating this training data, so you may want to consider ways to assess and improve the training data quality before running the model.\n\nwoody_veg_data = pd.read_csv(r\"./data/serc_2025_training_data.csv\")\nwoody_veg_data.head()\n\nWe can use the xarray sel method to select the reflectance data corresponding to the training points. This will return an xarray dataset with the reflectance values for each band at the training point locations. As a test, let’s plot the reflectance values for the first training point, which corresponds to an American Beech tree (Fagaceae family).\n\n# Define the coordinates of the first training data pixel\neasting = woody_veg_data.iloc[0]['adjEasting']\nnorthing = woody_veg_data.iloc[0]['adjNorthing']\n\n# Extract the reflectance data from serc_refl_xr for the specified coordinates\npixel_value = serc_refl_xr.sel(x=easting, y=northing, method='nearest')\npixel_value.reflectance\n\n# Plot the reflectance values for the pixel\nplt.plot(pixel_value['wavelengths'].values.flatten(), pixel_value['reflectance'].values.flatten(), 'o');\n\nAs another test, we can plot the refletance value for one of the water bodies that shows up in the reflectance data. In the interactive plot, hover your mouse over one of the water bodies to see the UTM x, y coordinates, and then set those as the easting and northing, as shown below.\n\n# Define the coordinates of the pixel over the pool in the NW corner of the site\neasting = 364750\nnorthing = 4305180\n\n# Extract and plot the reflectance data from serc_refl_xr specified coordinates\npixel_value = serc_refl_xr.sel(x=easting, y=northing, method='nearest')\nplt.plot(pixel_value['wavelengths'].values.flatten(), pixel_value['reflectance'].values.flatten(), 'o');\n\nYou can see that the spectral signature of water is quite different from that of vegetation.\nNow that we’ve extracted the pixel value for a single pixel, we can extract the reflectance values for all of the training data points. We will loop through the rows of the training dataframe and use the xarray.Dataset.sel method to select the reflectance values of the pixels corresponding to the same geographic location as the training data points, and then we will convert this into a pandas DataFrame for use in the random forest model.\n\n\n5.2 Inspect the training data\nIt is good practice to visually inspect the spectral signatures of the training data, for example, as shown above, to make sure you are executing the code correctly, and that there aren’t any major outliers (e.g. you might catch instances of geographic mismatch between the terrestrial location and the airborne data, or if there was a shadowing effect that caused the reflectance values to be very low).\n\n# Get the wavelengths as column names for reflectance\nwavelengths = serc_refl_xr.wavelengths.values\nwavelength_cols = [f'refl_{int(wl)}' for wl in wavelengths]\n\nrecords = []\n\nfor idx, row in woody_veg_data.iterrows():\n    # Find nearest pixel in xarray\n    y_val = serc_refl_xr.y.sel(y=row['adjNorthing'], method='nearest').item()\n    x_val = serc_refl_xr.x.sel(x=row['adjEasting'], method='nearest').item()\n    # Extract reflectance spectrum\n    refl = serc_refl_xr.reflectance.sel(y=y_val, x=x_val).values\n    # Build record: taxonID, easting, northing, reflectance values\n    record = {\n        'taxonID': row['taxonID'],\n        'family': row['family'],\n        'adjEasting': row['adjEasting'],\n        'adjNorthing': row['adjNorthing'],\n    }\n    # Add reflectance values with wavelength column names\n    record.update({col: val for col, val in zip(wavelength_cols, refl)})\n    records.append(record)\n\nNow create a dataframe from this records, and display the first few rows. You can see that the reflectance values are in columns named refl_381, refl_386, etc., and the family is in the family column.\n\nreflectance_df = pd.DataFrame.from_records(records)\n# display the updated dataframe, which now includes the reflectance values for all \nreflectance_df\n\nDisplay the unique taxonIDs and families represented in this training data set:\n\nreflectance_df.taxonID.unique()\n\n\nreflectance_df.family.unique()\n\nNext we can manipulate the dataframe using melt to reshape the data and make it easier to display the reflectance spectra for each family. This is a helpful first step to visualizing the data and understanding what we’re working with before getting into the classification model.\nAfter re-shaping, we can make a figure to display what the spectra look like for the different families that were recorded as part of the vegetation structure data.\n\n# Melt (re-shape) the dataframe; wavelength columns start with 'refl_'\nmelted_df = reflectance_df.melt(\n    id_vars=['family', 'adjEasting', 'adjNorthing'],\n    value_vars=[col for col in reflectance_df.columns if col.startswith('refl_')],\n    var_name='wavelength',\n    value_name='reflectance'\n)\n\n# Convert 'wavelength' from 'refl_XXX' to integer\nmelted_df['wavelength'] = melted_df['wavelength'].str.replace('refl_', '').astype(int)\n\n# Create a summary dataframe that aggregates statistics (mean, min, and max)\nsummary_df = (\n    melted_df\n    .groupby(['family', 'wavelength'])\n    .reflectance\n    .agg(['mean', 'min', 'max'])\n    .reset_index()\n)\n\nplt.figure(figsize=(12, 7))\n\n# Create a color palette\npalette = sns.color_palette('hls', n_colors=summary_df['family'].nunique())\n\n# Plot the mean reflectance spectra for each family, filling with semi-transparent color between the min and max value\nfor i, (family, group) in enumerate(summary_df.groupby('family')):\n    # print(family)\n    if family in ['Fagaceae','Magnoliaceae','Hamamelidaceae','Juglandaceae','Aceraceae']:\n    # Plot mean line\n        plt.plot(group['wavelength'], group['mean'], label=family, color=palette[i])\n        # Plot min-max fill\n        plt.fill_between(\n            group['wavelength'],\n            group['min'],\n            group['max'],\n            color=palette[i],\n            alpha=0.2\n        )\n\nplt.xlabel('Wavelength')\nplt.ylabel('Reflectance')\nplt.title('Average Reflectance Spectra by Family \\n (with Min/Max Range)')\nplt.legend(title='family')\nplt.tight_layout()\nplt.show()\n\nWe can see that the spectral signatures for the different families have similar shapes, and there is a decent amount of spread in the reflectance values for each family. Some of this spread may be due to the cloud conditions during the time of acquisition. Reflectance values of one species may vary depending on how cloudy it was, or whether there was a cloud obscuring the sun during the collection. The random forest model may not be able to fully distinguish between the different families based on their spectral signatures, but we will see!\n\n\n5.3 Set up the Random Forest Model to Classify Tree Families\nWe can set up our random forest model by following the steps below:\n\nPrepare the training data by dropping the family column and setting the family column as the target variable. Remove the bad bands (NaN) from the reflectance predictor variables.\nSplit the data into training and testing sets.\nTrain the random forest model on the training data.\nEvaluate the model on the testing data.\nVisualize the results.\n\nWe will need to import scikit-learn (sklearn) packages in order to run the random forest model. If you don’t have these packages installed, you can install them using !pip install scikit-learn.\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\n\n5.4 Prepare and clean the training data\n\n# 1. Prepare the Data\n# Identify reflectance columns\nrefl_cols = [col for col in reflectance_df.columns if col.startswith('refl_')]\n\n# Remove rows with any NaN in reflectance columns (these are the 'bad bands')\nclean_df = reflectance_df.dropna(axis=1)\n# re-define refl_columns after removing the ones that are all NaN\nrefl_cols = [col for col in clean_df.columns if col.startswith('refl_')]\n\n\n# display the cleaned dataframe\nclean_df\n\nNote that we have &gt; 360 predictor variables (reflectance values for each band), and only 100 training points, so the model may not perform very well, due to over fitting. You can try increasing the number of training points by using more of the training data, or by using a different classification algorithm. Recall that we just pulled woody vegetation data from this tile that covers the tower, and there are also data collected throughout the rest of the TOS terrestrial sampling plots, so you could pull in more training data from the other tiles as well. You would likely not need all of the reflectance bands - for example, you could take every 2nd or 3rd band, or perform a PCA to reduce the number of bands. These are all things you could test as part of your model. For this lesson, we will include all of the valid reflectance bands for the sake of simplicity.\nThat said, we will need to remove some of the families that are poorly represented in the training data, as they will not be able to be predicted by the model. We can do this by filtering out families that have less than 10 training points. If you leave these in, the model will not be able to predict them, and will return an error when you try to evaluate the model.\n\n# determine the number of training data points for each family\nclean_df[['taxonID','family']].groupby('family').count().sort_values('taxonID', ascending=False)\n\n\n# Remove the rows where there are fewer than 10 samples\n# List of families to remove\nfamilies_to_remove = ['Rosaceae', 'Pinaceae', 'Ulmaceae']\n\n# Remove rows where family is in the families_to_remove list\nclean_df = clean_df[~clean_df['family'].isin(families_to_remove)].copy()\n\n\n\n5.5 Encode the target variable\nNext we need to encode the target variable (family) as integers, so that the model can work properly. Encoding is the process of converting from human-readable text (words / characters) to the byte representations used by computers. We can do this using the LabelEncoder from scikit-learn.\n\n# Encode the Target Variable (family)\n# Machine learning models require numeric targets. Use LabelEncoder:\nle = LabelEncoder()\nclean_df['family_encoded'] = le.fit_transform(clean_df['family'])\n# Display the cleaned dataframe after encoding the target variable\nclean_df[['taxonID','family','family_encoded','adjEasting','adjNorthing','refl_381','refl_2461']].head()\n\n\n# Confirm that the number of unique encodings is the same as the number of unique families, as a sanity check\nclean_df['family_encoded'].nunique()\n\n\n\n5.6 Split the data into training and testing sets\nIn this next step, we will split the data into training and testing sets. We will use 80% of the data for training and 20% for testing (this is the default split). This is a common practice in machine learning to test the performance of the model, and to ensure that the model is able to generalize to new data (e.g. you’re not over-fitting).\n\n# Split Data into Train/Test Sets\nX = clean_df[refl_cols].values\ny = clean_df['family_encoded'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n\n\n5.7 Create a Random Forest Classifier Object\n\n# Create a Random Forest Classifier object and fit it to the training data\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\n\n\n\n5.8 Evaluate the Model\n\n# Determine the accuracy scores based off of the test set\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nWhat do these accuracy metrics mean?\n\nPrecision: Of the items predicted as a given class, what fraction were actually that class?\n(Precision = True Positives / (True Positives + False Positives))\nRecall: Of all actual items of a given class, what fraction were correctly predicted?\n(Recall = True Positives / (True Positives + False Negatives))\nF1-score: The harmonic mean of precision and recall. It balances both metrics.\n(F1 = 2 * (Precision * Recall) / (Precision + Recall))\nSupport: The number of true instances of each class in the dataset (i.e., how many samples belong to each class).\n\nThese metrics are commonly used to evaluate classification models. Ideally we would be closer to 1 for the precision, recall, and f1-scores, which would indicate that the model is performing well. The support values indicate how many training points were used for each family, and you can see that some families have very few training points, which is likely negatively impacting the model performance."
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#discussion",
    "href": "neon/02_neon-refl-classification.html#discussion",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "Discussion",
    "text": "Discussion\nGreat job! You now should have a fairly good grasp on working with NEON airborne and field datasets together.\nWhat are some things you could do to improve the classification results?\nModels are only as good as the underlying training data - so the better the training data (more + higher quality training data points) the better your results will be.\nYou could consider some of the following options: 1. Increase the number of training points: Use more training data from the other plots (and use more AOP tiles). You could also collect your own additional data within the NEON site. 2. Filter out sub-optimal data: Remove training points that were collected in poor weather conditions (e.g. &gt; 10% cloud cover), or that are outliers (e.g. due to spatial mis-match, shadowing, or other issues). 3. Average the reflectance values over an entire tree crown: In this example, we just pulled the reflectance values from a single pixel, but you could average the reflectance values over an entire tree crown to get a more representative value for each tree. If part of the tree is in shadow, you may want to remove that pixel from the average. 4. Tune the model parameters: You can adjust the hyperparameters of the random forest model, such as the number of trees, maximum depth, and minimum samples per leaf, to improve performance. 5. Use a different classification algorithm: Random forest is a good starting point, but you could also try other algorithms such as support vector machines, gradient boosting, or neural networks to see if they perform better."
  },
  {
    "objectID": "neon/02_neon-refl-classification.html#next-steps",
    "href": "neon/02_neon-refl-classification.html#next-steps",
    "title": "Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray",
    "section": "Next Steps",
    "text": "Next Steps\nIn this example, we just scratched the surface of what you can do with NEON reflectance data. Here are some next steps you could take to further explore and analyze the data:\n\nApply the model to the entire reflectance dataset: Use the trained model to predict the tree families for all pixels in the reflectance dataset, and visualize the results. You may wish to include more training data for non-tree species, since the AOP data also captures non-vegetation such as water bodies, buildings, roads, etc.\nTry out the same model on SERC AOP data that was acquired in better weather conditions: Use the reflectance data from SERC 2017, which was collected in clearer weather conditions, to see if the model performs better. Note that you may need to make some minor modifications to the aop_h5refl2xarray functions to accommodate the slightly different data structure of the directional reflectance data product (2019 data is not yet available with BRDF and topographic corrections.)\nExplore other NEON sites: Use the neonutilities package to explore reflectance data from other NEON sites, and compare the spectral signatures of different land cover types.\nAdd in other NEON AOP datasets: In this lesson, we only looked at the reflectance data. How might other NEON data products compliment this analysis? For example, you could look at the lidar data to get information about the structure of the vegetation, for example the Canopy Height Model (CHM) or the Digital Surface Model (DSM). You could also look at the AOP imagery data.\nUse the reflectance data for other applications: The reflectance data can be used for a variety of applications, such as mapping vegetation health, detecting disease or invasive species, and mapping droughts, wildfires, or other natural disturbances and their impacts. You could use a similar approach to explore some of these applications.\n\n\n# Example Challenge Solution: Apply the model to the full AOP reflectance data tile at SERC\n\n# 1. Prepare the data:\n\n# Extract the reflectance array from your xarray Dataset:\nrefl = serc_refl_xr['reflectance'].values  # shape: (y, x, bands)\n# Remove the bad bands so that we can apply the model, which only uses 363 bands\ngood_bands = serc_refl_xr['good_wavelengths'].values.astype(bool)\nrefl_good = refl[:, :, good_bands]         # shape: (y, x, n_good_bands)\n\n# 2. Reshape for prediction:\nnrows, ncols, nbands = refl_good.shape\nrefl_2d = refl_good.reshape(-1, nbands)\n\n# 3. Apply the model:\n# Use the trained random forest model (e.g., rf_model) to predict values for every pixel\npreds = clf.predict(refl_2d)\n\n# 4. Reshape predictions back to image (y, x):\npred_map = preds.reshape(nrows, ncols)\n\n# 5. Create an xarray DataArray for mapping, using the coordinates from your original data:\npred_xr = xr.DataArray(\n    pred_map,\n    dims=('y', 'x'),\n    coords={'y': serc_refl_xr['y'], 'x': serc_refl_xr['x']},\n    name='classification_prediction'\n)\n\n# 6. Plot the map, using hvplot to visualize:\n# pred_xr.hvplot.image(x='x', y='y', cmap='tab20', title='Random Forest Classification Map')\n\n\nclasses = le.classes_\nprint('classes:', classes)\n\nclass_labels = dict(enumerate(classes))\nprint('class labels:',class_labels)\n\n\n# Convert your prediction DataArray to a categorical type with labels:\npred_xr_labeled = pred_xr.copy()\npred_xr_labeled = pred_xr_labeled.assign_coords(\n    family=(('y', 'x'), np.vectorize(class_labels.get)(pred_xr.values))\n)\n\n\n# Plot the classification map, using hvplot to visualize:\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\n# Plot using hvplot with the family coordinate as the color dimension:\n\nfamily_codes = np.arange(7)\nfamily_names = classes\n# Choose 7 distinct colors (can use tab10, Set1, or your own)\ncolors = plt.get_cmap('tab10').colors[:7]  # 7 distinct colors from tab10\ncmap = ListedColormap([code_to_color[code] for code in family_codes])\n\n# Create a mapping from code to color\ncode_to_color = {code: colors[i] for i, code in enumerate(family_codes)}\n\npred_xr_labeled.hvplot.image(\n    x='x', y='y', groupby=[], color='family', cmap=cmap,\n    title='Random Forest Classification Map', frame_width=600, frame_height=600\n).opts(xormatter='%.0f', yformatter='%.0f')\n\n\n# Optional: Create a custom colorbar for the classification map\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\n\nfig, ax = plt.subplots(figsize=(6, 1))\nfig.subplots_adjust(bottom=0.5)\n\n# Create a colormap and norm\ncmap = ListedColormap([code_to_color[code] for code in family_codes])\nnorm = BoundaryNorm(np.arange(-0.5, 7.5, 1), cmap.N)\n\n# Create colorbar\ncb = plt.colorbar(\n    plt.cm.ScalarMappable(norm=norm, cmap=cmap),\n    cax=ax, orientation='horizontal', ticks=family_codes\n)\ncb.ax.set_xticklabels(family_names,fontsize=6)\ncb.set_label('Family')\nplt.show()\n\n\nAcknowledgements\nMuch of this tutorial was inspired by and adapated from lessons in the NASA VITALS GitHub Repository. Thank you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "",
    "text": "NASA’s airborne science program and the NSF-funded National Ecological Observatory Network’s (NEON’s) Airborne Observation Platform (AOP) offer complementary remote sensing datasets ideal for carrying out large-scale ecological research. Both facilities operate similar airborne imaging spectrometers to collect visible to shortwave infrared (VSWIR) hyperspectral data, supporting regional ecosystem studies.\nThe Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC) archives data from NASA-funded ecological campaigns focusing on diverse environments such as river deltas and wetlands, the arctic, and tropics across multiple continents (North and Central America, South Africa). NEON’s AOP gathers high-resolution hyperspectral imagery, lidar, and RGB photography at 81 U.S. sites, offering repeat data spanning 2-10 years, with collections starting in 2013.\nThis workshop introduces NEON and NASA airborne and field datasets through live-coding exercises presented as Python Jupyter Notebook tutorials, demonstrating data access, exploration, and analysis. Participants will learn to apply these datasets to answer ecological research questions, gaining insights into regional and landscape areas of interest.\nThis workshop is hosted by National Ecological Observatory Network (NEON), NASA Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC) with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance. Instructions for setting up the Python environment locally are provided in the setup instructions.",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n8:00 AM\nIntroduction: NEON and NASA Airborne and Field Data\nBridget Hass and Michele Thornton\n\n\n8:05 AM\nOverview of NEON Airborne Observation Platform\nBridget Hass\n\n\n8:30 AM\nNotebook 1: Reflectance Visualization and Classification\nBridget Hass\n\n\n9:30 AM\nBreak\n\n\n\n9:35 AM\nOverview of Recent NASA Airborne Missions (SHIFT, BioSCape, AUVELO)\nMichele Thornton\n\n\n10:00 AM\nNotebook 2: Discovery and Analysis of VegPlot and AVIRIS Instrument Data\nMichele Thornton\n\n\n10:55 AM\nDiscussion and Wrap Up\nAll",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Plot to Plane: Working with NASA and NEON Airborne and Field Datasets",
    "section": "Contact Info",
    "text": "Contact Info\nNEON AOP\nOrganization: National Ecological Observatory Network Airborne Observation Platform (NEON AOP)1\nWebsite: https://neonscience.org/\nContact: https://www.neonscience.org/about/contact-us/\n1NEON is a project fully funded by the National Science Foundation and operated by Battelle.\nORNL DAAC\nOrganization: NASA Earthdata Data Center, Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)\nWebsite: https://www.earthdata.nasa.gov/centers/ornl-daac\nContact:\nNASA Earthdata Forum: https://forum.earthdata.nasa.gov/\nORNL DAAC - uso@daac.ornl.gov",
    "crumbs": [
      "Welcome",
      "2025 ESA Workshop"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.\n\n\n\n\nBe respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.\n\n\n\n\nThe following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.\n\n\n\n\nIf you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.\n\n\n\nViolations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-commitment",
    "href": "CODE_OF_CONDUCT.html#our-commitment",
    "title": "Code of Conduct",
    "section": "",
    "text": "We are dedicated to fostering a respectful environment for everyone contributing to this project. We expect all participants to treat each other with respect, professionalism, and kindness.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#expected-behavior",
    "href": "CODE_OF_CONDUCT.html#expected-behavior",
    "title": "Code of Conduct",
    "section": "",
    "text": "Be respectful and considerate of others.\nEngage in constructive discussions and offer helpful feedback.\nGracefully accept constructive criticism.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "href": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "title": "Code of Conduct",
    "section": "",
    "text": "The following behaviors will not be tolerated:\n\nHarassment, discrimination, or intimidation of any kind.\nOffensive, abusive, or derogatory language and actions.\nPersonal attacks or insults.\nTrolling or disruptive conduct.\nSharing inappropriate content.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#reporting-violations",
    "href": "CODE_OF_CONDUCT.html#reporting-violations",
    "title": "Code of Conduct",
    "section": "",
    "text": "If you experience or witness any behavior that violates this Code of Conduct, please report it by contacting the project maintainers. All reports will be reviewed confidentially.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Code of Conduct",
    "section": "",
    "text": "Violations of this Code of Conduct may result in actions such as warnings, temporary bans, or permanent exclusion from participation at the discretion of the maintainers.",
    "crumbs": [
      "Contributing",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html",
    "href": "background/nasa_neon_comparison.html",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "",
    "text": "While NASA and NEON operate similar imaging spectrometer instruments, there are a number of differences between the two in terms of the goals driving the airborne campaigns as well as the datasets provided. This section provides a high-level overview of some of the similarities and differences between the two. Table 1 shows some of the differences in terms of the major questions (when, where, why) about the two missions. Table 2 compares some of the differences in terms of the data products, processing methods, and data management (storage and access).\nWhile both NEON and NASA provide free, open datasets, the major difference between the NEON and NASA remote sensing airborne campaigns is related to their missions. NEON’s overarching mission is to produce a long-term archive of standardized data over the same sites over a 30 year time period, in order to provide a picture of long-term ecological change. NASA’s missions are campaign-driven, meaning that each campaign is typically designed around a specific research question, or instrument testing in some cases. For a more complete list of the NASA airborne campaigns hosted by the ORNL DAAC, refer to Datasets from Airborne Campaigns. Despite the differences in the drivers of the missions, the NEON and NASA datasets provide complimentary hyperspectral data, which could be used together for studies in their own right. In addition, both field and airborne datasets provide ground-truth (or close to ground-truth) sources that can help calibrate and validate data acquired from NASA’s upcoming Surface Biology and Geology SBG satellite mission.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-products",
    "href": "background/nasa_neon_comparison.html#data-products",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "Data products",
    "text": "Data products\nNEON data are processed to Level 1 (flightline), Level 2 (derived; flightline) and Level 3 (derived; tiled mosaic) data products, and include a number of derived data products such as vegetation and water indices, LAI, fPAR, and Albedo. NASA data are processed to Level 1 (flightline) radiance, however some missions (e.g. Bioscape) have been mosaicked and tiled.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-processing",
    "href": "background/nasa_neon_comparison.html#data-processing",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "Data Processing",
    "text": "Data Processing\n\nAtmospheric correction\nNEON uses ATCOR-4 for the atmospheric correction. NASA uses ISOFIT. There may be differences data derived from NEON v. NASA due to the different atmospheric correction method applied, as well as other corrections. NEON and NASA have been in communication about using the same atmospheric correction algorithm (ISOFIT) but this is still being scoped out.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_neon_comparison.html#data-storage-and-access",
    "href": "background/nasa_neon_comparison.html#data-storage-and-access",
    "title": "Comparing NEON and NASA Airborne Campaigns and Datasets",
    "section": "Data Storage and Access",
    "text": "Data Storage and Access\n\nNEON\nNEON data are stored on Google Cloud Storage (GCS) and are accessible via the NEON Data Portal. A subset of the L3 data products are also available on Google Earth Engine.\nNEON provides an API for downloading from the Data Portal, and has developed tools in R (neonUtilities) and Python (neonutilities) for downloading NEON data, and wrangling OS and IS data.\n\n\nNASA\nNASA airborne data are stored on Amazon Web Services (AWS) and can be accessed through the ORNL DAAC. NASA provides tools including Earthdata Search as well as the Python earthaccess package to help data users discover and download datasets.",
    "crumbs": [
      "Background",
      "Comparing NEON and NASA Airborne Campaigns"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html",
    "href": "background/nasa_airborne_background.html",
    "title": "NASA Airborne and Field Datasets",
    "section": "",
    "text": "Placeholder for general background on NASA Airborne and Field Data",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-c-datasets",
    "href": "background/nasa_airborne_background.html#aviris-c-datasets",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS-C Datasets",
    "text": "AVIRIS-C Datasets\nThe AVIRIS-C is an imaging spectrometer that delivers calibrated images of the upwelling spectral radiance in 224 contiguous spectral channels with wavelengths from 400 to 2500 nanometers (nm).\n\n\n\nData Product\n\n\n\n\nL1B Calibrated Radiance, Facility Instrument Collection\n\n\nL2 Calibrated Reflectance, Facility Instrument Collection",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-ng-datasets",
    "href": "background/nasa_airborne_background.html#aviris-ng-datasets",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS-NG Datasets",
    "text": "AVIRIS-NG Datasets\nThe AVIRIS-NG is the successor to AVIRIS-Classic and provides high signal-to-noise ratio imaging spectroscopy measurements in 425 contiguous spectral channels with wavelengths in the solar reflected spectral range (380-2510 nm) with 5 nm sampling. The AVIRIS-NG started operation in 2014 and is expected to replace the AVIRIS-C instrument.\n\n\n\nData Product\n\n\n\n\nL1B Calibrated Radiance, Facility Instrument Collection\n\n\nL2 Surface Reflectance, Facility Instrument Collection",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-3-datasets",
    "href": "background/nasa_airborne_background.html#aviris-3-datasets",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS-3 Datasets",
    "text": "AVIRIS-3 Datasets\nThe AVIRIS-3 is the third of the AVIRIS spectrometer FI series and has higher signal-to-noise ratio performance than AVIRIS-C or AVIRIS-NG. The core spectrometer of AVIRIS-3 is an optically fast, F/1.8 Dyson imaging spectrometer spanning a wide width (39.5-degree field of view). The AVIRIS-3 provides measurements in 285 contiguous spectral channels with wavelengths in the solar reflected spectral range (390-2500 nm) with 7.4 nm sampling. The AVIRIS-3 started operation in 2023.\n\n\n\nData Product\n\n\n\n\nL1B Calibrated Radiance, Facility Instrument Collection\n\n\nL2A Orthocorrected Surface Reflectance, Facility Instrument Collection\n\n\nL2B Greenhouse Gas Enhancements, Facility Instrument Collection",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#aviris-data-tutorials",
    "href": "background/nasa_airborne_background.html#aviris-data-tutorials",
    "title": "NASA Airborne and Field Datasets",
    "section": "AVIRIS Data Tutorials",
    "text": "AVIRIS Data Tutorials\n\nAVIRIS Data - Discovery, Access and Analysis",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#hytes-data-products",
    "href": "background/nasa_airborne_background.html#hytes-data-products",
    "title": "NASA Airborne and Field Datasets",
    "section": "HyTES Data Products",
    "text": "HyTES Data Products\n\n\n\n\n\n\nData Product\n\n\n\n\nLevel 1a - Calibrated but not geocoded Level 1 Brightness Temperature at Sensor\n\n\nLevel 1b - Calibrated and geolocated Level 1 Brightness Temperature at Sensor\n\n\nLevel 2 - HyTES Level 2 Data for Emissivity and Land Surface Temperature\n\n\nLevel 3 - Multi-species gas products (geolocated) CH4, H2S, S02, N02",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/nasa_airborne_background.html#hytes-data-tutorials",
    "href": "background/nasa_airborne_background.html#hytes-data-tutorials",
    "title": "NASA Airborne and Field Datasets",
    "section": "HyTES Data Tutorials",
    "text": "HyTES Data Tutorials\n\nAccessing and Visualizing HyTES data",
    "crumbs": [
      "Background",
      "NASA Airborne Data Background"
    ]
  },
  {
    "objectID": "background/neon_background.html",
    "href": "background/neon_background.html",
    "title": "NEON Airborne and Field Datasets",
    "section": "",
    "text": "NEON is a continental-scale observation facility designed to collect long-term open-access ecological data to better understand the complexities of Earth’s ecosystems and how they are changing. NEON uses cutting-edge sensor networks, instrumentation, observational sampling, natural history archive facilities and remote sensing methods and technologies to collect data on plants, animals, soil, nutrients, freshwater and the atmosphere.\nNEON operates 81 field sites strategically located across 20 ecoclimatic Domains across the United States, including 47 terrestrial sites and 34 freshwater aquatic sites. When logistically possible, aquatic and terrestrial field sites are colocated (i.e. in close proximity) to support understanding of linkages across terrestrial and aquatic ecosystems and their interactions with the atmosphere. For example, Domain 08, the Ozarks Complex, has three co-located sets of terrestrial and aquatic field sites. These sites are situated along the same watershed system, creating a unique opportunity to study hydrology, nutrient transport, and biogeochemical cycling through the watershed.\n\n\n\nNEON Field Sites Map; Green: Terrestrial Sites, Blue: Aquatic Sites",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#what-is-neon",
    "href": "background/neon_background.html#what-is-neon",
    "title": "NEON Airborne and Field Datasets",
    "section": "",
    "text": "NEON is a continental-scale observation facility designed to collect long-term open-access ecological data to better understand the complexities of Earth’s ecosystems and how they are changing. NEON uses cutting-edge sensor networks, instrumentation, observational sampling, natural history archive facilities and remote sensing methods and technologies to collect data on plants, animals, soil, nutrients, freshwater and the atmosphere.\nNEON operates 81 field sites strategically located across 20 ecoclimatic Domains across the United States, including 47 terrestrial sites and 34 freshwater aquatic sites. When logistically possible, aquatic and terrestrial field sites are colocated (i.e. in close proximity) to support understanding of linkages across terrestrial and aquatic ecosystems and their interactions with the atmosphere. For example, Domain 08, the Ozarks Complex, has three co-located sets of terrestrial and aquatic field sites. These sites are situated along the same watershed system, creating a unique opportunity to study hydrology, nutrient transport, and biogeochemical cycling through the watershed.\n\n\n\nNEON Field Sites Map; Green: Terrestrial Sites, Blue: Aquatic Sites",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#neon-airborne-observation-platform-aop",
    "href": "background/neon_background.html#neon-airborne-observation-platform-aop",
    "title": "NEON Airborne and Field Datasets",
    "section": "NEON Airborne Observation Platform (AOP)",
    "text": "NEON Airborne Observation Platform (AOP)\n\n\n\nNEON Airborne Remote Sensing\n\n\nAirborne remote sensing surveys are conducted over NEON field sites during peak greenness and provide quantitative information on land cover and changes to ecological structure and chemistry, including the presence and effects of invasive species. The surveys are supported by the NEON Airborne Observation Platform (AOP), a suite of earth observation instruments installed into a Twin Otter aircraft designed to collect high-resolution remote sensing data at low altitude. AOP was designed to collect regional-scale landscape information at the NEON field sites. The AOP maps areas where NEON’s observational and instrumented sampling is occurring and allows relationships to be drawn between NEON’s detailed in-situ observations to the broader environmental and ecological conditions.\n\nAOP Payload Sensors\nThe AOP consists of three complete and comparable instrument payloads. Typically, two of the payloads are dedicated to collections of the NEON field sites while the third is dedicated to NEON’s Research Support services which support externally driven research. The primary sensors on each payload include\n\nA discrete and full-waveform lidar to provide three-dimensional structural information of the landscape,\nAn imaging spectrometer to allow discrimination of land cover types and chemical content of vegetation,\nA high-resolution digital camera to provide spatially accurate and detailed contextual information, and\nA GPS antenna and receiver and Inertial Measurement Unit (IMU) to provide high-accuracy positioning and orientation of the aircraft.\n\n\n\nAOP Data Products\nThe AOP produces approximately 30 data products. The products are separated into categories of Level 1, Level 2, and Level 3 (L1, L2, L3). L1 represents the least processed data products. Additional processing steps are required to transition the L1 data to the derived L2 and L3 data. Broadly, the L1 and L2 products are provided by individual aircraft flight line, while L3 products are provided in 1 km by 1 km tiles. Generally, the data volume for L1 products is the highest and decreases for L2 and L3 products. Details of the different products within each Level can be found in the individual webpages for each sensor. All AOP data products can be found on the NEON Data Portal, and a subset of the L3 data products are available on Google Earth Engine.\n\nImaging Spectrometer Data Products\nLevel 1 (L1) products include at-sensor radiance and surface reflectance which are distributed by flightline. The image data is georeferenced to the ITRF00 datum and projected into the appropriate UTM zone, and provided at 1 m spatial resolution. Both the radiance and reflectance image data are stored in an HDF5 file format that includes extensive metadata and data quality information. The HDF5 format was selected because of the flexibility it allows in storing associated metadata.\nLevel 2 (L2) products are derived from the L1 surface reflectance and are produced at the same spatial resolution (1 m), datum and map projection as the Level 1 products. The L2 products include a suite of spectral indices designed to strategically combine bands to highlight vegetation characteristics such as photosynthetic activity or water content. For example, NDVI (Normalized Difference Vegetation Index) is a well-known and commonly used vegetation index which combines information from the NIR and Red regions to estimate vegetative greenness and can be used as a proxy for plant health. The L2 products also include fPAR (fraction of photosynthetically active radiation) and LAI (leaf area index), products further derived from vegetation indices. Additionally, a surface Albedo product that estimates the integrated reflectance of all the NIS bands into a single value is also provided. All L2 products are distributed by flightline in a GeoTIFF (gtiff) format. Currently, all vegetation indices, water indices, fPAR, and LAI are delivered with associated simulated error images.\nLevel 3 (L3) products include mosaics of all L1 and L2 products, excluding at-sensor radiance, and are distributed as 1 km x 1 km tiles instead of flightlines. Tiles are created by making a full mosaic of all the data and sub-setting the 1 km x 1 km tiles. The tiles are designed so their boundaries are set to even 1000 m UTM coordinate intervals. During the mosaic generation, the algorithm preferentially selects pixels that were collected under the best weather conditions in regions with multiple potential pixels due to flightline overlap. If weather conditions were equivalent, pixels acquired nearest to nadir of the image acquisition are selected. Generally, this will correspond to pixels that are nearest to the center of the flightline. The tiles are created at the same spatial resolution (1 m) as the L1 and L2 products are in delivered in gtiff format, with the exception of the surface reflectance, which is delivered in HDF5 format.\n\nBRDF and topographic corrections\nStarting in 2024, NEON began producing BRDF (Bidirectional Reflectance Distribution Function) and topographic corrected reflectance data, which include “bidirectional” in the name, and end with revision .002 in the Data Product IDs. As of 2025, these bidirectional reflectance are currently only available for data collected between 2022-2024. NEON is beginning to back-process earlier years (pre-2022) to apply the BRDF and topographic corrections. Please look at the data availability charts for each product on the data portal to determine whether the bidirectional data are available. Eventually, only bidirectional data products will be delivered, with the exception of the Level 1 Spectrometer orthorectified surface directional reflectance (DP1.30006.001), which will continue to be delivered, so that researchers who wish to carry out their own BRDF, topographic, or other corrections may do so.\nTable 1 below shows a full list of NEON’s spectrometer-derived data products, including the corresponding bidirectional reflectance data products, if applicable.\n\n\n\nTable 1: NEON AOP Imaging Spectrometer Datasets\n\n\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nBRDF-Corrected DPID\n\n\n\n\nSpectrometer orthorectified at-sensor radiance\nL1\nDP1.30008.001\n\n\n\nSpectrometer orthorectified surface (bi)directional reflectance\nL1\nDP1.30006.001\nDP1.30006.002\n\n\nAlbedo - spectrometer - flightline\nL2\nDP2.30011.001\nDP2.30011.002\n\n\nLAI - spectrometer - flightline\nL2\nDP2.30012.001\nDP2.30012.002\n\n\nfPAR - spectrometer - flightline\nL2\nDP2.30014.001\nDP2.30014.002\n\n\nCanopy water indices - flightline\nL2\nDP2.30019.001\nDP2.30019.002\n\n\nVegetation indices - spectrometer - flightline\nL2\nDP2.30026.001\nDP2.30026.002\n\n\nAlbedo - spectrometer - mosaic\nL3\nDP3.30011.001\nDP3.30011.002\n\n\nLAI - Spectrometer - mosaic\nL3\nDP3.30012.001\nDP3.30012.002\n\n\nfPAR - spectrometer - mosaic\nL3\nDP3.30014.001\nDP3.30014.002\n\n\nCanopy water indices - mosaic\nL3\nDP3.30019.001\nDP3.30019.002\n\n\nVegetation indices - spectrometer - mosaic\nL3\nDP3.30026.001\nDP3.30026.002\n\n\n\n\n\n\nIn addition to the spectrometer-derived data products, NEON generates 5 lidar-derived products (Table 2) and 2 RGB camera data products (Table 3), summarized below. These data products provide valuable structural and visual information that compliment the spectrometer data.\n\n\n\nLiDAR Data Products\n\n\n\nTable 2: NEON AOP Lidar Datasets\n\n\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nATBD Document #\n\n\n\n\nLiDAR Slant Range Waveform\nL1\nDP1.30001.001\nNEON.DOC.001293\n\n\nDiscrete Return LiDAR Point Cloud\nL1\nDP1.30003.001\nNEON.DOC.001292, NEON.DOC.001288\n\n\nEcosystem Structure\nL3\nDP3.30015.001\nNEON.DOC.002387\n\n\nElevation – LiDAR\nL3\nDP3.30024.001\nNEON.DOC.002390\n\n\nSlope and Aspect – LiDAR\nL3\nDP3.30025.001\nNEON.DOC.003791\n\n\n\n\n\n\n\n\nRGB Camera Products\n\n\n\nTable 3: NEON AOP Camera Datasets\n\n\n\n\n\nProduct Name\nLevel\nData Product ID (DPID)\nATBD Document #\n\n\n\n\nHigh-resolution orthorectified camera imagery\nL1\nDP1.30010.001\nNEON.DOC.001211vB\n\n\nHigh-resolution orthorectified camera imagery mosaic\nL3\nDP3.30010.001\nNEON.DOC.005052vB",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "background/neon_background.html#neon-field-data",
    "href": "background/neon_background.html#neon-field-data",
    "title": "NEON Airborne and Field Datasets",
    "section": "NEON Field Data",
    "text": "NEON Field Data\nIn addition to the AOP remote sensing data, NEON also provides Observational Sampling (OS) data and Instrumented Sampling (IS) data at terrestrial and aquatic sites. The field and instrumented sampling are briefly described below, but we encourage exploring the NEON website further for a more detailed understanding of the sensors and data products provided by the OS and IS groups.\n\nObservational Sampling\n\n\n\nNEON Observational Samples\n\n\nNEON field scientists collect a broad variety of observations and samples at terrestrial and aquatic field sites at regular intervals throughout the year. The data and samples collected by NEON’s Aquatic Observation System (AOS) and Terrestrial Observation System (TOS) are designed to provide standardized, continentally distributed observations of organisms, biogeochemistry, and physical properties.\n\n\nInstrumented Sampling\n\n\n\nNEON Instrumented Sampling\n\n\nNEON deploys automated instruments to collect meteorological, soil, phenological, surface water, and groundwater data at NEON field sites.\nWhere logistically possible, NEON colocated aquatic sites with terrestrial sites (21 in total) to support an understanding of linkages across atmospheric, terrestrial, and aquatic ecosystems. The suite of OS, IS, and AOP data provide an unparalleled opportunity to study ecosystem-level change over time in the United States.",
    "crumbs": [
      "Background",
      "NEON Airborne Data Background"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "Please submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\nWe want your help! Even if you’re not a coder! There are several ways you can contribute to this repository:\n\nReport an Issue or make a recommendation\nUpdate code, documentation, notebooks, or other files (even fixing typos)\nPropose a new notebook\n\nIn the sections below we outline how to approach each of these types of contributions. If you’re new to GitHub, you can sign up here. There are a bunch of great resources on the GitHub Quickstart page. The GitHub Cheatsheet is also quite helpful, even for experienced users. Please reach out to lpdaac@usgs.gov with questions or concerns.\n\n\nIf you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!\n\n\n\nTo contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\n\n\n\n\nIn the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a community_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the community_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.\n\n\n\nThese contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "href": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "If you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "href": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "To contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "href": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "In the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a community_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the community_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#attribution",
    "href": "CONTRIBUTING.html#attribution",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "These contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html",
    "href": "neon/01_make-classification-training-df.html",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "",
    "text": "This notebook demonstrates how to generate a training dataset consisting of tree species, family, and location from the NEON Terrestrial Observation System (TOS) Vegetation Structure data product DP1.10098.001. We will use data from the Smithsonian Environmental Research Center (SERC) site in Maryland. In a subsequent tutorial titled Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray, we will use this training dataset to train a random forest machine learning model that predicts tree families from the hyperspectral signatures obtained from the airborne remote sensing data. These two tutorials outline a relatively simple modeling example, and represent a starting point for conducting machine learning analyses using NEON data!\n\n\n\nUse the neonutilities load_by_product function to read in NEON vegetation structure data at a given site\nUse the NEON locations API to determine the geographic position of the vegetation records in UTM x, y coordinates\nFilter the datset to include only the latest data and columns of interest\nFilter the data geospatially to keep data that are within a single AOP 1 km x 1 km tile\n\n\n\n\n\nThe lesson Compare tree height measured from the ground to a Lidar-based Canopy Height Model is another example of linking ground to airborne data, and shows similar steps of pre-processing TOS woody vegetation data.\nThe paper Individual canopy tree species maps for the National Ecological Observatory Network outlines methods for large-scale classification using NEON data. The associated NEON Science Seminar Harnessing NEON to enable the future of forest remote sensing may be a useful resource. This talk provides a high-level overview of modeling approaches for tree crown delineation and tree classification using NEON airborne remote sensing data. You can also watch the video below.\n\n\n\n\n\nRefer to the Vegetation Structure User Guide for more details on this data product, and to better understand the data quality flags, the sampling.\n\nDisclaimer: this notebook is intended to provide an example of how to create an initial training data set for pairing with remote sensing data, and to conduct some exploratory analysis of the vegetation structure data. This does not incorporate outlier detection and removal, or comprehensive pre-processing steps. As part of creating a machine learning model, it is important to assess the training data quality and look for outliers or other potential data quality issues which may impact model results. Refer to the Compare tree height measured from the ground to a Lidar-based Canopy Height Model lesson (the first additional resource above) for more details on how you would address geographic mismatch between the AOP and TOS data.\nLet’s get started! First, import the required Python packages.\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport neonutilities as nu\nimport numpy as np\nimport pandas as pd\nimport requests\nimport seaborn as sns",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#download-and-explore-vegetation-structure-data-dp1.10098.001",
    "href": "neon/01_make-classification-training-df.html#download-and-explore-vegetation-structure-data-dp1.10098.001",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "Download and Explore Vegetation Structure Data (DP1.10098.001)",
    "text": "Download and Explore Vegetation Structure Data (DP1.10098.001)\nIn this first section we’ll load the vegetation structure data, find the locations of the mapped trees, and join to the species and family observations.\nDownload the vegetation structure data using the load_by_product function in the neonutilities package (imported as nu). Inputs to the function can be shown by typing help(load_by_product).\nRefer to t e&lt;a href”=https://www.neonscience.org/sites/default/files/cheat-sheet-neonUtilities.pd”f target=_blank R neonutilities cheat sheet or the neonUtilities package for more details and the complete index of possible function inputs. The cheat sheet is focused on the R package, but nearly all the inputs are the sam.e\nNote that in this example, we will pull in all the woody vegetation data (collected over all years), but if you are trying to modeldata collected in a single year, you can select just that year by specifying the startdate and enddate, or later filtering out the vegetation data by theeventID We have set check_size=False since the data are not very large, but to check the size of what the data you are downloading first, you could omit this input, or set it to True..\n\nveg_dict = nu.load_by_product(dpid=\"DP1.10098.001\", \n                              site=\"SERC\", \n                              package=\"basic\", \n                              release=\"RELEASE-2025\",\n                              check_size=False)\n\nFinding available files\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:56&lt;00:00,  2.46s/it]\nDownloading 23 NEON DP1.10098.001 files totaling approximately 40.0 MB.\nDownloading files\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:24&lt;00:00,  1.07s/it]\nC:\\Users\\bhass\\.conda\\envs\\lpdaac_vitals\\lib\\site-packages\\neonutilities\\unzip_and_stack.py:140: UserWarning: Filepaths on Windows are limited to 260 characters. Attempting to extract a filepath that is &gt; 260 characters long. Move your working or savepath directory closer to the root directory or enable long path support in Windows.\n  warnings.warn(\nStacking data files\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01&lt;00:00,  2.38it/s]\n\n\nGet a list of the points\n\nveg_map_all = veg_dict[\"vst_mappingandtagging\"]\nveg_map = veg_map_all.loc[veg_map_all[\"pointID\"] != \"\"]\nveg_map = veg_map.reindex()\nveg_map[\"points\"] = veg_map[\"namedLocation\"] + \".\" + veg_map[\"pointID\"]\nveg_points = list(set(list(veg_map[\"points\"])))\n\nLook at the unique eventIDs. . All sampling at a site that occurs within a given bout is identified by a unique eventID, which represents the date of the bout.\n\nveg_map_all.eventID.unique()\n\narray(['vst_SERC_2015', 'vst_SERC_2016', 'vst_SERC_2017', 'vst_SERC_2018',\n       'vst_SERC_2019', 'vst_SERC_2020', 'vst_SERC_2021', 'vst_SERC_2022',\n       'vst_SERC_2023'], dtype=object)\n\n\nGet the number of records for each eventID:\n\n# Group by 'eventID' and get the count\neventID_counts = veg_map_all[['individualID','eventID']].groupby(['eventID']).count()\nprint(\"\\nCounts of each eventID:\\n\", eventID_counts)\n\n\nCounts of each eventID:\n                individualID\neventID                    \nvst_SERC_2015          1890\nvst_SERC_2016          1330\nvst_SERC_2017            96\nvst_SERC_2018           127\nvst_SERC_2019           254\nvst_SERC_2020            22\nvst_SERC_2021            54\nvst_SERC_2022           494\nvst_SERC_2023            40\n\n\nIt looks like most of the trees were mapped in 2015 and 2016, which was when the SERC plots were first established. You could look at data only from one year, and compare to AOP data from the same year, or if you are not too worried about matching measurements to remote sensing data collected in the same year, you could use all years. We’ll do the latter in this example.",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#determine-the-geographic-location-of-the-surveyed-vegetation",
    "href": "neon/01_make-classification-training-df.html#determine-the-geographic-location-of-the-surveyed-vegetation",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "Determine the geographic location of the surveyed vegetation",
    "text": "Determine the geographic location of the surveyed vegetation\nLoop through all of the points in veg_points to determine the easting and norhting from the NEON Locations API.\n\neasting = []\nnorthing = []\ncoord_uncertainty = []\nelev_uncertainty = []\nfor i in veg_points:\n    vres = requests.get(\"https://data.neonscience.org/api/v0/locations/\"+i)\n    vres_json = vres.json()\n    easting.append(vres_json[\"data\"][\"locationUtmEasting\"])\n    northing.append(vres_json[\"data\"][\"locationUtmNorthing\"])\n    props = pd.DataFrame.from_dict(vres_json[\"data\"][\"locationProperties\"])\n    cu = props.loc[props[\"locationPropertyName\"]==\"Value for Coordinate uncertainty\"][\"locationPropertyValue\"]\n    coord_uncertainty.append(cu[cu.index[0]])\n    eu = props.loc[props[\"locationPropertyName\"]==\"Value for Elevation uncertainty\"][\"locationPropertyValue\"]\n    elev_uncertainty.append(eu[eu.index[0]])\n\npt_dict = dict(points=veg_points, \n               easting=easting,\n               northing=northing,\n               coordinateUncertainty=coord_uncertainty,\n               elevationUncertainty=elev_uncertainty)\n\npt_df = pd.DataFrame.from_dict(pt_dict)\npt_df.set_index(\"points\", inplace=True)\n\nveg_map = veg_map.join(pt_df, \n                     on=\"points\", \n                     how=\"inner\")\n\nNext, use the stemDistance and stemAzimuth data to calculate the precise locations of individuals, relative to the reference locations.\n\n\\(Easting = easting.pointID + stemDistance*sin(\\theta)\\)\n\\(Northing = northing.pointID + stemDistance*cos(\\theta)\\)\n\\(\\theta = stemAzimuth*\\pi/180\\)\n\nAlso adjust the coordinate and elevation uncertainties.\n\nveg_map[\"adjEasting\"] = (veg_map[\"easting\"]\n                        + veg_map[\"stemDistance\"]\n                        * np.sin(veg_map[\"stemAzimuth\"]\n                                   * np.pi / 180))\n\nveg_map[\"adjNorthing\"] = (veg_map[\"northing\"]\n                        + veg_map[\"stemDistance\"]\n                        * np.cos(veg_map[\"stemAzimuth\"]\n                                   * np.pi / 180))\n\nveg_map[\"adjCoordinateUncertainty\"] = veg_map[\"coordinateUncertainty\"] + 0.6\n\nveg_map[\"adjElevationUncertainty\"] = veg_map[\"elevationUncertainty\"] + 1\n\nLook at the columns to see all the information contained in this dataset.\n\nveg_map.columns\n\nIndex(['uid', 'namedLocation', 'date', 'eventID', 'domainID', 'siteID',\n       'plotID', 'pointID', 'stemDistance', 'stemAzimuth', 'recordType',\n       'individualID', 'supportingStemIndividualID', 'previouslyTaggedAs',\n       'otherTagID', 'otherTagOrg', 'samplingProtocolVersion',\n       'identificationHistoryID', 'taxonID', 'scientificName', 'genus',\n       'family', 'taxonRank', 'identificationReferences', 'morphospeciesID',\n       'morphospeciesIDRemarks', 'identificationQualifier', 'remarks',\n       'measuredBy', 'recordedBy', 'dataQF', 'publicationDate', 'release',\n       'points', 'easting', 'northing', 'coordinateUncertainty',\n       'elevationUncertainty', 'adjEasting', 'adjNorthing',\n       'adjCoordinateUncertainty', 'adjElevationUncertainty'],\n      dtype='object')\n\n\n\nCombine location with tree traits\nNow we have the mapped locations of individuals in the vst_mappingandtagging table, and the taxa (species, or taxonID), family, scientific name, as well as annual measurements of tree dimensions such as height and diameter in the vst_apparentindividual table. To bring these measurements together, join the two tables. Refer to the Quick Start Guide for Vegetation Structure for more information about the data tables and the joining instructions.\n\nveg_dict[\"vst_apparentindividual\"].set_index(\"individualID\", inplace=True)\nveg = veg_map.join(veg_dict[\"vst_apparentindividual\"],\n                   on=\"individualID\",\n                   how=\"inner\",\n                   lsuffix=\"_MAT\",\n                   rsuffix=\"_AI\")\n\n\n# show all the columns in the joined veg dataset\nveg.columns\n\nIndex(['uid_MAT', 'namedLocation_MAT', 'date_MAT', 'eventID_MAT',\n       'domainID_MAT', 'siteID_MAT', 'plotID_MAT', 'pointID', 'stemDistance',\n       'stemAzimuth', 'recordType', 'individualID',\n       'supportingStemIndividualID', 'previouslyTaggedAs', 'otherTagID',\n       'otherTagOrg', 'samplingProtocolVersion', 'identificationHistoryID',\n       'taxonID', 'scientificName', 'genus', 'family', 'taxonRank',\n       'identificationReferences', 'morphospeciesID', 'morphospeciesIDRemarks',\n       'identificationQualifier', 'remarks_MAT', 'measuredBy_MAT',\n       'recordedBy_MAT', 'dataQF_MAT', 'publicationDate_MAT', 'release_MAT',\n       'points', 'easting', 'northing', 'coordinateUncertainty',\n       'elevationUncertainty', 'adjEasting', 'adjNorthing',\n       'adjCoordinateUncertainty', 'adjElevationUncertainty', 'uid_AI',\n       'namedLocation_AI', 'date_AI', 'eventID_AI', 'domainID_AI', 'siteID_AI',\n       'plotID_AI', 'subplotID', 'tempStemID', 'tagStatus', 'growthForm',\n       'plantStatus', 'stemDiameter', 'measurementHeight',\n       'changedMeasurementLocation', 'height', 'baseCrownHeight',\n       'breakHeight', 'breakDiameter', 'maxCrownDiameter',\n       'ninetyCrownDiameter', 'canopyPosition', 'shape', 'basalStemDiameter',\n       'basalStemDiameterMsrmntHeight', 'maxBaseCrownDiameter',\n       'ninetyBaseCrownDiameter', 'dendrometerInstallationDate',\n       'initialGapMeasurementDate', 'initialBandStemDiameter',\n       'initialDendrometerGap', 'dendrometerHeight', 'dendrometerGap',\n       'dendrometerCondition', 'bandStemDiameter', 'remarks_AI',\n       'recordedBy_AI', 'measuredBy_AI', 'dataEntryRecordID', 'dataQF_AI',\n       'publicationDate_AI', 'release_AI'],\n      dtype='object')\n\n\n\n# look at the dataframe for out a subset of the columns that may be relevant\nveg[['date_AI','individualID','scientificName','taxonID','family','growthForm','plantStatus','plotID_AI','pointID','stemDiameter','adjEasting','adjNorthing']].head(5)\n\n\n\n\n\n\n\n\ndate_AI\nindividualID\nscientificName\ntaxonID\nfamily\ngrowthForm\nplantStatus\nplotID_AI\npointID\nstemDiameter\nadjEasting\nadjNorthing\n\n\n\n\n1\n2015-09-28\nNEON.PLA.D02.SERC.08038\nCarya glabra (Mill.) Sweet\nCAGL8\nJuglandaceae\nsingle bole tree\nLive\nSERC_045\n43\n46.4\n364809.083993\n4.304727e+06\n\n\n1\n2016-10-11\nNEON.PLA.D02.SERC.08038\nCarya glabra (Mill.) Sweet\nCAGL8\nJuglandaceae\nsingle bole tree\nLive\nSERC_045\n43\n47.1\n364809.083993\n4.304727e+06\n\n\n1\n2017-11-16\nNEON.PLA.D02.SERC.08038\nCarya glabra (Mill.) Sweet\nCAGL8\nJuglandaceae\nsingle bole tree\nLive\nSERC_045\n43\n47.4\n364809.083993\n4.304727e+06\n\n\n1\n2018-10-02\nNEON.PLA.D02.SERC.08038\nCarya glabra (Mill.) Sweet\nCAGL8\nJuglandaceae\nsingle bole tree\nLive\nSERC_045\n43\n47.9\n364809.083993\n4.304727e+06\n\n\n1\n2019-11-07\nNEON.PLA.D02.SERC.08038\nCarya glabra (Mill.) Sweet\nCAGL8\nJuglandaceae\nsingle bole tree\nLive\nSERC_045\n43\n47.9\n364809.083993\n4.304727e+06",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#remove-duplicate-records",
    "href": "neon/01_make-classification-training-df.html#remove-duplicate-records",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "Remove duplicate records",
    "text": "Remove duplicate records\nSome of these trees may have several recorded measurements, since we included all years of data. We will select only the latest recorded data so as to remove any duplicated data points. While measurements such as the tree height and stem diamter may change from one year to the next, the species should not.\n\n# Convert 'date_AI' to datetime\nveg.loc[:, 'date_AI'] = pd.to_datetime(veg['date_AI'])\n\n# Sort the DataFrame by 'individualID' and 'date_AI' in descending order\nveg_date_sorted = veg.sort_values(by=['individualID', 'date_AI'], ascending=[True, False])\n\n# Drop duplicates, keeping the first occurrence (which is the latest, or most recent, due to sorting)\nveg_latest = veg_date_sorted.drop_duplicates(subset='individualID', keep='first').copy()\n\n# Display the DataFrame with only the latest entries for each individualID\nprint(len(veg_latest))\n\n# Display a subset of the columns\nveg_latest[['date_AI','individualID','scientificName','taxonID','family','growthForm','plantStatus','plotID_AI','pointID','stemDiameter','adjEasting','adjNorthing']].head(5)\n\n1185\n\n\n\n\n\n\n\n\n\ndate_AI\nindividualID\nscientificName\ntaxonID\nfamily\ngrowthForm\nplantStatus\nplotID_AI\npointID\nstemDiameter\nadjEasting\nadjNorthing\n\n\n\n\n805\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00017\nAcer rubrum L.\nACRU\nAceraceae\nsingle bole tree\nLive\nSERC_054\n39\n27.5\n365000.851457\n4.305652e+06\n\n\n823\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00029\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_054\n57\n34.2\n365008.826768\n4.305655e+06\n\n\n819\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00032\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_054\n39\n32.0\n365002.039970\n4.305661e+06\n\n\n3865\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00039\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_054\n57\n10.7\n364993.798628\n4.305656e+06\n\n\n3136\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00041\nLiquidambar styraciflua L.\nLIST2\nHamamelidaceae\nsmall tree\nStanding dead\nSERC_054\n57\n9.7\n364990.841914\n4.305660e+06\n\n\n\n\n\n\n\nNow create a new dataframe containing only the veg data that are within a single AOP tile (which are 1 km x 1 km in size). For this, you will need to know the bounds (minimum and maximum UTM easting and northing) of the area you are sampling. For this exercise, we will choose the AOP data with SW (lower left) UTM coordinates of 364000, 4305000. This tile encompasses the NEON tower at the SERC site.\n\nveg_tower_tile = veg_latest[(veg_latest['adjEasting'].between(364000, 365000)) & (veg_latest['adjNorthing'].between(4305000, 4306000))]\n\nHow many records do we have within this tile?\n\nlen(veg_tower_tile)\n\n207\n\n\nThere are 207 unique vegetation records in this area. We can also look at the unique taxonIDs that are represented.\n\n# look at the unique Taxon IDs\nveg_tower_tile.taxonID.unique()\n\narray(['FAGR', 'LIST2', 'QUFA', 'LITU', 'ACRU', 'CACA18', 'NYSY', 'ULMUS',\n       'CAGL8', 'QURU', 'QUAL', 'CATO6', 'PINUS', 'QUERC', 'COFL2',\n       'PRAV', 'QUVE'], dtype=object)\n\n\nLet’s keep only a subset of the columns that we are interested in, and look at the dataframe:\n\nveg_tower_tile_short = veg_tower_tile[['date_AI','individualID','scientificName','taxonID','family','growthForm','plantStatus','plotID_AI','pointID','stemDiameter','adjEasting','adjNorthing']]\nveg_tower_tile_short\n\n\n\n\n\n\n\n\ndate_AI\nindividualID\nscientificName\ntaxonID\nfamily\ngrowthForm\nplantStatus\nplotID_AI\npointID\nstemDiameter\nadjEasting\nadjNorthing\n\n\n\n\n3865\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00039\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_054\n57\n10.7\n364993.798628\n4.305656e+06\n\n\n3136\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00041\nLiquidambar styraciflua L.\nLIST2\nHamamelidaceae\nsmall tree\nStanding dead\nSERC_054\n57\n9.7\n364990.841914\n4.305660e+06\n\n\n800\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00043\nQuercus falcata Michx.\nQUFA\nFagaceae\nsingle bole tree\nDead, broken bole\nSERC_054\n57\n37.1\n364991.794414\n4.305655e+06\n\n\n911\n2022-12-05 00:00:00\nNEON.PLA.D02.SERC.00062\nQuercus falcata Michx.\nQUFA\nFagaceae\nsingle bole tree\nLive\nSERC_054\n39\n59.7\n364990.424114\n4.305650e+06\n\n\n1762\n2023-11-29 00:00:00\nNEON.PLA.D02.SERC.00173\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_047\n21\n22.5\n364673.259742\n4.305225e+06\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3787\n2023-11-29 00:00:00\nNEON.PLA.D02.SERC.14231\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_047\n23\n10.8\n364679.533579\n4.305222e+06\n\n\n4103\n2023-01-16 00:00:00\nNEON.PLA.D02.SERC.14548\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_057\n41\n11.1\n364455.772024\n4.305415e+06\n\n\n4096\n2023-01-16 00:00:00\nNEON.PLA.D02.SERC.14563\nQuercus alba L.\nQUAL\nFagaceae\nsingle bole tree\nLive\nSERC_057\n43\n10.1\n364470.101669\n4.305412e+06\n\n\n3818\n2022-11-15 00:00:00\nNEON.PLA.D02.SERC.14709\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive\nSERC_052\n41\n10.4\n364577.886559\n4.305883e+06\n\n\n3828\n2022-11-17 00:00:00\nNEON.PLA.D02.SERC.14723\nFagus grandifolia Ehrh.\nFAGR\nFagaceae\nsingle bole tree\nLive, physically damaged\nSERC_067\n59\n10.1\n364353.644854\n4.305790e+06\n\n\n\n\n207 rows × 12 columns\n\n\n\nTo get a better sense of the data, we can also look at the # of each species, to see if some species have more representation than others.\n\n# display the taxonID counts, sorted descending\nveg_tower_tile_taxon_counts = veg_tower_tile[['individualID','taxonID']].groupby(['taxonID']).count()\nveg_tower_tile_taxon_counts.sort_values(by='individualID',ascending=False)\n\n\n\n\n\n\n\n\nindividualID\n\n\ntaxonID\n\n\n\n\n\nFAGR\n47\n\n\nLITU\n35\n\n\nLIST2\n29\n\n\nACRU\n16\n\n\nCAGL8\n12\n\n\nCACA18\n11\n\n\nQUAL\n10\n\n\nNYSY\n10\n\n\nCATO6\n10\n\n\nQUFA\n9\n\n\nULMUS\n5\n\n\nQURU\n4\n\n\nCOFL2\n3\n\n\nPINUS\n2\n\n\nQUVE\n2\n\n\nPRAV\n1\n\n\nQUERC\n1\n\n\n\n\n\n\n\n\n# display the family counts, sorted descending\nveg_tower_tile_family_counts = veg_tower_tile[['individualID','family']].groupby(['family']).count()\nveg_tower_tile_family_counts.sort_values(by='individualID',ascending=False)\n\n\n\n\n\n\n\n\nindividualID\n\n\nfamily\n\n\n\n\n\nFagaceae\n73\n\n\nMagnoliaceae\n35\n\n\nHamamelidaceae\n29\n\n\nJuglandaceae\n22\n\n\nAceraceae\n16\n\n\nCornaceae\n13\n\n\nBetulaceae\n11\n\n\nUlmaceae\n5\n\n\nPinaceae\n2\n\n\nRosaceae\n1\n\n\n\n\n\n\n\nIt looks like there are a number of species mapped in this tower plot. You can use the https://plants.usda.gov website to look up the species information. The top 5 most abundant mapped species are linked below.\n\nFAGR: American Beech (Fagus grandifolia Ehrh.)\nLITU: Tuliptree (Liriodendron tulipifera L.)\nLIST2: Sweetgum (Liquidambar styraciflua L.)\nACRU: Red Maple (Acer rubrum L.)\nCAGL8: Sweet pignut hickory (Carya glabra (Mill.))\n\nWhen carrying out classification, the species that only have small representation (1-5 samples) may not be modeled accurately due to a lack of sufficient training data. The challenge of mapping rarer species due to insufficient training data is well known. In the next tutorial, we will remove these poorly represented samples before generating a model.",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#write-training-dataframe-to-csv-file",
    "href": "neon/01_make-classification-training-df.html#write-training-dataframe-to-csv-file",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "Write training dataframe to csv file",
    "text": "Write training dataframe to csv file\nNonetheless, we have a fairly decent training dataset to work with. We can save the dataframe to a csv file called serc_training_data.csv as follows:\n\nveg_tower_tile_short.to_csv(r'.\\data\\serc_training_data.csv',index=False)",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#plot-species-and-stem-diameter",
    "href": "neon/01_make-classification-training-df.html#plot-species-and-stem-diameter",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "Plot species and stem diameter",
    "text": "Plot species and stem diameter\nFinally, we can make a quick plot using seaborn (imported as sns) to show the spatial distrubtion of the trees surveyed in this area, along with their species (scientificName). Most of this code helps improve the formatting and appearance of the figure; the first sns.scatterplot chunk is all you really need to do to plot the essentials.\n\nax = sns.scatterplot(\n    data=veg_tower_tile_short,\n    x='adjEasting',\n    y='adjNorthing',\n    size='stemDiameter',\n    hue='scientificName',\n    sizes=(2, 20),\n    alpha=0.7\n)\n\nax.set_aspect('equal', adjustable='datalim')\n\n# Remove scientific notation\nax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x:.0f}'))\nax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda y, _: f'{y:.0f}'))\n\nax.legend_.remove()\n\nhandles, labels = ax.get_legend_handles_labels()\nn_species = veg_tower_tile_short['scientificName'].nunique()\n\n# Species legend\nhue_handles = handles[1:n_species+1]\nhue_labels = labels[1:n_species+1]\nlegend1 = ax.legend(\n    hue_handles, hue_labels,\n    title=\"Species\",\n    bbox_to_anchor=(1.05, 0.5),\n    loc='center left',\n    fontsize='small',\n    title_fontsize='small'\n)\nax.add_artist(legend1)\n\n# Size legend (skip the first label, which is the old 'stemDiameter')\nsize_handles = handles[n_species+2:]\nsize_labels = labels[n_species+2:]\nlegend2 = ax.legend(\n    size_handles, size_labels,\n    title=\"Stem Diameter (cm)\",\n    bbox_to_anchor=(1.5, 0.5),\n    loc='center left',\n    fontsize='small',\n    title_fontsize='small'\n)\n\n# Add title and axis labels\nax.set_title(\"SERC Tree Species and Stem Diameters\", fontsize=14)\nax.set_xlabel(\"Easting (m)\", fontsize=12)\nax.set_ylabel(\"Northing (m)\", fontsize=12)\n\nplt.show()",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon/01_make-classification-training-df.html#recap",
    "href": "neon/01_make-classification-training-df.html#recap",
    "title": "Make Training Data for Species Modeling from NEON TOS Vegetation Structure Data",
    "section": "Recap",
    "text": "Recap\nIn this lesson we have curated a training data set containing information about the tree family and species as well as its geographic location in UTM x, y coordinates. We can now pair this dataset with remote sensing data and create a model to predict the tree’s family based off airborne spectral data. The next tutorial, Tree Classification with NEON Airborne Imaging Spectrometer Data using Python xarray, will show how to do this!",
    "crumbs": [
      "Tutorials",
      "1 NEON - Make a Training Dataset from Vegetation Structure Data"
    ]
  },
  {
    "objectID": "neon-nasa.html",
    "href": "neon-nasa.html",
    "title": "NEON and NASA Airborne and Field Tutorials",
    "section": "",
    "text": "Please view the NEON NASA Workshop Page for workshop details. Resources in this repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace. For local Python environment setup instructions please see the Setup Instructions.\nWelcome to the NEON - NASA Airborne Hyperspectral Data Resources Repository! This repository provides Python Jupyter notebooks to help the community work with visible to short-wave infrared imaging spectroscopy data from NEON’s Airbonre Observation Platform and missions carried out with the NASA AVIRIS sensor. These complimentary hyperspectral datasets provide an opportunity to …\nIn the interest of open science, this repository has been made public, but is still under active development. Make sure to consult the change log for the most recent changes to the repository. Contributions from all parties are welcome.",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "neon-nasa.html#contact-info",
    "href": "neon-nasa.html#contact-info",
    "title": "NEON and NASA Airborne and Field Tutorials",
    "section": "Contact Info",
    "text": "Contact Info\nNEON AOP\nOrganization: National Ecological Observatory Network Airborne Observation Platform (NEON AOP)1\nWebsite: https://neonscience.org/\nContact: https://www.neonscience.org/about/contact-us/\n1NEON is a project fully funded by the National Science Foundation and operated by Battelle.\nORNL DAAC\nOrganization: NASA Earthdata Data Center, Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC)\nWebsite: https://www.earthdata.nasa.gov/centers/ornl-daac\nContact:\nNASA Earthdata Forum: https://forum.earthdata.nasa.gov/\nORNL DAAC - uso@daac.ornl.gov\nDate last modified: 06-17-2025",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html",
    "href": "setup/setup_instructions.html",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "The how-tos and tutorials in this repository require a NASA Earthdata account, an installation of Git, and a compatible Python Environment. Resources in this repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace.\nFor local Python environment setup we recommend using mamba to manage Python packages. To install mamba, download miniforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\n\nThese Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate ornl_daac_neon\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact ORNL DAAC User Services.",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html#python-environment-setup",
    "href": "setup/setup_instructions.html#python-environment-setup",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "These Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n ornl_daac_neon -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate ornl_daac_neon\nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact ORNL DAAC User Services.",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  }
]